{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome!","text":"<p>PlanExe turns a single plain-English goal into a ~40-page strategic plan in ~15 minutes using local or cloud LLMs. It\u2019s an accelerator for first drafts \u2014 not a replacement for human refinement. PlanExe removes most of the labor for the planning scaffold; the final 10\u201330% that makes a plan credible and defensible remains human work.</p> <p>Try it first, then decide if you want to run it locally.</p> <ul> <li>Open a full sample report (HTML): Minecraft Escape sample report</li> <li>Try PlanExe in your browser (no installation): planexe.org</li> <li>See more examples: planexe.org/examples</li> </ul>"},{"location":"#start-here-pick-your-path","title":"Start here (pick your path)","text":"<p>Use the short decision guide: Start here</p>"},{"location":"#core-guides","title":"Core guides","text":"<ul> <li>Prompt writing guide</li> <li>Plan output anatomy</li> <li>Costs and models</li> </ul>"},{"location":"#what-you-get","title":"What you get","text":"<p>PlanExe generates a single HTML report (a self-contained artifact you can open in a browser). See the sample report here: Minecraft Escape sample report</p>"},{"location":"#2-minute-tour","title":"2-minute tour","text":"<p>Open the sample report and do this:</p> <ol> <li>Read Executive Summary to see the top-level deliverables, budget, risks, and next steps.</li> <li>Jump to Gantt Interactive to see how the goal gets broken down into many concrete tasks.</li> <li>Open Premortem to see what could go wrong and what to do about it.</li> </ol>"},{"location":"#get-help","title":"Get help","text":"<p>If you run into issues, join the PlanExe Discord \u2014 the community and maintainers are there to help. When you ask, include what you tried, your setup (OS, Docker vs local, model/LLM), and any error output so others can help you quickly.</p> <p>Join the PlanExe Discord \u2192</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Website: planexe.org</li> <li>GitHub: PlanExeOrg/PlanExe</li> <li>Discord: planexe.org/discord</li> </ul>"},{"location":"AGENTS/","title":"How PlanExe-docs Builds and Publishes to docs.planexe.org","text":"<p>This document describes how the PlanExe-docs repository takes content from this directory (<code>PlanExe/docs/</code>) and publishes it to https://docs.planexe.org.</p>"},{"location":"AGENTS/#overview","title":"Overview","text":"<ul> <li>Content source: This directory (<code>PlanExe/docs/</code>). All Markdown files, images, and assets here become the published documentation.</li> <li>Build &amp; deploy: The PlanExe-docs repo. It holds MkDocs config, GitHub Actions workflow, and build scripts.</li> <li>Output: Static site served via GitHub Pages at docs.planexe.org.</li> </ul>"},{"location":"AGENTS/#pipeline-ci","title":"Pipeline (CI)","text":"<ol> <li>Trigger    The Deploy Documentation workflow runs when:</li> <li>There is a push to <code>main</code> on PlanExe-docs, or</li> <li>It is started manually (<code>workflow_dispatch</code>), or</li> <li>A <code>repository_dispatch</code> event <code>docs-updated</code> is sent (e.g. when PlanExe is updated and you want to redeploy docs).</li> </ol> <p>Pushing only to PlanExe does not by itself update docs.planexe.org. This repo has a workflow (<code>.github/workflows/docs-update.yml</code>) that runs on push to <code>main</code> when <code>docs/</code> changes and sends <code>repository_dispatch</code> to PlanExe-docs. For that to work you must add a secret in PlanExe (see below). Otherwise, after editing <code>PlanExe/docs/</code>, either run the Deploy workflow manually in PlanExe-docs, or push to PlanExe-docs <code>main</code> (e.g. after syncing content) to deploy.</p> <ol> <li>Checkout </li> <li>PlanExe-docs repo (workflow, <code>mkdocs.yml</code>, <code>requirements.txt</code>, etc.).  </li> <li> <p>PlanExe repo into <code>planexe-source/</code> (so this <code>docs/</code> directory is available).</p> </li> <li> <p>Build </p> </li> <li><code>mkdir -p docs</code> in the PlanExe-docs workspace.  </li> <li><code>cp -r planexe-source/docs/* docs/</code> \u2014 all content from this <code>PlanExe/docs/</code> directory is copied into PlanExe-docs\u2019 <code>docs/</code> folder.  </li> <li> <p><code>mkdocs build --site-dir site</code> \u2014 MkDocs (Material theme, config from <code>mkdocs.yml</code>) builds the site into <code>site/</code>.</p> </li> <li> <p>Deploy </p> </li> <li>The peaceiris/actions-gh-pages action publishes the <code>site/</code> directory to the gh-pages branch of PlanExe-docs.  </li> <li>Custom domain docs.planexe.org is set via <code>cname: docs.planexe.org</code> in the workflow.  </li> <li>GitHub Pages serves the site from that branch, so updates appear at https://docs.planexe.org.</li> </ol>"},{"location":"AGENTS/#key-files","title":"Key files","text":"What Where Doc content (you edit here) <code>PlanExe/docs/</code> (this directory) MkDocs config, theme, plugins PlanExe-docs <code>mkdocs.yml</code> Deploy workflow PlanExe-docs <code>.github/workflows/deploy.yml</code> Build dependencies PlanExe-docs <code>requirements.txt</code> Frontpage <code>PlanExe/docs/index.md</code> (used as site index)"},{"location":"AGENTS/#linking-between-documentation-pages","title":"Linking between documentation pages","text":"<p>When adding or editing links from one doc file to another in <code>PlanExe/docs/</code>, use paths that MkDocs (used by PlanExe-docs <code>build.py</code>) can resolve. Otherwise the build will report \"unrecognized relative link\" and leave the URL as-is on the published site.</p> <p>Do:</p> <ul> <li>Use the <code>.md</code> extension in relative links to other docs in this directory.</li> <li>Same directory: <code>[MCP](mcp/mcp_details.md)</code>, <code>[Getting started](getting_started.md)</code>.</li> <li>Subdirectory: <code>[Extra](guides/extra.md)</code> (if you have <code>docs/guides/extra.md</code>).</li> </ul> <p>Do not:</p> <ul> <li>Use trailing slashes for doc-to-doc links: <code>[MCP](mcp/)</code> is not resolved by MkDocs and will trigger a build warning.</li> </ul> <p>Examples (in any file under <code>PlanExe/docs/</code>):</p> <pre><code>[PlanExe MCP interface](mcp/planexe_mcp_interface.md)\n[Docker](docker.md)\n[OpenRouter](ai_providers/openrouter.md)\n</code></pre> <p>External links (e.g. <code>https://planexe.org/</code>) are unchanged; this applies only to links between documentation <code>.md</code> files in this repo.</p>"},{"location":"AGENTS/#documentation-conventions","title":"Documentation conventions","text":"<ul> <li>Tone: keep it factual and direct; avoid marketing terms like \u201cquickstart,\u201d \u201cfastest,\u201d or \u201cseamless.\u201d</li> <li>Style guide: follow <code>docs_style_guide.md</code> for structure and terminology.</li> <li>Social cards: if a page needs a specific social card title, add front matter:   <pre><code>---\ntitle: Your page title\n---\n</code></pre></li> <li>Links: prefer Markdown links for URLs in prose, not bare URLs.</li> <li>AI providers: provider docs live under <code>ai_providers/</code> (e.g. <code>ai_providers/openrouter.md</code>).</li> <li>MCP setup: the MCP setup guide is <code>mcp/mcp_setup.md</code> (avoid \u201cquickstart\u201d).</li> </ul>"},{"location":"AGENTS/#local-preview","title":"Local preview","text":"<p>To build and preview the same site locally:</p> <ol> <li>Clone both PlanExe and PlanExe-docs.  </li> <li>From PlanExe-docs, run <code>python build.py</code> (optionally set <code>PLANEXE_REPO</code> if PlanExe is not at <code>../PlanExe</code>).  </li> <li>This copies <code>PlanExe/docs/</code> into a temp <code>docs/</code> dir, runs <code>mkdocs build</code>, and writes output to <code>site/</code>.  </li> <li>Run <code>python serve.py</code> to serve <code>site/</code> at <code>http://127.0.0.1:18525/</code>.</li> </ol>"},{"location":"AGENTS/#auto-deploy-from-planexe-optional","title":"Auto-deploy from PlanExe (optional)","text":"<p>To have the live site update when you push to PlanExe <code>main</code> with changes under <code>docs/</code>:</p> <ol> <li>In PlanExe repo: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret.</li> <li>Name: <code>PLANEXE_DOCS_DISPATCH_TOKEN</code>. Value: a Personal Access Token (or fine-grained PAT) with repo scope for PlanExeOrg/PlanExe-docs (or at least permission to trigger workflows in PlanExe-docs).</li> <li>Push to PlanExe <code>main</code> with changes under <code>docs/</code>. The workflow <code>.github/workflows/docs-update.yml</code> runs and sends <code>repository_dispatch</code> to PlanExe-docs; PlanExe-docs then checks out PlanExe, copies <code>docs/</code>, builds, and deploys.</li> </ol> <p>If the secret is not set, the \"Notify docs deploy\" workflow in PlanExe will fail at the dispatch step. You can still update the live site by running the Deploy Documentation workflow manually in PlanExe-docs (Actions \u2192 Deploy Documentation \u2192 Run workflow), or by pushing to PlanExe-docs <code>main</code>.</p>"},{"location":"AGENTS/#summary","title":"Summary","text":"<p>Edits in PlanExe/docs/ are what get published. PlanExe-docs orchestrates copy \u2192 MkDocs build \u2192 GitHub Pages deploy to docs.planexe.org. Push to PlanExe-docs <code>main</code>, trigger the Deploy workflow manually in PlanExe-docs, or set up <code>PLANEXE_DOCS_DISPATCH_TOKEN</code> in PlanExe so pushes to <code>docs/</code> auto-trigger the deploy.</p>"},{"location":"costs_and_models/","title":"Costs and models","text":"<p>PlanExe makes many LLM calls per plan. Model choice affects cost, speed, and quality.</p>"},{"location":"costs_and_models/#guidance","title":"Guidance","text":"<ul> <li>Most reliable: paid cloud models via OpenRouter.</li> <li>Lowest cost: older, smaller models (quality can drop).</li> <li>Local models: require strong hardware and are slower.</li> <li>Speed matters: tokens per second can be the difference between minutes and hours.</li> </ul>"},{"location":"costs_and_models/#typical-costs","title":"Typical costs","text":"<p>Costs vary by model and prompt size. PlanExe can use 100+ calls per plan, so avoid expensive models unless you need the highest quality.</p>"},{"location":"costs_and_models/#speed-and-iteration","title":"Speed and iteration","text":"<p>Fast models can complete a plan in roughly 10\u201320 minutes. Slow models may take hours. In practice, it is often better to iterate quickly and generate several candidate plans than to wait for one slow run.</p>"},{"location":"costs_and_models/#choosing-a-provider","title":"Choosing a provider","text":"<ul> <li>OpenRouter: easiest path for most users.</li> <li>Ollama / LM Studio: good for local experimentation.</li> </ul> <p>See the provider guides: - OpenRouter - Ollama - LM Studio</p>"},{"location":"deployment_hosting/","title":"Deployment and hosting","text":"<p>This page summarizes ways to deploy PlanExe.</p>"},{"location":"deployment_hosting/#local-recommended-for-most-users","title":"Local (recommended for most users)","text":"<ul> <li>Use Docker with the single\u2011user UI.</li> <li>See: Getting started</li> </ul>"},{"location":"deployment_hosting/#railway","title":"Railway","text":"<ul> <li>See the experimental guide: Railway</li> <li>Expect to tune env vars and ports for production use.</li> </ul>"},{"location":"deployment_hosting/#mcp-deployments","title":"MCP deployments","text":"<ul> <li>MCP server lives in <code>mcp_cloud</code>.</li> <li>Local proxy lives in <code>mcp_local</code>.</li> <li>Start with: MCP setup</li> </ul>"},{"location":"docker/","title":"PlanExe uses Docker","text":"<p>Docker is the supported way to run PlanExe locally and in most deployments. This page covers common Docker workflows and troubleshooting.</p>"},{"location":"docker/#basic-lifecycle","title":"Basic lifecycle","text":"<ul> <li>Stop everything: <code>docker compose down</code></li> <li>Build fresh (no cache) after code moves: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user</code></li> <li>Start services: <code>docker compose up</code></li> <li>Stop services (leave images): <code>docker compose down</code></li> <li>Build fresh and start services: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user &amp;&amp; docker compose up</code></li> </ul>"},{"location":"docker/#while-developing","title":"While developing","text":"<ul> <li>Live rebuild/restart on changes: <code>docker compose watch</code> (requires Docker Desktop 4.28+).   If watch misses changes after file moves, rerun the no-cache build above.</li> <li>View logs: </li> <li><code>docker compose logs -f worker_plan</code></li> <li><code>docker compose logs -f frontend_single_user</code></li> <li><code>docker compose logs -f frontend_multi_user</code></li> </ul>"},{"location":"docker/#run-individual-files","title":"Run individual files","text":"<ul> <li>Rebuild the worker image when code or data files change: <code>docker compose build --no-cache worker_plan</code>.</li> <li>Run a one-off module inside the worker image (same deps/env as the API): <code>docker compose run --rm worker_plan python -m worker_plan_internal.fiction.fiction_writer</code> (swap the module path as needed). If containers are already up, use <code>docker compose exec worker_plan python -m ...</code> instead.</li> <li>For host Ollama access, set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:11434</code> (default Ollama port). On Linux, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> if that hostname is missing, or use your bridge IP.</li> <li>Ensure required env vars (e.g., <code>DEFAULT_LLM</code>) are available via <code>.env</code> or your shell before running the command.</li> </ul>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the pipeline stops immediately with missing module errors, rebuild with <code>--no-cache</code> so new files are inside the images.</li> <li>If you change environment variables (e.g., <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT</code>), restart: <code>docker compose down</code> then <code>docker compose up</code>.</li> <li>If <code>frontend_multi_user</code> can't start because host port 5000 is busy, map it elsewhere: <code>export PLANEXE_FRONTEND_MULTIUSER_PORT=5001</code> (or another free port) before <code>docker compose up</code>.</li> <li>To clean out containers, network, and orphans: <code>docker compose down --remove-orphans</code>.</li> <li>To reclaim disk space when builds start failing with <code>No space left on device</code>:</li> <li>See current usage: <code>docker system df</code></li> <li>Aggressively prune (images, caches, networks not in use): <code>docker system prune -a</code><ul> <li>Expect a confirmation prompt; this removed ~37 GB here by deleting unused images and build cache.</li> </ul> </li> <li>If needed, prune build cache separately: <code>docker builder prune</code></li> </ul>"},{"location":"docker/#port-5432-already-in-use-postgres-conflict","title":"Port 5432 already in use (Postgres conflict)","text":"<p>If <code>database_postgres</code> fails to start with a \"port already in use\" error, another PostgreSQL is likely running on your machine. This is common on developer machines where you have: - macOS: Postgres.app (a popular menu-bar Postgres), Homebrew PostgreSQL (<code>brew install postgresql</code>), or pgAdmin's bundled server - Linux: System PostgreSQL installed via <code>apt install postgresql</code> or similar - Windows: PostgreSQL installer, pgAdmin, or other database tools</p> <p>Solution: Set <code>PLANEXE_POSTGRES_PORT</code> to a different value: <pre><code>export PLANEXE_POSTGRES_PORT=5433\ndocker compose up\n</code></pre></p> <p>This only affects the HOST port (how you access Postgres from your machine). Inside Docker, containers always connect to each other on port 5432\u2014this is hardcoded and unaffected by <code>PLANEXE_POSTGRES_PORT</code>.</p> <p>To make this permanent, add to your <code>.env</code> file: <pre><code>PLANEXE_POSTGRES_PORT=5433\n</code></pre></p> <p>When connecting from your host machine (e.g., DBeaver, <code>psql</code>), use the port you set: <pre><code>psql -h localhost -p 5433 -U planexe -d planexe\n</code></pre></p>"},{"location":"docker/#environment-notes","title":"Environment notes","text":"<ul> <li>The worker exports logs to stdout when <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT=true</code> (set in <code>docker-compose.yml</code>).</li> <li>Shared volumes: <code>./run</code> is mounted into both services; <code>.env</code> and <code>llm_config.json</code> are mounted read-only. Ensure they exist on the host before starting.***</li> <li>Database: Postgres runs in <code>database_postgres</code> and listens on host <code>${PLANEXE_POSTGRES_PORT:-5432}</code> mapped to container <code>5432</code>; data is persisted in the named volume <code>database_postgres_data</code>.</li> <li>Multiuser UI: binds to container port <code>5000</code>, exposed on host <code>${PLANEXE_FRONTEND_MULTIUSER_PORT:-5001}</code>.</li> <li>MCP server downloads: set <code>PLANEXE_MCP_PUBLIC_BASE_URL</code> so clients receive a reachable <code>/download/...</code> URL (defaults to <code>http://localhost:8001</code> in compose).</li> </ul>"},{"location":"docker/#host-opener-open-output-dir","title":"Host opener (Open Output Dir)","text":"<p>Because Docker containers cannot launch host apps, the <code>Open Output Dir</code> button needs a host-side service.</p> <p>Set these environment variables before starting: - <code>PLANEXE_OPEN_DIR_SERVER_URL</code> so the container can reach the host opener:   - macOS/Windows (Docker Desktop): <code>http://host.docker.internal:5100</code>   - Linux: <code>http://172.17.0.1:5100</code> (or add <code>host.docker.internal</code> pointing to the bridge IP). - <code>PLANEXE_HOST_RUN_DIR</code>: optional; defaults to <code>PlanExe/run</code> on the host. Set an absolute path if you relocate the run directory.</p> <p>1) Start host opener before Docker (on the host): <pre><code>cd open_dir_server\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -r requirements.txt\npython app.py\n</code></pre> 2) Provide <code>PLANEXE_OPEN_DIR_SERVER_URL</code> via your shell env, <code>.env</code>, or docker compose environment for <code>frontend_single_user</code>.</p>"},{"location":"docs_style_guide/","title":"Docs style guide","text":"<p>Short, consistent rules for PlanExe docs.</p>"},{"location":"docs_style_guide/#tone-and-voice","title":"Tone and voice","text":"<ul> <li>Be direct and practical.</li> <li>Be factual and specific.</li> <li>Avoid marketing terms like: quickstart, fastest.</li> <li>Avoid long personal stories in setup guides.</li> <li>Prefer short sentences.</li> </ul>"},{"location":"docs_style_guide/#structure","title":"Structure","text":"<ul> <li>Start with a 1\u20132 sentence summary.</li> <li>Use numbered steps for setup.</li> <li>End with \u201cNext steps\u201d or \u201cTroubleshooting.\u201d</li> </ul>"},{"location":"docs_style_guide/#terminology","title":"Terminology","text":"<ul> <li>Use plan for the output.</li> <li>Use report for the HTML artifact.</li> <li>Use MCP server for the PlanExe MCP service.</li> </ul>"},{"location":"docs_style_guide/#code-blocks","title":"Code blocks","text":"<ul> <li>Use fenced code blocks with language when possible.</li> <li>Keep commands copy\u2011paste friendly.</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-planexe-a-finished-plan","title":"Is PlanExe a finished plan?","text":"<p>No. It creates a strong draft and structure, but the final plan still needs human review and refinement.</p>"},{"location":"faq/#how-long-does-a-plan-take","title":"How long does a plan take?","text":"<p>Depends on provider and model. Expect minutes, not seconds.</p>"},{"location":"faq/#what-does-it-cost","title":"What does it cost?","text":"<p>Costs depend on the model. Paid cloud models are more reliable; local models are slower but can be cheaper.</p>"},{"location":"faq/#why-is-the-output-vague-sometimes","title":"Why is the output vague sometimes?","text":"<p>Prompts that are too short or unclear usually produce vague output. Use the Prompt writing guide.</p>"},{"location":"faq/#can-i-run-it-locally","title":"Can I run it locally?","text":"<p>Yes. Follow Getting started to run with Docker.</p>"},{"location":"faq/#where-are-the-outputs-saved","title":"Where are the outputs saved?","text":"<p>On local runs, outputs are written to <code>run/</code> in the repo root.</p>"},{"location":"getting_started/","title":"Getting started with PlanExe","text":"<p>This guide shows new users how to launch the <code>frontend_single_user</code> UI with Docker using OpenRouter as the LLM provider. No local Python or pip setup is needed.</p>"},{"location":"getting_started/#1-prerequisites","title":"1. Prerequisites","text":"<p>Install Docker.</p> <p>Create an account on OpenRouter and top up around 5 USD in credits (paid models works, the free models are unreliable). It cost around 0.1 USD to generate a plan, when using PlanExe's default settings.</p>"},{"location":"getting_started/#2-clone-the-repo","title":"2. Clone the repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre>"},{"location":"getting_started/#3-configure-secrets","title":"3. Configure secrets","text":"<p>Copy <code>.env.docker-example</code> to <code>.env</code>.</p> <p>Add your OpenRouter key: <pre><code>OPENROUTER_API_KEY='sk-or-v1-your-key'\n</code></pre></p>"},{"location":"getting_started/#4-start-the-single-user-stack","title":"4. Start the single-user stack","text":"<pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> <p>Wait for http://localhost:7860 to become available.</p> <p>Stop with <code>Ctrl+C</code>.</p>"},{"location":"getting_started/#5-use-the-ui","title":"5. Use the UI","text":"<p>Open http://localhost:7860 in your browser. </p> <p>You can now submit your prompt.</p> <p>The generated plans are written to <code>run/&lt;timestamped-output-dir&gt;</code>.</p> <p></p>"},{"location":"getting_started/#verification","title":"Verification","text":"<ul> <li>You can open the UI at http://localhost:7860.</li> <li>A plan run creates a new folder in <code>run/</code>.</li> </ul>"},{"location":"getting_started/#troubleshooting-and-next-steps","title":"Troubleshooting and next steps","text":"<ul> <li>For Docker tips, see docker.md.</li> <li>For OpenRouter-specific notes, see openrouter.md.</li> <li>If the UI fails to load or plans don\u2019t start, check worker logs: <code>docker compose logs -f worker_plan</code>.</li> <li>Learn how to write better prompts: Prompt writing guide</li> </ul>"},{"location":"getting_started/#community","title":"Community","text":"<p>Need help? Join the PlanExe Discord.</p>"},{"location":"install_developer/","title":"Installing PlanExe for developers","text":"<p>I assume that you are a python developer.</p> <p>You need several open terminals to do development on this project.</p>"},{"location":"install_developer/#clone-repo","title":"Clone repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\n</code></pre>"},{"location":"install_developer/#prepare-env-file","title":"Prepare <code>.env</code> file","text":"<p>Create a <code>.env</code> file from the <code>.env.developer-example</code> file.</p> <p>Update <code>OPENROUTER_API_KEY</code> with your open router api key.</p>"},{"location":"install_developer/#open_dir_server","title":"<code>open_dir_server</code>","text":"<p>In a new terminal:  Follow the open_dir_server instructions.</p>"},{"location":"install_developer/#worker_plan","title":"<code>worker_plan</code>","text":"<p>In a new terminal:  Follow the worker_plan instructions.</p>"},{"location":"install_developer/#frontend_single_user","title":"<code>frontend_single_user</code>","text":"<p>In a new terminal:  Follow the frontend_single_user instructions.</p>"},{"location":"install_developer/#database_postgres","title":"<code>database_postgres</code>","text":"<p>In a new terminal:  Follow the database_postgres instructions.</p>"},{"location":"install_developer/#worker_plan_database","title":"<code>worker_plan_database</code>","text":"<p>In a new terminal:  Follow the worker_plan_database instructions.</p>"},{"location":"install_developer/#frontend_multi_user","title":"<code>frontend_multi_user</code>","text":"<p>In a new terminal:  Follow the frontend_multi_user instructions.</p>"},{"location":"install_developer/#tests","title":"Tests","text":"<p>In a new terminal:  Run the tests to ensure that the project works correctly. <pre><code>PROMPT&gt; python test.py\nsnip lots of output snip\nRan 117 tests in 0.059s\n\nOK\n</code></pre></p>"},{"location":"install_developer/#now-planexe-have-been-installed","title":"Now PlanExe have been installed.","text":""},{"location":"llm_config/","title":"LLM config (llm_config.json)","text":"<p>This file defines which LLM providers and models PlanExe can use. Each top\u2011level key is a model id used in the UI and pipeline.</p> <p><code>llm_config.json</code> lives in the PlanExe repo root and is read at runtime. Environment variables are substituted from <code>.env</code>.</p>"},{"location":"llm_config/#file-structure","title":"File structure","text":"<pre><code>{\n  \"model-id\": {\n    \"comment\": \"Human description\",\n    \"priority\": 1,\n    \"luigi_workers\": 4,\n    \"class\": \"OpenRouter\",\n    \"arguments\": {\n      \"model\": \"google/gemini-2.0-flash-001\",\n      \"api_key\": \"${OPENROUTER_API_KEY}\",\n      \"temperature\": 0.1,\n      \"timeout\": 60.0,\n      \"is_function_calling_model\": false,\n      \"is_chat_model\": true,\n      \"max_tokens\": 8192,\n      \"max_retries\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"llm_config/#top-level-fields","title":"Top-level fields","text":"<ul> <li>comment: Plain\u2011text description for humans. Optional.</li> <li>priority: Lower number = higher priority when <code>auto</code> is selected. Optional.</li> <li>luigi_workers: Number of Luigi workers used for this model. Use <code>1</code> for local models (Ollama/LM Studio).</li> <li>class: Provider class name (e.g., <code>OpenRouter</code>, <code>OpenAI</code>, <code>Ollama</code>, <code>LMStudio</code>, <code>OpenAILike</code>).</li> <li>arguments: Provider\u2011specific settings passed to the LLM client.</li> </ul>"},{"location":"llm_config/#common-arguments","title":"Common arguments","text":"<p>These keys are common across most providers:</p> <ul> <li>model / model_name: Provider model identifier.</li> <li>api_key: API key reference (usually <code>${ENV_VAR}</code>).</li> <li>base_url / api_base: Override the provider base URL.</li> <li>temperature: Controls randomness. Lower is more deterministic.</li> <li>timeout / request_timeout: Max time per request in seconds.</li> <li>max_tokens / max_completion_tokens: Output token limit (provider specific).</li> <li>max_retries: Retry count on transient errors.</li> <li>is_function_calling_model: Whether the model supports structured/tool output.</li> <li>is_chat_model: Whether the model uses chat format.</li> </ul>"},{"location":"llm_config/#choosing-values","title":"Choosing values","text":"<ul> <li>Use luigi_workers = 1 for local models (Ollama / LM Studio).</li> <li>Use luigi_workers &gt; 1 for cloud models if you want parallel tasks.</li> <li>Keep timeout higher for slower models.</li> </ul>"},{"location":"llm_config/#notes","title":"Notes","text":"<ul> <li>If <code>llm_config.json</code> is missing, PlanExe logs a warning and proceeds with defaults.</li> <li>Changes to <code>llm_config.json</code> require a container restart (or rebuild if baked into the image).</li> </ul>"},{"location":"plan/","title":"Future plan for PlanExe","text":"<p>Using the \"5 Whys\" method:</p> <p>I want multiple-agents talking with each other, via zulip.</p> <p>Why?</p> <p>Can I \u201cexecute\u201d a plan from start to finish, with the agents doing all the labor. Zulip is open source. So I\u2019m not dependent on Discord/Teams/Slack.</p> <p>Why?</p> <p>When humans are doing the labor, they have to decompose the problem into tasks.  In my experience with PlanExe, AI can decompose sometimes better/worse than humans. Possible via MCP to interface with issue tracker. delegate parts of the plan to humans.</p> <p>Why?</p> <p>Companies spend lots of effort on planning and getting the right people to communicate with meetings, emails. Something I dislike about working in coding jobs. Wasting time and money on planning.</p> <p>Why?</p> <p>Cut cost and optimize speed.</p> <p>Why?</p> <p>To satisfy my own curiosity. I\u2019m curious to what kind of outcome it is. An AI organization/company, possible distributed network. Is it a global organism as seen in scifi movies that are controlled by AIs, that takes power away from politicians. My concerns are: will it be able to adapt to a changing world. Re-plan in real-time when a shipment is delayed, a machine breaks down, or an unexpected storm hits. quiet, compounding errors, security oversights, and cost blowouts.</p>"},{"location":"plan/#execute-the-plan","title":"Execute the plan","text":"<p>Currently it's up to humans to execute a plan. How can this be automated?</p> <p>Ideally take an entire plan and go with it.</p>"},{"location":"plan/#improve-plan","title":"Improve plan","text":"<p>Prompt optimizing with A/B testing: Make tiny tweaks to one system prompt at a time, and see how it compares to baseline. If most generated plans gets improved, then keep the new system prompt. Verify across multiple LLMs/reasoning models, that the new system prompt makes an improvement. Store the new system prompt in the repo. Find weaknesses that are common for the generated plans. Pick the earliest task in the pipeline that impact this weakness. Schedule this weakness for the next A/B test improvement iteration.</p> <p>Boost initial prompt: The <code>initial prompt</code> has the biggest impact on the generated plan, if it's bad then the final plan is bad. If it's well written, concise, there is a higher chance for a realistic/feasible plan. Currently I use AIs to write the initial prompt for me by first having a long conversation about the topic, and showing examples of other initial prompts that have worked well in the past. It may by a small tweak to the initial prompt and it yields a better plan. It may be an entire rewrite of the initial prompt. The user may have specified a vague prompt, or the user may not be domain expert, the prompt may be non-sense, or the prompt may be overly specific so PlanExe attends to the wrong things. Suggest changes to the initial prompt. This can be by picking a bigger budget, a different technology, a different set of levers, fixing typos.</p> <ul> <li>User specifies a budget of 0..100 USD. Which is unrealistic, when the plan is to hire a team, and work on it for months.</li> <li>User leaves out physical location(s). So PlanExe picks a random location in a different part of the world.</li> </ul> <p>Dynamic plugins: Have AI's rewrite PlanExe as they see fit, depending on what the user have prompted it with. So if it's a software project, it writes PlanExe plugins that are going to be needed. And then proceeds to creating the plan. In the middle of the plan creation, it may be necessary to create more PlanExe plugins as issues shows up.</p> <p>Grid search: Currently PlanExe only generates a plan for 1 permutation of levers. A plan may have 10 levers with 3-5 settings. Here it could be interesting to create  100 full plans, each with a different combination of levers. Compare the generated plans against each other  and pick the most 3 promising plans.</p> <p>Multiple refinements: Currently PlanExe generates the first iteration of the plan. Usually issues arises when making the first iteration, that have to be incorporated into the timeline. In the future I want to do multiple iterations, until the plan is of a reasonable quality.</p> <p>Validate the plan with deep research: Currently there is no validation. It's up to humans to be skeptic about the plan, does this make sense, check everything. There may be issues with: assumptions, numbers, flaws.</p> <p>Money: Currently the LLMs make up numbers. Alternate between these: Tweak the plan. Tweak the budget. Repeat. Obtain latest market data. Obtain info about what resources the user has available. Populate a Cost-Breakdown-Structure.</p> <p>Gantt in parallel: Currently the Gantt is waterfall. For a team with several people it's possible to do tasks in parallel. Obtain info about what resources the user has available, and if they are willing to do tasks in parallel.</p>"},{"location":"plan/#secondary-issues","title":"Secondary issues","text":""},{"location":"plan/#luigi-can-run-tasks-in-parallel","title":"Luigi can run tasks in parallel","text":"<p>I'm not making use of it. </p> <p>Until 2026-jan-01 I had this limitation: The PythonAnywhere doesn't like long running child processes/threads, anything longer than 5 minutes gets killed. There are always-on workers, but these must not spawn long running processes/threads. I'm considering finding another provider. Starting from 2026-jan-01 I\u2019m using Docker and no longer using pythonanywhere, I can start looking into running parallel tasks within Luigi.</p>"},{"location":"plan/#mcp-on-railway","title":"MCP on Railway","text":"<p>Doing inference in the cloud cost money. If users are to use MCP in the cloud, they will have to pay for it.</p> <ul> <li>Scenario A: Users can buy credit via <code>PLANEXE_MCP_API_KEY</code>.</li> <li>Scenario B: Users can BYOK (Bring your own key).</li> </ul> <p>Both scenarios will need user management with login.</p>"},{"location":"plan/#tertiary-issues","title":"Tertiary issues","text":""},{"location":"plan/#capture-reasoning-response","title":"Capture reasoning response","text":"<p>Currently I only capture the final response, without any reasoning. I want to capture the reasoning, since it may be helpful for troubleshooting. Or for other AIs to assess the reasoning steps leading up to the response.</p>"},{"location":"plan/#token-counting","title":"Token counting","text":"<p>So that I can see how much does it cost to generate a plan. Reasoning models. How many tokens are spent on reasoning vs generating the final response.</p>"},{"location":"plan/#debugging","title":"Debugging","text":"<p>Get step-by-step debugging working again. Now that I have switched to Docker, I have multiple python projects in the same repo, that use different incompatible packages.</p>"},{"location":"plan/#github-ci-that-runs-tests","title":"GitHub CI that runs tests","text":"<p>The hard thing is getting the venv's working.</p>"},{"location":"plan/#table-of-content","title":"Table of content","text":"<p>Currently the generated report has expandable/collapsible sections. There is an overwhelming amount of content inside each sections. I'm considering having a table of content in the left sidebar, similar to this: Railway Dockerfiles guide It uses Docusaurus which uses React. I'm no fan of React. I'm considering using mkdocs instead.</p>"},{"location":"plan/#eliminate-redundant-user-prompts-in-the-log-file","title":"Eliminate redundant user prompts in the log file","text":"<p>Get rid of some of the many user prompt logging statements, so the log.txt is less noisy. These user prompts are saved to the <code>track_activity.jsonl</code> file already. So having them in the log.txt is redundant.</p>"},{"location":"plan/#ssl-when-connecting-with-the-database","title":"SSL when connecting with the database","text":"<p>I can't afford the pro plan to have a dedicated Postgres server. Currently when connecting to Railway, it's via a TCP Proxy and it's unencrypted. Either upgrade to pro, or use SSL certificates within the \"database_postgres\" Dockerfile.</p>"},{"location":"plan_output_anatomy/","title":"Plan output anatomy","text":"<p>PlanExe produces a single HTML report. This page explains what each section is for and how to use it.</p>"},{"location":"plan_output_anatomy/#executive-summary","title":"Executive summary","text":"<ul> <li>Use it to validate scope, deliverables, and assumptions.</li> <li>If this is wrong, the rest will be wrong too.</li> </ul>"},{"location":"plan_output_anatomy/#gantt-chart","title":"Gantt chart","text":"<ul> <li>A draft timeline with dependencies.</li> <li>Validate durations and sequencing with domain experts.</li> </ul>"},{"location":"plan_output_anatomy/#governance-structure","title":"Governance structure","text":"<ul> <li>Roles, decision rights, and accountability.</li> <li>Useful for stakeholder alignment early.</li> </ul>"},{"location":"plan_output_anatomy/#risk-register","title":"Risk register","text":"<ul> <li>A first pass at risks and mitigations.</li> <li>Expect to add domain\u2011specific risks.</li> </ul>"},{"location":"plan_output_anatomy/#swot-analysis","title":"SWOT analysis","text":"<ul> <li>A strategic snapshot of strengths/weaknesses/opportunities/threats.</li> <li>Good for framing strategy, not for execution details.</li> </ul>"},{"location":"plan_output_anatomy/#next-steps","title":"Next steps","text":"<ul> <li>A prioritized list of actions to start implementation.</li> <li>Use it to build a real project backlog.</li> </ul>"},{"location":"plan_output_anatomy/#how-to-use-the-plan","title":"How to use the plan","text":"<ol> <li>Validate scope (Executive summary).</li> <li>Correct timelines (Gantt).</li> <li>Stress\u2011test risks with stakeholders.</li> <li>Refine budgets (PlanExe budgets are headline\u2011only).</li> <li>Turn next steps into tickets.</li> </ol>"},{"location":"prompt_writing_guide/","title":"Prompt writing guide","text":"<p>PlanExe creates better plans when the input prompt is detailed and specific. Aim for 300\u2013800 words.</p>"},{"location":"prompt_writing_guide/#what-a-good-prompt-includes","title":"What a good prompt includes","text":"<ul> <li>Goal and scope: what you want, and what you explicitly do not want.</li> <li>Audience: who the plan is for (customers, users, stakeholders).</li> <li>Constraints: budget, timeline, geography.</li> <li>Location(s): country/region/city. If you have it, include the exact street address. Regulations and feasibility change by location.</li> <li>Success criteria: what success looks like, and how it is measured.</li> <li>Resources: team size, skills, existing assets, tools.</li> </ul>"},{"location":"prompt_writing_guide/#good-vs-weak-prompts","title":"Good vs. weak prompts","text":"<p>Weak</p> <p>Construct a bridge.</p> <p>Better</p> <p>Construct a bridge between Spain and Morocco across the Strait of Gibraltar. Target a feasibility\u2011to\u2011groundbreak timeline of 5 years with a total budget range of 8\u201312B EUR. The plan should cover geotechnical surveys, environmental impact assessments, maritime traffic coordination, and cross\u2011border permitting. Include options for rail + road, and describe staging for phase 1 (single rail) and phase 2 (road expansion). Exclude toll\u2011system design and focus on structural, logistics, and governance planning.</p>"},{"location":"prompt_writing_guide/#recommended-structure","title":"Recommended structure","text":"<pre><code>Goal:\n\nContext / background:\n\nTarget users / customers:\n\nScope (in/out):\n\nConstraints (budget, timeline, geography):\n\nLocation(s):\n\nSuccess criteria:\n\nTeam / resources available:\n</code></pre>"},{"location":"prompt_writing_guide/#budget-and-money","title":"Budget and money","text":"<p>Budget can be natural language. Examples:</p> <ul> <li>A range: \u201c$200k\u2013$400k total.\u201d</li> <li>Phased: \u201c$10M for phase 1, $5M for phase 2.\u201d</li> <li>With constraints: \u201cCapex only, exclude staffing.\u201d</li> <li>Use standard currencies (EUR, DKK, RUB, BRL, etc.). Crypto budgets (BTC, ETH) are not supported.</li> </ul>"},{"location":"prompt_writing_guide/#budget-mistakes-to-avoid","title":"Budget mistakes to avoid","text":"<ul> <li>Setting the budget to 0/none/N/A when the goal is serious and requires resources.</li> <li>Using a currency code as the budget value (e.g., budget = DKK) instead of a real budget.</li> </ul>"},{"location":"prompt_writing_guide/#common-mistakes-to-avoid","title":"Common mistakes to avoid","text":"<ul> <li>Being too vague (one sentence).</li> <li>Missing constraints (budget, time, scope).</li> <li>Leaving out the location(s), which makes regulations and feasibility assumptions unreliable.</li> <li>Conflicting requirements (e.g., \u201claunch in 2 weeks\u201d + \u201centerprise compliance\u201d).</li> </ul>"},{"location":"railway/","title":"PlanExe on Railway - Experimental","text":"<p>As of 2026-Jan-04, I'm experimenting with Railway. Currently the <code>frontend_multi_user</code> UI is an ugly MVP. I recommend going with the <code>frontend_single_user</code>, that doesn't use database.</p> <p>In this project, the files named <code>railway.md</code> or <code>railway.toml</code>, are related to how things are configured in my Railway setup.</p>"},{"location":"railway/#project-settings","title":"Project Settings","text":""},{"location":"railway/#environments","title":"Environments","text":"<p>Create these environments: - <code>production</code> - <code>staging</code></p>"},{"location":"railway/#shared-variables-production","title":"Shared variables - production","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than staging\n</code></pre>"},{"location":"railway/#shared-variables-staging","title":"Shared variables - staging","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than production\n</code></pre>"},{"location":"railway/#using-shared-variables-in-services","title":"Using Shared Variables in Services","text":"<p>Each service that connects to the database must reference the shared password variable in its own environment variables.</p> <p>In Railway, go to each service \u2192 Variables and add:</p> <pre><code>PLANEXE_POSTGRES_PASSWORD=\"${{shared.PLANEXE_POSTGRES_PASSWORD}}\"\n</code></pre> <p>Services that need this variable: - <code>database_postgres</code> - <code>frontend_multi_user</code> - <code>worker_plan_database</code></p> <p>This ensures all services use the same password, and you only need to update it in one place (the shared variables) when rotating credentials.</p>"},{"location":"recent_changes/","title":"Recent Changes in PlanExe","text":""},{"location":"recent_changes/#2025-dec-31","title":"2025-dec-31","text":"<p>PlanExe is now using Docker.</p> <p>So you no longer have to be python developer to install it on your own computer.</p> <p>Over the last month I have migrated PlanExe to Docker.</p> <p>So that I can deploy PlanExe on Railway and similar web providers.</p> <p>Previously I have been using PythonAnywhere, and I was stuck in a dependency hell, where I couldn't add packages without breaking other packages.</p> <p>Now with docker, I don't have these incompatibility issues. However docker have its own issues.</p> <p>The last version BEFORE the transition to docker is available here: PlanExe 2025-dec-31 release</p> <p>The main branch will be docker from now on: PlanExe main branch</p>"},{"location":"start_here/","title":"Start here","text":"<p>Pick the path that matches your goal. Each path is short and points to a deeper guide.</p>"},{"location":"start_here/#path-1-i-want-to-use-planexe-without-local-setup","title":"Path 1: I want to use PlanExe without local setup","text":"<ol> <li>Skim a real report: Minecraft Escape sample report</li> <li>Open the web app and create a report: planexe.org</li> <li>If you like the output, go local for full control: Getting started</li> </ol>"},{"location":"start_here/#path-2-i-want-to-run-planexe-locally-docker","title":"Path 2: I want to run PlanExe locally (Docker)","text":"<ol> <li>Follow: Getting started</li> <li>Learn the Docker lifecycle: Docker</li> <li>Pick an AI provider: OpenRouter (recommended)</li> </ol>"},{"location":"start_here/#path-3-i-want-to-integrate-planexe-via-mcp","title":"Path 3: I want to integrate PlanExe via MCP","text":"<ol> <li>Read the overview: MCP welcome</li> <li>Follow the setup guide: MCP setup</li> <li>See tool details: MCP details</li> </ol>"},{"location":"start_here/#path-4-i-want-to-develop-on-planexe","title":"Path 4: I want to develop on PlanExe","text":"<ol> <li>Install the dev setup: Developer install</li> <li>Read a component doc to understand the architecture: Open dir server</li> <li>Use Docker for local services: Docker</li> </ol>"},{"location":"statistics/","title":"PlanExe Statistics","text":""},{"location":"statistics/#github-stars","title":"GitHub stars","text":"<p>The badge above shows the current star count for PlanExeOrg/PlanExe and links to the repository.</p>"},{"location":"statistics/#star-history","title":"Star history","text":"<p>The chart above shows star count over time. Click it to open the interactive chart on star-history.com.</p>"},{"location":"statistics/#commit-history","title":"Commit history","text":"<p>PlanExe commit activity \u2014 commits per week over the last year (GitHub Insights).</p>"},{"location":"statistics/#activity-on-openrouter","title":"Activity on OpenRouter","text":"<p>OpenRouter shows app analytics (usage, top models) for apps that use their API. PlanExe was originally under a personal GitHub account and is now under the PlanExeOrg organization, so there are two app pages:</p> <ul> <li>Current (PlanExeOrg): PlanExe on OpenRouter \u2014 PlanExeOrg/PlanExe</li> <li>Legacy (neoneye): PlanExe on OpenRouter \u2014 neoneye/PlanExe</li> </ul>"},{"location":"statistics/#discord","title":"Discord","text":"<p>PlanExe community server (ID <code>1337721703534690317</code>).</p> <ul> <li>Join: planexe.org/discord</li> <li>Widget JSON API: discord.com/api/guilds/1337721703534690317/widget.json \u2014 returns server name, invite, channels, members online, and <code>presence_count</code></li> </ul> <p>Live widget:</p>"},{"location":"troubleshooting_stuck_pipeline/","title":"Troubleshooting a stuck pipeline","text":"<p>The gradio app (<code>app_text2plan.py</code>) starts the <code>run_plan_pipeline</code> process via a <code>Popen</code> call. </p> <ul> <li>Environment, if the gradio app runs in a slightly different environment than when running via commandline <code>python -m worker_plan_internal.plan.run_plan_pipeline</code>, then the child process may behave differently. I have verified that the parent process and child process runs with the same environment variables.</li> <li>Buffering, if the parent process isn't reading stdout/stderr fast enough, the child process may freeze. I have reworked the <code>Popen</code> code so the stdout/stderr goes to <code>/dev/null</code>.</li> <li>Other issues, if the pipeline still hangs, let me know, it may be some issue I'm not aware of.</li> </ul>"},{"location":"troubleshooting_stuck_pipeline/#manually-resuming-a-stuck-pipeline","title":"Manually resuming a stuck pipeline","text":"<p>In the UI copy/paste the run_id that is stuck, eg: <code>20250209_030626</code></p> <p>Insert it on commandline, and run the pipeline, like this:</p> <pre><code>PROMPT&gt; RUN_ID=20250209_030626 python -m worker_plan_internal.plan.run_plan_pipeline\n</code></pre>"},{"location":"troubleshooting_stuck_pipeline/#why-does-the-pipeline-get-stuck","title":"Why does the pipeline get stuck?","text":"<p>The <code>log.txt</code> contains the output from the logger with <code>DEBUG</code> level, the most detailed. Alas the <code>log.txt</code> have little info about what exactly went wrong.  The exceptions rarely have useful info.</p> <ul> <li>Censorship, if it's a sensitive topic, then the LLM may refuse to answer.</li> <li>Timeout, that happens often when using AI providers in the cloud.</li> <li>Invalid json, responds from the server that doesn't adhere to the json schema. Too high a temperature setting may cause the LLM to be too creative and diverge from the json schema. Try use a lower temperature.</li> <li>Too long answer, if the respond from the server gets too long so it gets truncated, so it's invalid json.</li> <li>Other, there may be other reasons that I'm not aware of, please let me know if you encounter such a scenario.</li> </ul>"},{"location":"ai_providers/lm_studio/","title":"Using PlanExe with LM Studio","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>LM Studio is an open source app for macOS, Windows, and Linux for running LLMs on your own computer. It is useful for local troubleshooting.</p> <p>PlanExe processes more text than regular chat. You will need capable hardware to run an LLM at a reasonable speed.</p>"},{"location":"ai_providers/lm_studio/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install LM Studio on your host and download a small model inside LM Studio (e.g. <code>Qwen2.5-7B-Instruct-1M</code>, ~4.5 GB).</li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for LM Studio) and use the <code>lmstudio-...</code> entry in <code>llm_config.json</code>, setting <code>base_url</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Linux bridge IP.</li> <li>Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</li> </ol>"},{"location":"ai_providers/lm_studio/#host-only-no-docker","title":"Host-only (no Docker)","text":"<p>For advanced users: use the host entry (e.g. <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://127.0.0.1:1234</code>. Start your preferred PlanExe runner (e.g. a local Python environment) and ensure the LM Studio server is running before you submit jobs.</p>"},{"location":"ai_providers/lm_studio/#configuration","title":"Configuration","text":"<p>In <code>llm_config.json</code>, find a config that starts with <code>lmstudio-</code> (e.g. <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>). In LM Studio, find the model with that exact id and download it. The Qwen model is on Hugging Face (~4.5 GB).</p> <p>In LM Studio, go to the Developer page (Cmd+2 / Ctrl+2 / Windows+2), start the server, and confirm the UI shows Status: Running and Reachable at: http://127.0.0.1:1234.</p>"},{"location":"ai_providers/lm_studio/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with a ~7B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>Structured output matters: not all models return clean structured output. If you see malformed or JSON errors, try a nearby model or quantization.</li> </ul>"},{"location":"ai_providers/lm_studio/#run-lm-studio-locally-with-docker","title":"Run LM Studio locally with Docker","text":"<p>Containers cannot reach <code>127.0.0.1</code> on your host. Set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Docker bridge IP on Linux (often <code>http://172.17.0.1:1234</code>). On Linux, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code> if that hostname is missing.</p> <p>To find your bridge IP on Linux:</p> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <p>If <code>docker0</code> is missing (e.g. with Podman or alternate bridge names), inspect the default bridge gateway:</p> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <p>Example <code>llm_config.json</code> entry (add <code>base_url</code> when using Docker):</p> <pre><code>\"lmstudio-qwen2.5-7b-instruct-1m\": {\n    \"comment\": \"Runs via LM Studio on the host; PlanExe in Docker points to the host LM Studio server.\",\n    \"class\": \"LMStudio\",\n    \"arguments\": {\n        \"model_name\": \"qwen2.5-7b-instruct-1m\",\n        \"base_url\": \"http://host.docker.internal:1234/v1\",\n        \"temperature\": 0.2,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <p>After editing <code>llm_config.json</code>, rebuild or restart the worker and frontends: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config).</p>"},{"location":"ai_providers/lm_studio/#troubleshooting","title":"Troubleshooting","text":"<p>When you click Submit in PlanExe, a new output directory is created containing <code>log.txt</code>. Open that file and scroll to the bottom for error messages.</p> <p>Report issues on Discord. Include system info (e.g. \u201cI\u2019m on macOS with M1 Max, 64 GB\u201d).</p> <p>Where to look for logs:</p> <ul> <li>Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container).</li> <li>Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to LM Studio).</li> <li>Structured-output failures: If you see JSON/parse errors or malformed output in <code>log.txt</code>, try a different model or quantization; not all models return structured output cleanly.</li> </ul>"},{"location":"ai_providers/lm_studio/#run-lm-studio-on-a-remote-computer","title":"Run LM Studio on a remote computer","text":"<p>Use a secure tunnel instead of exposing the server directly. From your local machine:</p> <pre><code>ssh -N -L 1234:localhost:1234 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:1234</code> while the tunnel is running.</p>"},{"location":"ai_providers/lm_studio/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/mistral/","title":"Using PlanExe with Mistral","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>If you want to use Mistral, OpenRouter has several mistral models. </p>"},{"location":"ai_providers/mistral/#docker-setup-for-mistral","title":"Docker setup for Mistral","text":"<p>Mistral support is not baked into the Docker image by default. You must add the Mistral LlamaIndex extension to the worker, rebuild the image, and supply your API key.</p> <ol> <li>Install Docker (with Docker Compose), then clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre></li> <li>Enable the Mistral client inside the worker image by editing <code>worker_plan/pyproject.toml</code>. Under <code>[project].dependencies</code>, add or uncomment these lines: <pre><code>\"llama-index-llms-mistralai==0.4.0\",\n\"mistralai==1.5.2\",\n</code></pre>    Without this step, the Docker image will not have the <code>MistralAI</code> class.</li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> and add your key: <pre><code>MISTRAL_API_KEY='INSERT-YOUR-SECRET-KEY-HERE'\n</code></pre></li> <li>Add (or keep) a Mistral entry in <code>llm_config.json</code> (example below).</li> <li>Rebuild the images so the new dependencies are baked in: <pre><code>docker compose build --no-cache worker_plan frontend_single_user\n</code></pre></li> <li>Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> 7) Open http://localhost:7860, go to Settings, and pick your Mistral model (e.g., <code>mistral-paid-large</code>). If you later tweak only <code>llm_config.json</code>, just restart the containers (<code>docker compose restart worker_plan frontend_single_user</code>); rebuilds are only needed when dependencies change.</li> </ol>"},{"location":"ai_providers/mistral/#why-use-mistral","title":"Why use Mistral?","text":"<p>Mistral can have run your own fine tuned model in the cloud. If you have sensitive business data that you don't want to share with the world, then this is one way to do it.</p> <p>Create an account on the mistral.ai website and buy 10 EUR of credits.</p> <p>List of available models.</p> <p>Using the free models, and the API is rate limited to 1 request per second. PlanExe cannot deal with rate limiting and PlanExe does 70-100 requests, so it's likely going to yield errors.</p>"},{"location":"ai_providers/mistral/#create-api-key","title":"Create API key","text":"<ol> <li>Visit api-keys.</li> <li>Click <code>Create new key</code> and name the new key <code>PlanExe</code>.</li> <li>In the <code>.env</code> file in the root dir of the PlanExe repo, create a row named <code>MISTRAL_API_KEY</code>. Copy/paste the newly created api key into that row.</li> </ol> <p>The <code>.env</code> file should look something like the following, with your own key inserted. <pre><code>MISTRAL_API_KEY='AWkg3SxFTLWaPJClbASfv9h3VPItroof'\n</code></pre></p>"},{"location":"ai_providers/mistral/#edit-the-llm_configjson","title":"Edit the <code>llm_config.json</code>","text":"<p>The JSON should look something like this:</p> <pre><code>{\n    \"mistral-paid-large\": {\n        \"comment\": \"This is paid. Possible free to use for a limited time. Check the pricing before use.\",\n        \"class\": \"MistralAI\",\n        \"arguments\": {\n            \"model\": \"mistral-large-latest\",\n            \"api_key\": \"${MISTRAL_API_KEY}\",\n            \"temperature\": 1.0,\n            \"timeout\": 60.0,\n            \"max_tokens\": 8192,\n            \"max_retries\": 5\n        }\n    }\n}\n</code></pre>"},{"location":"ai_providers/mistral/#use-the-mistral-model","title":"Use the Mistral model","text":"<ol> <li>Restart PlanExe</li> <li>Go to the <code>Settings</code> tab</li> <li>Select the <code>mistral-paid-large</code> model.</li> </ol>"},{"location":"ai_providers/mistral/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/ollama/","title":"Using PlanExe with Ollama","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>Ollama is an open source app for macOS/Windows/Linux for running LLMs on your own computer (or on a remote computer).</p> <p>PlanExe processes more text than regular chat. You will need expensive hardware to run a LLM at a reasonable speed.</p>"},{"location":"ai_providers/ollama/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install Ollama on your host and pull a small model: <code>ollama run llama3.1</code> (downloads ~4.9 GB and proves the host service works).  </li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for Ollama) and pick the Docker entry in <code>llm_config.json</code> (snippet below) so <code>base_url</code> points to <code>http://host.docker.internal:11434</code> (Docker Desktop) or your Linux bridge IP.  </li> <li>Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</li> </ol>"},{"location":"ai_providers/ollama/#host-only-no-docker-for-advanced-users","title":"Host-only (no Docker) \u2014 for advanced users","text":"<ul> <li>Use the host entry (e.g., <code>\"ollama-llama3.1\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://localhost:11434</code>.</li> <li>Start your preferred PlanExe runner (e.g., a local Python environment) and ensure Ollama is already running on the host before you submit jobs.</li> </ul>"},{"location":"ai_providers/ollama/#configuration","title":"Configuration","text":"<p>In the <code>llm_config.json</code> find a config that starts with <code>ollama-</code> such as <code>\"ollama-llama3.1\"</code> (host) or <code>\"docker-ollama-llama3.1\"</code> (Docker). Use the <code>docker-</code> entry when PlanExe runs in Docker so requests reach the host.</p> <p>On the Ollama Search Models website. Find the corresponding model. Go to the info page for the model: ollama/library/llama3.1. The info page shows how to install the model on your computer, in this case <code>ollama run llama3.1</code>. To get started, go for a <code>8b</code> model that is <code>4.9GB</code>.</p>"},{"location":"ai_providers/ollama/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with an 8B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>If you need faster responses, move to a bigger GPU box or use a cloud model via OpenRouter instead of upsizing Ollama locally.</li> </ul>"},{"location":"ai_providers/ollama/#run-ollama-locally-with-docker","title":"Run Ollama locally with Docker","text":"<ul> <li>Make sure the container can reach Ollama on the host. On macOS/Windows (Docker Desktop) use the preconfigured entry in <code>llm_config.json</code> (snippet below) with <code>base_url</code> pointing to <code>http://host.docker.internal:11434</code>. On Linux, use your Docker bridge IP (often <code>http://172.17.0.1:11434</code>) and, if needed, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code>.</li> <li>Find your bridge IP on Linux:</li> </ul> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <ul> <li>If <code>docker0</code> is missing (alternate bridge names, Podman, etc.), inspect the default bridge gateway instead:</li> </ul> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <ul> <li>Example <code>llm_config.json</code> entry:</li> </ul> <pre><code>\"docker-ollama-llama3.1\": {\n    \"comment\": \"This runs on your own computer. It's free. Requires Ollama to be installed. PlanExe runs in a Docker container, and ollama is installed on the host the computer.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://host.docker.internal:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <ul> <li>Restart or rebuild the worker/frontends after updating <code>llm_config.json</code>: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config baked in).</li> </ul>"},{"location":"ai_providers/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>Use the command line to compare Ollama's list of installed models with the configurations in your <code>llm_config.json</code> file. Run:</p> <pre><code>PROMPT&gt; ollama list\nNAME                                             ID              SIZE      MODIFIED       \nhf.co/unsloth/Llama-3.1-Tulu-3-8B-GGUF:Q4_K_M    08fe35cc5878    4.9 GB    19 minutes ago    \nphi4:latest                                      ac896e5b8b34    9.1 GB    6 weeks ago       \nqwen2.5-coder:latest                             2b0496514337    4.7 GB    2 months ago      \nllama3.1:latest                                  42182419e950    4.7 GB    5 months ago      \n</code></pre> <p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p> <p>Where to look for logs: - Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container). - Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to Ollama). - Structured-output failures: if you see JSON/parse errors or malformed outputs in <code>log.txt</code>, try a different Ollama model or quantization; not all models return structured output cleanly.</p>"},{"location":"ai_providers/ollama/#how-to-add-a-new-ollama-model-to-llm_configjson","title":"How to add a new Ollama model to <code>llm_config.json</code>","text":"<p>You can find models and installation instructions here: - Ollama \u2013 Overview of popular models, curated by the Ollama team. - Hugging Face \u2013 A vast collection of GGUF models.</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output. Not every model does this reliably; you may need to try a few nearby models (or quantizations) before finding one that cleanly returns the structured responses PlanExe expects.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Follow the instructions on Ollama or Hugging Face to install the model.</li> <li>Copy the model id from the <code>ollama list</code> command, such as <code>llama3.1:latest</code></li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"ai_providers/ollama/#run-ollama-on-a-remote-computer","title":"Run Ollama on a remote computer","text":"<p>In <code>llm_config.json</code>, insert <code>base_url</code> with the url to run on. Prefer a secure tunnel (example below) or a firewall-restricted host\u2014avoid exposing Ollama publicly.</p> <p>SSH tunnel example from your local machine:</p> <pre><code>ssh -N -L 11434:localhost:11434 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:11434</code> while the tunnel is running.</p> <pre><code>\"ollama-llama3.1\": {\n    \"comment\": \"This runs on on a remote computer. Requires Ollama to be installed.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://example.com:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre>"},{"location":"ai_providers/ollama/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/openrouter/","title":"Using PlanExe with OpenRouter","text":"<p>For new users, OpenRouter is the recommended starting point. When you have have generated a few plans via OpenRouter, then you can try switch to other AI providers.</p> <p>OpenRouter provides access to a large number of LLM models, that runs in the cloud.</p> <p>Unfortunately there is no <code>free</code> model that works reliable with PlanExe.</p> <p>In my experience, the <code>paid</code> models are the most reliable. Models like google/gemini-2.0-flash-001. and openai/gpt-4o-mini are cheap and faster than running models on my own computer and without risk of it overheating.</p> <p>I haven't been able to find a <code>free</code> model on OpenRouter that works well with PlanExe.</p> <p>Avoid pricey <code>paid</code> models. PlanExe does more than 100 LLM inference calls per plan, so each run uses many tokens. With a cheap model, creating a full plan costs less than 0.30 USD; with one of the newest models, the price can exceed 20 USD. To keep PlanExe affordable for as many users as possible, the defaults use older, cheaper models.</p>"},{"location":"ai_providers/openrouter/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install Docker (with Docker Compose) \u2014 no local Python or pip is needed now.</li> <li>Clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre></li> <li>Copy <code>.env.docker-example</code> to <code>.env</code>, then set your API key and pick a default OpenRouter profile so the worker uses the cloud model by default: <pre><code>OPENROUTER_API_KEY='sk-or-v1-...'\nDEFAULT_LLM='openrouter-paid-gemini-2.0-flash-001'   # or openrouter-paid-openai-gpt-4o-mini\n</code></pre>    The containers mount <code>.env</code> and <code>llm_config.json</code> automatically.</li> <li>Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre></li> <li>Wait for http://localhost:7860 to come up, submit a prompt, and watch progress with <code>docker compose logs -f worker_plan</code>.</li> <li>Outputs are written to <code>run/&lt;timestamped-output-dir&gt;</code> on the host (mounted from the containers).</li> <li>Stop with <code>Ctrl+C</code> (or <code>docker compose down</code>). If you change <code>llm_config.json</code>, restart the containers so they reload it: <code>docker compose restart worker_plan frontend_single_user</code> (or <code>docker compose down &amp;&amp; docker compose up</code>). No rebuild is needed for config-only edits.</li> </ol>"},{"location":"ai_providers/openrouter/#configuration","title":"Configuration","text":"<p>Visit OpenRouter, create an account, purchase 5 USD in credits (plenty for making a several plans), and generate an API key.</p> <p>Copy <code>.env.docker-example</code> to a new file called <code>.env</code> (loaded by Docker at startup).</p> <p>Open the <code>.env</code> file in a text editor and insert your OpenRouter API key. Like this:</p> <pre><code>OPENROUTER_API_KEY='INSERT YOUR KEY HERE'\n</code></pre> <p>If you edit <code>llm_config.json</code> later, restart the worker/frontend containers to pick up the changes: <code>docker compose restart worker_plan frontend_single_user</code> (or stop/start). Rebuilds are only needed when dependencies change.</p>"},{"location":"ai_providers/openrouter/#troubleshooting","title":"Troubleshooting","text":"<p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>When running in Docker, also check the worker logs for 401/429 or connectivity errors:</p> <pre><code>docker compose logs -f worker_plan\n</code></pre> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p>"},{"location":"ai_providers/openrouter/#how-to-add-a-new-openrouter-model-to-llm_configjson","title":"How to add a new OpenRouter model to <code>llm_config.json</code>","text":"<p>The OpenRouter/rankings page shows an overview of the most popular models. New models are added frequently</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Copy the model id from the openrouter website.</li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"ai_providers/openrouter/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"developer/database_postgres/","title":"Database Postgres","text":"<p>Database container for PlanExe. Used as a queue mechanism for planning tasks. The <code>worker_plan_database</code> listens for an incoming task, and runs PlanExe and then goes back to listen for more incoming tasks.</p> <p>In a single user environment, then this is overkill. The file system is sufficient.</p> <p>In a multi user environment, then there are many moving parts, and here a database is relevant.</p> <ul> <li>Build/run via <code>docker compose up database_postgres</code> (or <code>docker compose build database_postgres</code>).</li> <li>Defaults: <code>PLANEXE_POSTGRES_USER=planexe</code>, <code>PLANEXE_POSTGRES_PASSWORD=planexe</code>, <code>PLANEXE_POSTGRES_DB=planexe</code> (override with env or <code>.env</code>).</li> <li>Ports: <code>${PLANEXE_POSTGRES_PORT:-5432}</code> on the host mapped to <code>5432</code> in the container. Set <code>PLANEXE_POSTGRES_PORT</code> in <code>.env</code> or your shell to avoid clashes.</li> <li>Data: persisted in the named volume <code>database_postgres_data</code>.</li> </ul>"},{"location":"developer/database_postgres/#choose-a-host-port","title":"Choose a host port","text":"<p>The default PostgreSQL port is 5432. On developer machines, this port is often already occupied by a local PostgreSQL installation:</p> <ul> <li>macOS: Postgres.app (a popular menu-bar Postgres that auto-starts), Homebrew PostgreSQL (<code>brew install postgresql</code>), or pgAdmin's bundled server</li> <li>Linux: System PostgreSQL installed via <code>apt install postgresql</code>, <code>dnf install postgresql-server</code>, etc.</li> <li>Windows: PostgreSQL installer, pgAdmin, or other database tools</li> </ul> <p>If port 5432 is in use, Docker will fail to start <code>database_postgres</code> with a \"port already in use\" error.</p> <p>Solution: Set <code>PLANEXE_POSTGRES_PORT</code> to a different value before starting the container:</p> <pre><code>export PLANEXE_POSTGRES_PORT=5433\ndocker compose up database_postgres\n</code></pre> <p>Or add it to your <code>.env</code> file to make it permanent: <pre><code>PLANEXE_POSTGRES_PORT=5433\n</code></pre></p> <p>Replace <code>5433</code> with any free host port you prefer.</p> <p>Important: This only affects the HOST port mapping (how you access Postgres from your machine). Inside Docker, containers always communicate with each other on the internal port 5432\u2014this is hardcoded and not affected by <code>PLANEXE_POSTGRES_PORT</code>.</p>"},{"location":"developer/database_postgres/#verify-the-container","title":"Verify the container","text":"<ul> <li>Check status: <code>docker compose ps database_postgres</code></li> <li>Shell in to confirm Postgres is the right one: <code>docker compose exec database_postgres psql -U planexe -d planexe</code></li> </ul>"},{"location":"developer/database_postgres/#dbeaver","title":"DBeaver","text":"<p>For managing the database, I recommend using the <code>DBeaver Community</code> app, which is open source.</p> <p>https://github.com/dbeaver/dbeaver</p> <p>Connect with host <code>localhost</code>, port <code>${PLANEXE_POSTGRES_PORT:-5432}</code>, database <code>planexe</code>, user <code>planexe</code>, password <code>planexe</code> (or whatever you set in <code>.env</code>).</p>"},{"location":"developer/database_postgres/#railway-dbeaver","title":"Railway + DBeaver","text":"<p>DBeaver cannot connect via the Railway CLI tunnel (<code>railway ssh</code>/<code>connect</code>), because the CLI does not provide a traditional TCP port forward. Instead, use Railway's TCP Proxy feature.</p>"},{"location":"developer/database_postgres/#1-enable-tcp-proxy-in-railway","title":"1. Enable TCP Proxy in Railway","text":"<ol> <li>Go to your Railway dashboard \u2192 <code>database_postgres</code> service</li> <li>Navigate to Settings \u2192 Networking \u2192 Public Networking</li> <li>Add a TCP Proxy with port <code>5432</code></li> <li>Railway will assign a hostname and port, e.g., <code>subsubdomain.subdomain.example.com:12345</code></li> </ol> <p>Warning: Only enable TCP Proxy after setting a secure password (see below).</p> <p>Warning: The TCP Proxy connection is unencrypted. Railway's TCP Proxy forwards raw TCP traffic without adding TLS, and the <code>postgres:16-alpine</code> image doesn't have SSL enabled by default. Your password and data travel in plain text. Consider disabling TCP Proxy when not in use, or configure SSL on the PostgreSQL container for production use.</p>"},{"location":"developer/database_postgres/#2-set-a-secure-password","title":"2. Set a secure password","text":"<p>The default password <code>planexe</code> is too easy to guess. PostgreSQL only sets the password on first initialization, so if the database already exists:</p> <ol> <li>Connect with the current password</li> <li>Run: <code>ALTER USER planexe WITH PASSWORD 'your-secure-password';</code></li> <li>Update <code>POSTGRES_PASSWORD</code> in Railway's environment variables to match</li> </ol>"},{"location":"developer/database_postgres/#3-connect-with-dbeaver","title":"3. Connect with DBeaver","text":"<p>In DBeaver, create a new PostgreSQL connection with \"Connect by: Host\":</p> Field Value Host Your TCP Proxy hostname (e.g., <code>subsubdomain.subdomain.example.com</code>) Port Your assigned port (e.g., <code>12345</code>, NOT 5432) Database <code>planexe</code> Username <code>planexe</code> Password Your secure password <p>Click Test Connection to verify.</p>"},{"location":"developer/database_postgres/#4-security-check","title":"4. Security check","text":"<p>Try connecting with password <code>planexe</code>. If it succeeds, the password hasn't been changed yet\u2014go back to step 2.</p> <p>See <code>railway.md</code> for more details.</p>"},{"location":"developer/database_postgres/#ssl-future-plan","title":"SSL (Future Plan)","text":"<p>The current setup uses unencrypted connections. For production use with public TCP Proxy exposure, SSL/TLS should be enabled to encrypt traffic between clients and the database.</p>"},{"location":"developer/database_postgres/#whats-needed","title":"What's needed","text":""},{"location":"developer/database_postgres/#1-generate-ssl-certificates","title":"1. Generate SSL certificates","text":"<p>You'll need a certificate and private key. Options: - Self-signed: Quick for internal use, but clients must trust the certificate manually - Let's Encrypt: Free, but requires domain validation (complex for raw TCP) - Commercial CA: Trusted by default, but costs money</p> <p>Example self-signed certificate generation:</p> <pre><code>openssl req -new -x509 -days 365 -nodes \\\n  -out server.crt \\\n  -keyout server.key \\\n  -subj \"/CN=database_postgres\"\n</code></pre>"},{"location":"developer/database_postgres/#2-update-the-dockerfile","title":"2. Update the Dockerfile","text":"<p>Add the certificates and configure PostgreSQL to use them:</p> <pre><code>FROM postgres:16-alpine\n\n# ... existing ENV statements ...\n\n# Copy SSL certificates\nCOPY server.crt /var/lib/postgresql/server.crt\nCOPY server.key /var/lib/postgresql/server.key\n\n# Set correct permissions (required by PostgreSQL)\nRUN chmod 600 /var/lib/postgresql/server.key &amp;&amp; \\\n    chown postgres:postgres /var/lib/postgresql/server.crt /var/lib/postgresql/server.key\n\n# Enable SSL in PostgreSQL\nRUN echo \"ssl = on\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample &amp;&amp; \\\n    echo \"ssl_cert_file = '/var/lib/postgresql/server.crt'\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample &amp;&amp; \\\n    echo \"ssl_key_file = '/var/lib/postgresql/server.key'\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample\n</code></pre>"},{"location":"developer/database_postgres/#3-configure-dbeaver-for-ssl","title":"3. Configure DBeaver for SSL","text":"<p>In DBeaver's connection settings:</p> <ol> <li>Go to the SSL tab</li> <li>Check \"Use SSL\"</li> <li>Set SSL mode:</li> <li><code>require</code> \u2014 Encrypt connection, don't verify certificate</li> <li><code>verify-ca</code> \u2014 Encrypt and verify certificate against a CA</li> <li><code>verify-full</code> \u2014 Encrypt, verify certificate, and check hostname</li> <li>For self-signed certs, you may need to import the CA/certificate or set \"Trust all certificates\"</li> </ol>"},{"location":"developer/database_postgres/#4-enforce-ssl-on-the-server-optional","title":"4. Enforce SSL on the server (optional)","text":"<p>To reject unencrypted connections, add to <code>pg_hba.conf</code>:</p> <pre><code># Require SSL for all remote connections\nhostssl all all 0.0.0.0/0 scram-sha-256\n</code></pre>"},{"location":"developer/database_postgres/#resources","title":"Resources","text":"<ul> <li>PostgreSQL SSL Documentation</li> <li>pg_hba.conf Documentation</li> </ul>"},{"location":"developer/database_postgres/#railway-backup-to-local-file","title":"Railway backup to local file","text":"<p>Use <code>database_postgres/download_backup.py</code> to stream a compressed dump from the Railway <code>database_postgres</code> service to your machine.</p> <p>Prereq: Railway CLI installed and logged in.</p> <pre><code>python database_postgres/download_backup.py\n</code></pre> <ul> <li>Runs <code>railway link</code> (skip with <code>--skip-link</code> if already linked).</li> <li>Streams <code>pg_dump -F c -Z9</code> via <code>railway ssh</code> and writes <code>YYYYMMDD-HHMM.dump</code> in the current directory.</li> <li>Options:</li> <li><code>--user</code> Postgres user (default: <code>$PLANEXE_POSTGRES_USER</code> or <code>planexe</code>)</li> <li><code>--db</code> Postgres database (default: <code>$PLANEXE_POSTGRES_DB</code> or <code>planexe</code>)</li> <li><code>--output-dir path</code> Directory for the dump file</li> <li><code>--filename name.dump</code> Override dump filename</li> <li><code>--service other_service</code> Railway service name</li> <li><code>--skip-link</code> Skip <code>railway link</code> if already linked</li> </ul>"},{"location":"developer/database_postgres/#restore-a-backup-locally","title":"Restore a backup locally","text":"<p>Run a Postgres you can reach (for example <code>docker compose up database_postgres</code> on your machine), then restore the custom-format dump:</p> <pre><code>PGPASSWORD=planexe pg_restore \\\n  -h localhost \\\n  -p 5432 \\\n  -U planexe \\\n  -d planexe \\\n  /path/to/19841231-2359.dump\n</code></pre> <ul> <li>The dump is custom format (<code>pg_dump -F c</code>), so use <code>pg_restore</code>, not <code>psql</code>.</li> <li>Ensure the target database exists; add <code>-c</code> to drop objects before recreating them if you want a clean restore.</li> <li>If you changed credentials/DB name in <code>.env</code> or Railway, use those here.</li> </ul>"},{"location":"developer/frontend_multi_user/","title":"Frontend multi user - Experimental","text":"<p>My recommendation: Avoid this, instead go with <code>frontend_single_user</code>. This multi user UI is the bare minimum, unpolished. It has a queue mechanism, admin UI, but it has no user account management. I use it for handling multiple users. It requires lots of setup to get working. It's not something that simply works out of the box. Save yourself the trouble, go with <code>frontend_single_user</code> instead.</p> <p>Flask-based multi-user UI for PlanExe. Runs in Docker, uses Postgres (defaults to the <code>database_postgres</code> service), and only needs the lightweight <code>worker_plan_api</code> helpers (no full <code>worker_plan</code> install).</p>"},{"location":"developer/frontend_multi_user/#quickstart-with-docker","title":"Quickstart with Docker","text":"<ul> <li>Ensure <code>.env</code> and <code>llm_config.json</code> exist in the repo root (they are mounted into the container).</li> <li><code>docker compose up frontend_multi_user</code></li> <li>Open http://localhost:${PLANEXE_FRONTEND_MULTIUSER_PORT:-5001}/ (container listens on 5000). Health endpoint: <code>/healthcheck</code>.</li> </ul>"},{"location":"developer/frontend_multi_user/#config-env","title":"Config (env)","text":"<ul> <li><code>PLANEXE_FRONTEND_MULTIUSER_DB_HOST|PORT|NAME|USER|PASSWORD</code>: Postgres target (defaults follow <code>database_postgres</code> / <code>planexe</code> values).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_ADMIN_USERNAME</code> / <code>PLANEXE_FRONTEND_MULTIUSER_ADMIN_PASSWORD</code>: Admin login for the UI; must be set (service fails to start if missing).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_HOST</code>: bind address inside the container (default 0.0.0.0).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_PORT</code>: Flask port inside the container (default 5000).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_DEBUG</code>: set <code>true</code> to enable Flask debug.</li> <li><code>PLANEXE_CONFIG_PATH</code>: defaults to <code>/app</code> so PlanExe picks up <code>.env</code> + <code>llm_config.json</code> that compose mounts.</li> </ul>"},{"location":"developer/frontend_multi_user/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>frontend_multi_user</code> so its dependencies stay isolated:</p> <pre><code>cd frontend_multi_user\npython3 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e .\nexport PYTHONPATH=$PWD/..:$PWD/../worker_plan:$PYTHONPATH\npython src/app.py\n</code></pre> <p>Run <code>deactivate</code> when you are done with the venv.</p> <p>The <code>PYTHONPATH</code> makes <code>worker_plan_api</code> and <code>database_api</code> importable without installing the full <code>worker_plan</code> package (which has fragile dependencies in <code>worker_plan_internal</code>).</p>"},{"location":"developer/frontend_single_user/","title":"Frontend Single User","text":"<p>This directory contains the PlanExe single-user Gradio frontend.</p>"},{"location":"developer/frontend_single_user/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>frontend_single_user</code> so its dependencies stay isolated (they may be incompatible with <code>worker_plan</code>):</p> <pre><code>cd frontend_single_user\npython3 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -r requirements.txt\nexport PYTHONPATH=$PWD/../worker_plan:$PYTHONPATH\npython app.py\n</code></pre> <p>The app loads environment variables from a <code>.env</code> file (if present). Create one with:</p> <pre><code># .env\nPLANEXE_WORKER_PLAN_URL=http://localhost:8000\nPLANEXE_OPEN_DIR_SERVER_URL=http://localhost:5100\n</code></pre> <p>Then open http://localhost:7860 (or your <code>PLANEXE_GRADIO_SERVER_PORT</code>). Run <code>deactivate</code> when you are done with the venv.</p> <p>If you prefer to install the shared API package instead of using <code>PYTHONPATH</code>, run <code>pip install -e ../worker_plan</code> (this will bring the worker dependencies into the same venv).</p>"},{"location":"developer/frontend_single_user/#environment-variables","title":"Environment variables","text":"Variable Default Purpose <code>PLANEXE_WORKER_PLAN_URL</code> <code>http://worker_plan:8000</code> Base URL for <code>worker_plan</code> service the UI calls. <code>PLANEXE_WORKER_PLAN_TIMEOUT</code> <code>30</code> HTTP timeout (seconds) for <code>worker_plan</code> requests. <code>PLANEXE_GRADIO_SERVER_NAME</code> <code>0.0.0.0</code> Host/interface Gradio binds to. <code>PLANEXE_GRADIO_SERVER_PORT</code> <code>7860</code> Port Gradio listens on. <code>PLANEXE_PASSWORD</code> (unset) Optional password to protect the UI (<code>user</code> / <code>&lt;value&gt;</code>). Leave unset for local development without auth. <code>PLANEXE_OPEN_DIR_SERVER_URL</code> (unset) URL of the host opener service for \u201cOpen Output Dir\u201d; leave unset to hide the button."},{"location":"developer/frontend_single_user/#password","title":"Password","text":"<p>Leave <code>PLANEXE_PASSWORD</code> unset when running PlanExe on your own computer.</p> <p>However when running in the cloud, here you may want password protection.</p> <p>Set <code>PLANEXE_PASSWORD</code> to turn on Gradio\u2019s basic auth. Example:</p> <pre><code>export PLANEXE_PASSWORD=123\ndocker compose up\n</code></pre> <p>Then open the app and log in with username <code>user</code> and password <code>123</code>.</p>"},{"location":"developer/mcp_cloud/","title":"PlanExe MCP Cloud - Experimental, likely to be changed a lot!","text":"<p>Model Context Protocol (MCP) interface for PlanExe. Implements the MCP specification defined in <code>docs/mcp/planexe_mcp_interface.md</code>.</p>"},{"location":"developer/mcp_cloud/#overview","title":"Overview","text":"<p>mcp_cloud provides a standardized MCP interface for PlanExe's plan generation workflows. It connects to <code>worker_plan_database</code> via the shared Postgres database (<code>database_api</code> models).</p>"},{"location":"developer/mcp_cloud/#features","title":"Features","text":"<ul> <li>Task Management: Create and stop plan generation tasks</li> <li>Progress Tracking: Real-time status and progress updates</li> <li>File Metadata: Get report/zip metadata and download URLs</li> </ul>"},{"location":"developer/mcp_cloud/#run-as-task-mcp-tasks-protocol","title":"Run as task (MCP tasks protocol)","text":"<p>MCP has two ways to run long-running work: tools (what we use) and the tasks protocol (\"Run as task\" in some UIs). PlanExe uses tools only: <code>task_create</code>, <code>task_status</code>, <code>task_stop</code>, <code>task_download</code>. The agent creates a task, polls status, then downloads; that is the intended flow per <code>docs/mcp/planexe_mcp_interface.md</code>. We do not advertise or implement the MCP tasks protocol (tasks/get, tasks/result, etc.). Clients like Cursor do not support it properly\u2014use the tools directly.</p>"},{"location":"developer/mcp_cloud/#client-choice-guide","title":"Client Choice Guide","text":"<ul> <li>Use <code>mcp_cloud</code> directly (HTTP): If you are running in the cloud or you do   not need files saved to the local filesystem.</li> <li>Use <code>mcp_local</code> (proxy): Recommended when you want artifacts downloaded to   your local disk (<code>PLANEXE_PATH</code>). The proxy forwards MCP calls to this server   and handles file downloads locally.</li> <li>Recommended flow: Docker (<code>mcp_cloud</code>) \u2192 <code>mcp_local</code> \u2192 MCP client (LM Studio/Claude).</li> </ul>"},{"location":"developer/mcp_cloud/#docker-usage-recommended","title":"Docker Usage (Recommended)","text":"<p>Build and run mcp_cloud with HTTP endpoints:</p> <pre><code>docker compose up --build mcp_cloud\n</code></pre> <p>mcp_cloud exposes HTTP endpoints on port <code>8001</code> (or <code>${PLANEXE_MCP_HTTP_PORT}</code>). Set <code>PLANEXE_MCP_API_KEY</code> in your <code>.env</code> file or environment to enable API key authentication.</p>"},{"location":"developer/mcp_cloud/#connecting-via-httpurl","title":"Connecting via HTTP/URL","text":"<p>After starting with Docker, configure your MCP client (e.g., LM Studio) to connect via HTTP:</p> <p>Local Docker (development):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"http://localhost:8001/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Railway/Cloud deployment:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"https://your-app.up.railway.app/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Alternative header format (also supported):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"https://your-app.up.railway.app/mcp\",\n      \"headers\": {\n        \"API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Set <code>PLANEXE_MCP_API_KEY</code> to the same value you use in <code>Authorization: Bearer &lt;key&gt;</code> or <code>X-API-Key</code>.</p>"},{"location":"developer/mcp_cloud/#available-http-endpoints","title":"Available HTTP Endpoints","text":"<ul> <li><code>POST /mcp</code> - Main MCP JSON-RPC endpoint (Streamable HTTP; may use SSE for streaming)</li> <li><code>GET /mcp/tools</code> - List tools (JSON). No SSE required. Use this if your client reports \"SSE error\" when connecting to <code>/mcp</code>.</li> <li><code>POST /mcp/tools/call</code> - Call a tool (JSON). No SSE required.</li> <li><code>GET /healthcheck</code> - Health check endpoint</li> <li><code>GET /docs</code> - OpenAPI documentation (Swagger UI)</li> </ul>"},{"location":"developer/mcp_cloud/#sse-error-or-no-server-sse-stream-from-the-client","title":"\"SSE error\" or \"no Server-SSE stream\" from the client","text":"<p>Some MCP clients (e.g. OpenClaw/mcporter) connect by doing a GET to the server URL and expect a Server-Sent Events (SSE) stream (<code>Content-Type: text/event-stream</code>). That is the Streamable HTTP transport. This server mounts FastMCP at <code>/mcp</code>; GET /mcp returns a 307 redirect to <code>/mcp/</code>, and the Streamable HTTP handshake may not match what the client expects, so the client reports \"SSE error\" or \"could not fetch \u2026 no SSE stream\".</p> <p>You do not need SSE for tools. MCP over HTTP can use plain JSON:</p> <ul> <li>List tools: <code>GET http://&lt;host&gt;:8001/mcp/tools</code> \u2192 returns <code>{\"tools\": [...]}</code> (JSON).</li> <li>Call a tool: <code>POST http://&lt;host&gt;:8001/mcp/tools/call</code> with body <code>{\"tool\": \"task_create\", \"arguments\": {\"prompt\": \"\u2026\", \"speed_vs_detail\": \"ping\"}}</code> \u2192 returns JSON.</li> </ul> <p>If your client only supports Streamable HTTP and fails on <code>/mcp</code>, you have two options:</p> <ol> <li>Point the client at the JSON API if it allows a separate \"tools list\" URL: use <code>GET /mcp/tools</code> for listing and <code>POST /mcp/tools/call</code> for calls (no SSE).</li> <li>Use baseUrl with trailing slash (e.g. <code>http://192.168.1.10:8001/mcp/</code>) so the client does not follow a redirect; whether that fixes SSE depends on how the client and FastMCP do the Streamable HTTP handshake.</li> </ol>"},{"location":"developer/mcp_cloud/#environment-variables","title":"Environment Variables","text":""},{"location":"developer/mcp_cloud/#http-server-configuration","title":"HTTP Server Configuration","text":"<ul> <li><code>PLANEXE_MCP_API_KEY</code>: Required for production. API key for authentication. Clients can provide <code>Authorization: Bearer &lt;key&gt;</code> or <code>X-API-Key</code>.</li> <li><code>PLANEXE_MCP_HTTP_HOST</code>: HTTP server host (default: <code>127.0.0.1</code>). Use <code>0.0.0.0</code> to bind all interfaces (containers/cloud).</li> <li><code>PLANEXE_MCP_HTTP_PORT</code>: HTTP server port (default: <code>8001</code>). Railway will override with <code>PORT</code> env var.</li> <li><code>PLANEXE_MCP_PUBLIC_BASE_URL</code>: Public base URL for report download links (default unset; clients can use the connected base URL).</li> <li><code>PORT</code>: Railway-provided port (takes precedence over <code>PLANEXE_MCP_HTTP_PORT</code>)</li> <li><code>PLANEXE_MCP_CORS_ORIGINS</code>: Comma-separated list of allowed origins (default: <code>http://localhost,http://127.0.0.1</code>).</li> <li><code>PLANEXE_MCP_MAX_BODY_BYTES</code>: Max request size for <code>POST /mcp/tools/call</code> (default: <code>1048576</code>).</li> <li><code>PLANEXE_MCP_RATE_LIMIT</code>: Max requests per window for <code>POST /mcp/tools/call</code> (default: <code>60</code>).</li> <li><code>PLANEXE_MCP_RATE_WINDOW_SECONDS</code>: Rate limit window in seconds (default: <code>60</code>).</li> </ul>"},{"location":"developer/mcp_cloud/#database-configuration","title":"Database Configuration","text":"<p>mcp_cloud uses the same database configuration as other PlanExe services:</p> <ul> <li><code>SQLALCHEMY_DATABASE_URI</code>: Full database connection string (takes precedence)</li> <li><code>PLANEXE_POSTGRES_HOST</code>: Database host (default: <code>database_postgres</code>)</li> <li><code>PLANEXE_POSTGRES_PORT</code>: Database port (default: <code>5432</code>)</li> <li><code>PLANEXE_POSTGRES_DB</code>: Database name (default: <code>planexe</code>)</li> <li><code>PLANEXE_POSTGRES_USER</code>: Database user (default: <code>planexe</code>)</li> <li><code>PLANEXE_POSTGRES_PASSWORD</code>: Database password (default: <code>planexe</code>)</li> <li><code>PLANEXE_WORKER_PLAN_URL</code>: URL of the worker_plan HTTP service (default: <code>http://worker_plan:8000</code>)</li> </ul>"},{"location":"developer/mcp_cloud/#mcp-tools","title":"MCP Tools","text":"<p>See <code>docs/mcp/planexe_mcp_interface.md</code> for full specification. Available tools:</p> <ul> <li><code>prompt_examples</code> - Return example prompts. Use these as examples for task_create.</li> <li><code>task_create</code> - Create a new task (returns task_id as UUID)</li> <li><code>task_status</code> - Get task status and progress</li> <li><code>task_stop</code> - Stop an active task</li> <li><code>task_file_info</code> - Get file metadata for report or zip</li> </ul> <p>Note: <code>task_download</code> is a synthetic tool provided by <code>mcp_local</code>, not by this server. If your client exposes <code>task_download</code>, use it to save the report or zip locally; otherwise use <code>task_file_info</code> to get <code>download_url</code> and fetch the file yourself.</p> <p>Tip: Call <code>prompt_examples</code> to get example prompts to use with task_create. The catalog is the same as in the frontends (<code>worker_plan.worker_plan_api.PromptCatalog</code>). When running with <code>PYTHONPATH</code> set to the repo root (e.g. stdio setup), the catalog is loaded automatically; otherwise built-in examples are returned.</p> <p>Download flow: call <code>task_file_info</code> to obtain the <code>download_url</code>, then fetch the report via <code>GET /download/{task_id}/030-report.html</code> (API key required if configured).</p>"},{"location":"developer/mcp_cloud/#debugging-tools","title":"Debugging tools","text":"<p>Use the MCP Inspector to verify tool registration and output schemas.</p> <p>Everything reference (stdio):</p> <pre><code>npx @modelcontextprotocol/inspector --transport stdio npx -y @modelcontextprotocol/server-everything\n</code></pre> <p>Steps: - Click \"Connect\" - Click \"Tools\" - Click \"List Tools\"</p> <p>PlanExe MCP (HTTP):</p> <pre><code>npx @modelcontextprotocol/inspector --transport http --server-url http://localhost:8001/mcp\n</code></pre> <p>Steps: - Click \"Connect\" - Click \"Tools\" - Click \"List Tools\"</p>"},{"location":"developer/mcp_cloud/#architecture","title":"Architecture","text":"<p>mcp_cloud maps MCP concepts to PlanExe's database models:</p> <ul> <li>Task \u2192 <code>TaskItem</code> (each task corresponds to a TaskItem)</li> <li>Run \u2192 Execution of a TaskItem by <code>worker_plan_database</code></li> <li>Report \u2192 HTML report fetched from <code>worker_plan</code> via HTTP API</li> </ul> <p>mcp_cloud reads task state and progress from the database, and fetches artifacts from <code>worker_plan</code> via HTTP instead of accessing the run directory directly. This allows mcp_cloud to work without mounting the run directory, making it compatible with Railway and other cloud platforms that don't support shared volumes across services.</p>"},{"location":"developer/mcp_cloud/#connecting-via-stdio-advanced-contributor-mode","title":"Connecting via stdio (Advanced / Contributor Mode)","text":"<p>For local development, you can run mcp_cloud over stdio instead of HTTP. This is useful for testing but requires local Python + Postgres setup. For most users, the recommended flow is Docker (server) + <code>mcp_local</code> (client).</p>"},{"location":"developer/mcp_cloud/#setup","title":"Setup","text":"<ol> <li>Install dependencies in a virtual environment:</li> </ol> <pre><code>cd mcp_cloud\npython3.13 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <ol> <li>Ensure the database is accessible. If using Docker for the database:</li> </ol> <pre><code># From repo root, ensure database_postgres is running\ndocker compose up -d database_postgres\n</code></pre> <ol> <li>Set environment variables (create a <code>.env</code> file in the repo root or export them):</li> </ol> <pre><code>export PLANEXE_POSTGRES_HOST=localhost\nexport PLANEXE_POSTGRES_PORT=5432  # Or your mapped port (e.g., 5433 if you set PLANEXE_POSTGRES_PORT)\nexport PLANEXE_POSTGRES_DB=planexe\nexport PLANEXE_POSTGRES_USER=planexe\nexport PLANEXE_POSTGRES_PASSWORD=planexe\n</code></pre> <p>Note: The <code>PYTHONPATH</code> environment variable in the LM Studio config (see below) ensures that the <code>database_api</code> module can be imported. Make sure the path points to the PlanExe repository root (where <code>database_api/</code> is located).</p>"},{"location":"developer/mcp_cloud/#lm-studio-configuration","title":"LM Studio Configuration","text":"<p>Add the following to your LM Studio MCP servers configuration file:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"/absolute/path/to/PlanExe/mcp_cloud/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"mcp_cloud.app\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/absolute/path/to/PlanExe\",\n        \"PLANEXE_POSTGRES_HOST\": \"localhost\",\n        \"PLANEXE_POSTGRES_PORT\": \"5432\",\n        \"PLANEXE_POSTGRES_DB\": \"planexe\",\n        \"PLANEXE_POSTGRES_USER\": \"planexe\",\n        \"PLANEXE_POSTGRES_PASSWORD\": \"planexe\"\n      }\n    }\n  }\n}\n</code></pre> <p>Important: Replace <code>/absolute/path/to/PlanExe</code> with the actual absolute path to your PlanExe repository on your system.</p> <p>Example (if PlanExe is at <code>/absolute/path/to/PlanExe</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"/absolute/path/to/PlanExe/mcp_cloud/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"mcp_cloud.app\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/absolute/path/to/PlanExe\",\n        \"PLANEXE_POSTGRES_HOST\": \"localhost\",\n        \"PLANEXE_POSTGRES_PORT\": \"5432\",\n        \"PLANEXE_POSTGRES_DB\": \"planexe\",\n        \"PLANEXE_POSTGRES_USER\": \"planexe\",\n        \"PLANEXE_POSTGRES_PASSWORD\": \"planexe\"\n      }\n    }\n  }\n}\n</code></pre> <p>Using Docker (more complex, but keeps dependencies isolated):</p> <p>You can use <code>docker compose exec</code> to run mcp_cloud:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"compose\",\n        \"-f\",\n        \"/absolute/path/to/PlanExe/docker-compose.yml\",\n        \"exec\",\n        \"-T\",\n        \"mcp_cloud\",\n        \"python\",\n        \"-m\",\n        \"mcp_cloud.app\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Note: This requires the <code>mcp_cloud</code> container to be running (<code>docker compose up -d mcp_cloud</code>).</p>"},{"location":"developer/mcp_cloud/#troubleshooting","title":"Troubleshooting","text":"<p>Connection issues: - Ensure the database is running and accessible at the configured host/port - Check that the <code>PYTHONPATH</code> in the LM Studio config points to the PlanExe repository root (containing <code>database_api/</code>, <code>mcp_cloud/</code>, etc.) - Verify the Python interpreter path in the <code>command</code> field is correct and points to the venv Python</p> <p>Import errors: - If you see <code>ModuleNotFoundError: No module named 'database_api'</code>, check that <code>PYTHONPATH</code> is set correctly - If you see <code>ModuleNotFoundError: No module named 'mcp'</code>, ensure you've installed the requirements: <code>pip install -r requirements.txt</code></p> <p>Database connection errors: - Verify Postgres is running: <code>docker compose ps database_postgres</code> - Check the port mapping: if you set <code>PLANEXE_POSTGRES_PORT=5433</code>, use <code>5433</code> in your env vars, not <code>5432</code> - Test connection: <code>psql -h localhost -p 5432 -U planexe -d planexe</code> (or your port)</p> <p>Path issues: - Always use absolute paths in LM Studio config, not relative paths - On Windows, use forward slashes in the config JSON (e.g., <code>C:/Users/...</code>) or escaped backslashes</p>"},{"location":"developer/mcp_cloud/#development","title":"Development","text":"<p>Run locally for testing:</p> <pre><code>cd mcp_cloud\nsource .venv/bin/activate  # If not already activated\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m mcp_cloud.app\n</code></pre>"},{"location":"developer/mcp_cloud/#railway-deployment","title":"Railway Deployment","text":"<p>See <code>railway.md</code> for Railway-specific deployment instructions. The server automatically detects Railway's <code>PORT</code> environment variable and binds to it.</p>"},{"location":"developer/mcp_cloud/#notes","title":"Notes","text":"<ul> <li>mcp_cloud communicates with <code>worker_plan_database</code> indirectly via the database for task management.</li> <li>Artifacts are fetched from <code>worker_plan</code> via HTTP instead of accessing the run directory directly. This avoids needing a shared volume mount, making it compatible with Railway and other cloud platforms.</li> <li>For artifacts:</li> <li><code>report.html</code> is fetched efficiently via the dedicated <code>/runs/{run_id}/report</code> endpoint</li> <li>Other files are fetched by downloading the run zip and extracting the file (less efficient but works without additional endpoints)</li> <li>Artifact writes are not yet supported via HTTP (would require a write endpoint in <code>worker_plan</code>).</li> <li>Artifact writes are rejected while a run is active (strict policy per spec).</li> <li>Task IDs use the TaskItem UUID (e.g., <code>5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1</code>).</li> <li>Security: Always set <code>PLANEXE_MCP_API_KEY</code> in production deployments to prevent unauthorized access.</li> </ul>"},{"location":"developer/mcp_local/","title":"PlanExe MCP locally - Experimental, likely to be changed a lot!","text":"<p>Model Context Protocol (MCP) local proxy for PlanExe.</p> <p>It runs on the user's computer and provides local disk access for downloads. The pipeline still runs in <code>mcp_cloud</code>, the MCP server running in the cloud; this proxy forwards tool calls over HTTP and downloads artifacts from <code>/download/{task_id}/...</code>.</p>"},{"location":"developer/mcp_local/#tools","title":"Tools","text":"<p><code>prompt_examples</code> - Return example prompts. Use these as examples for task_create. You can also call <code>task_create</code> with any prompt\u2014short prompts produce less detailed plans. <code>task_create</code> - Initiate creation of a plan. <code>task_status</code> - Get status and progress about the creation of a plan. <code>task_stop</code> - Abort creation of a plan. <code>task_download</code> - Download the plan, either html report or a zip with everything, and save it to disk.</p> <p>Tip: Call <code>prompt_examples</code> to get example prompts to use with task_create. The full catalog lives at <code>worker_plan/worker_plan_api/prompt/data/simple_plan_prompts.jsonl</code>.</p> <p><code>task_download</code> is a synthetic tool provided by the local proxy. It calls the remote MCP tool <code>task_file_info</code> to obtain a download URL, then downloads the file locally into <code>PLANEXE_PATH</code>.</p>"},{"location":"developer/mcp_local/#run-as-task-mcp-tasks-protocol","title":"Run as task (MCP tasks protocol)","text":"<p>Some MCP clients (e.g. the MCP Inspector) show a \"Run as task\" option for tools. That refers to the MCP tasks protocol: a separate mechanism where the client runs a tool in the background using RPC methods like <code>tasks/run</code>, <code>tasks/get</code>, <code>tasks/result</code>, and <code>tasks/cancel</code>, instead of a single blocking tool call.</p> <p>PlanExe does not use or advertise the MCP tasks protocol. Our interface is tool-based only: the agent calls <code>task_create</code> \u2192 gets a <code>task_id</code> \u2192 polls <code>task_status</code> \u2192 uses <code>task_download</code>. That flow is defined in <code>docs/mcp/planexe_mcp_interface.md</code> and is the intended design.</p> <p>You should not enable \"Run as task\" for PlanExe. The Python MCP SDK and clients like Cursor do not properly support the tasks protocol (method registration and initialization fail). Use the tools directly: create a task, poll status, then download when done.</p>"},{"location":"developer/mcp_local/#how-it-talks-to-mcp_cloud","title":"How it talks to mcp_cloud","text":"<ul> <li>The remote base URL is <code>PLANEXE_URL</code> (for example <code>http://localhost:8001/mcp</code>).</li> <li>Tool calls prefer the remote HTTP wrapper (<code>/mcp/tools/call</code>).</li> <li>If the HTTP wrapper is unavailable, the proxy falls back to MCP JSON-RPC   over <code>POST /mcp</code> (not SSE).</li> <li>Downloads use the remote <code>/download/{task_id}/...</code> endpoints.</li> <li>Authentication uses <code>PLANEXE_MCP_API_KEY</code> as a <code>Bearer</code> token.</li> <li>Retry behavior: Transient failures (server 5xx errors, network timeouts) are   automatically retried up to 3 times with exponential backoff (1s, 2s delays).   Client errors (4xx) are not retried. Retries are logged at WARNING level.</li> </ul>"},{"location":"developer/mcp_local/#debugging-with-mcp-inspector","title":"Debugging with MCP Inspector","text":"<p>Run the MCP inspector with the local script and environment variables:</p> <pre><code>npx @modelcontextprotocol/inspector \\\n  -e \"PLANEXE_URL\"=\"http://localhost:8001/mcp\" \\\n  -e \"PLANEXE_MCP_API_KEY\"=\"insert-your-api-key-here\" \\\n  -e \"PLANEXE_PATH\"=\"/Users/your-name/Desktop\" \\\n  --transport stdio \\\n  uv run --with mcp /absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\n</code></pre> <p>Then click \"Connect\", open \"Tools\", and use \"List Tools\" or invoke individual tools.</p>"},{"location":"developer/mcp_local/#client-configuration-local-script","title":"Client configuration (local script)","text":"<p>Clone the PlanExe repository on your computer. Use the absolute path to <code>planexe_mcp_local.py</code> and set <code>PLANEXE_PATH</code> to a directory where PlanExe is allowed to save files.</p>"},{"location":"developer/mcp_local/#local-docker-development","title":"Local Docker (development)","text":"<pre><code>\"planexe\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp\",\n    \"/absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n  ],\n  \"env\": {\n    \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n    \"PLANEXE_MCP_API_KEY\": \"insert-your-api-key-here\",\n    \"PLANEXE_PATH\": \"/User/your-name/Desktop\"\n  }\n}\n</code></pre>"},{"location":"developer/mcp_local/#remote-server-railway-or-cloud","title":"Remote server (Railway or cloud)","text":"<pre><code>\"planexe\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp\",\n    \"/absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n  ],\n  \"env\": {\n    \"PLANEXE_URL\": \"https://your-railway-app.up.railway.app/mcp\",\n    \"PLANEXE_MCP_API_KEY\": \"insert-your-api-key-here\",\n    \"PLANEXE_PATH\": \"/User/your-name/Desktop\"\n  }\n}\n</code></pre>"},{"location":"developer/open_dir_server/","title":"Host Open Dir Server","text":""},{"location":"developer/open_dir_server/#why-this-exists","title":"Why this exists","text":"<ul> <li>Docker containers cannot launch host applications (e.g., macOS Finder) because they are isolated from the host OS.</li> <li>The Gradio frontend runs in a container and cannot run <code>open</code>, <code>xdg-open</code>, or <code>start</code> on the host.</li> <li>This small FastAPI service runs on the host and receives a path from the frontend, then asks the host OS to open that path.</li> </ul>"},{"location":"developer/open_dir_server/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ on the host (outside Docker).</li> </ul>"},{"location":"developer/open_dir_server/#setup-virtual-environment","title":"Setup (virtual environment)","text":"<pre><code>cd open_dir_server\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npython app.py\n</code></pre>"},{"location":"developer/open_dir_server/#configuration","title":"Configuration","text":"<p>Environment variables (<code>PLANEXE_</code> prefixed): - <code>PLANEXE_OPEN_DIR_SERVER_HOST</code> (default <code>127.0.0.1</code>) - <code>PLANEXE_OPEN_DIR_SERVER_PORT</code> (default <code>5100</code>) - <code>PLANEXE_HOST_RUN_DIR</code>: optional; only allow opening paths under this directory. Defaults to <code>PlanExe/run</code>.</p> <p>Frontend configuration: - Set <code>PLANEXE_OPEN_DIR_SERVER_URL</code> so the container can reach the host service:   - macOS/Windows (Docker Desktop): <code>http://host.docker.internal:5100</code>   - Linux (Docker Engine): <code>http://172.17.0.1:5100</code> (or add <code>host.docker.internal</code> pointing to the bridge IP).   - Local host-only (no Docker): <code>http://localhost:5100</code></p> <p>If you relocate the run directory, set <code>PLANEXE_HOST_RUN_DIR</code> to an absolute path, for example: - macOS: <code>/Users/you/PlanExe/run</code> - Linux: <code>/home/you/PlanExe/run</code> - Windows: <code>C:\\Users\\you\\PlanExe\\run</code></p>"},{"location":"developer/open_dir_server/#start-the-server","title":"Start the server","text":"<p>From <code>open_dir_server</code>: <pre><code>cd open_dir_server\nsource .venv/bin/activate\npython app.py\n</code></pre> The service will listen on <code>PLANEXE_OPEN_DIR_SERVER_HOST:PLANEXE_OPEN_DIR_SERVER_PORT</code>.</p>"},{"location":"developer/open_dir_server/#stop-the-server","title":"Stop the server","text":"<ul> <li>Press <code>Ctrl+C</code> in the terminal where it is running.</li> </ul>"},{"location":"developer/worker_plan/","title":"Worker plan service","text":"<p>This directory hosts the shared <code>worker_plan_internal</code> package that generates plans.</p> <ul> <li><code>worker_plan_internal/</code>: core planning logic.</li> <li><code>worker_plan_api/</code>: shared types (e.g., filenames) used by both the worker and frontend.</li> </ul>"},{"location":"developer/worker_plan/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>worker_plan</code> so its dependencies stay isolated (they may be incompatible with <code>frontend_single_user</code>). Use Python 3.13 \u2014 several native wheels (pydantic-core, orjson, tiktoken, greenlet, jiter) do not yet publish for 3.14 and will fail to build.</p> <pre><code>cd worker_plan\npython3.13 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e .\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m worker_plan.app\n</code></pre> <p>The app reads configuration from the <code>.env</code> file (located in the project root or <code>PLANEXE_CONFIG_PATH</code>). Host and port default to <code>localhost:8000</code> and can be overridden via <code>PLANEXE_WORKER_HOST</code> and <code>PLANEXE_WORKER_PORT</code>.</p> <p>The frontend can then point at <code>http://localhost:8000</code> via <code>PLANEXE_WORKER_PLAN_URL</code>.</p> <p>If you hit <code>ModuleNotFoundError: No module named 'worker_plan'</code>, ensure you: - are in <code>PlanExe/worker_plan</code> (not a subfolder) - ran <code>pip install -e .</code> in this venv without errors - exported <code>PYTHONPATH=$PWD/..:$PYTHONPATH</code> before starting uvicorn (the package lives one level up when your CWD is <code>worker_plan</code>)</p> <p>If you must stay on Python 3.14, expect source builds and potential failures; exporting <code>PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1</code> before <code>pip install -e .</code> may allow wheels to build, but 3.13 is recommended for a smooth setup.</p>"},{"location":"developer/worker_plan/#environment-variables","title":"Environment variables","text":"Variable Default Purpose <code>PLANEXE_WORKER_HOST</code> <code>0.0.0.0</code> Host address the worker binds to (only when running via <code>python -m worker_plan.app</code>). <code>PLANEXE_WORKER_PORT</code> <code>8000</code> Port the worker listens on (only when running via <code>python -m worker_plan.app</code>). <code>PLANEXE_RUN_DIR</code> <code>run</code> Directory under which run output folders are created. <code>PLANEXE_HOST_RUN_DIR</code> (unset) Optional host path base returned in <code>display_run_dir</code> to hint where runs live on the host. <code>PLANEXE_CONFIG_PATH</code> <code>.</code> Working directory for the pipeline; used as the <code>cwd</code> when spawning <code>worker_plan_internal.plan.run_plan_pipeline</code>. <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT</code> <code>false</code> When <code>true</code>, pipe pipeline stdout/stderr to the worker logs instead of suppressing them. <code>PLANEXE_PURGE_ENABLED</code> <code>false</code> Enable the background scheduler that purges old run directories. <code>PLANEXE_PURGE_MAX_AGE_HOURS</code> <code>1</code> Maximum age (hours) of runs to delete when purging (scheduler and manual default). <code>PLANEXE_PURGE_INTERVAL_SECONDS</code> <code>3600</code> How often the purge scheduler runs when enabled. <code>PLANEXE_PURGE_RUN_PREFIX</code> <code>PlanExe_</code> Only purge runs whose IDs start with this prefix. <code>PLANEXE_LOG_LEVEL</code> <code>INFO</code> Sets the console log level for the worker API and the pipeline process. Accepted values are the standard logging levels (e.g., <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>). <p><code>PLANEXE_LOG_LEVEL</code> affects both the FastAPI worker and the spawned pipeline logs written to stdout. File logs in <code>run/&lt;id&gt;/log.txt</code> always include DEBUG and above.</p>"},{"location":"developer/worker_plan_database/","title":"Worker plan database","text":"<p>Subclass of the <code>worker_plan</code> service that runs the PlanExe pipeline with a Postgres database.</p> <ul> <li>Polls <code>TaskItem</code> rows, marks them processing, and runs the pipeline.</li> <li>Reports state/progress back to the DB and posts confirmations to MachAI.</li> <li>Uses the same <code>worker_plan_internal</code> code as <code>worker_plan</code>, plus the shared <code>database_api</code> models.</li> <li>Configure MachAI confirmation endpoints with <code>PLANEXE_IFRAME_GENERATOR_CONFIRMATION_PRODUCTION_URL</code> and <code>PLANEXE_IFRAME_GENERATOR_CONFIRMATION_DEVELOPMENT_URL</code> (both are required; the worker fails fast if missing).</li> </ul>"},{"location":"developer/worker_plan_database/#docker-usage","title":"Docker usage","text":"<ul> <li>Build/run single worker: <code>docker compose up --build worker_plan_database</code></li> <li>Run three workers (each with <code>PLANEXE_WORKER_ID=1/2/3</code>): <code>docker compose up -d worker_plan_database_1 worker_plan_database_2 worker_plan_database_3</code></li> <li>Reads <code>SQLALCHEMY_DATABASE_URI</code> when provided, otherwise builds one from:</li> <li><code>PLANEXE_POSTGRES_HOST|PORT|DB|USER|PASSWORD</code></li> <li>falls back to the <code>database_postgres</code> service defaults (<code>planexe/planexe</code> on port 5432)</li> <li>Logs stream to stdout with 12-factor style logging. Configure with <code>PLANEXE_LOG_LEVEL</code> (defaults to <code>INFO</code>).</li> <li>Volumes mounted in compose: <code>./run</code> (pipeline output), <code>.env</code>, <code>llm_config.json</code></li> <li>Entrypoint: <code>python -m worker_plan_database.app</code></li> </ul>"},{"location":"developer/worker_plan_database/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>worker_plan_database</code> so its dependencies stay isolated:</p> <pre><code>cd worker_plan_database\npython3.13 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e ../worker_plan\npip install -r requirements.txt\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m worker_plan_database.app\n</code></pre> <p>Run <code>deactivate</code> when you are done with the venv.</p> <p>The <code>PYTHONPATH</code> addition allows imports of <code>database_api</code> and <code>worker_plan_database</code> modules. The <code>pyrightconfig.json</code> and <code>.vscode/settings.json</code> configure the same paths for editor/IDE support. In Cursor/VS Code, select the interpreter from <code>.venv/bin/python</code> via Cmd+Shift+P \u2192 \"Python: Select Interpreter\".</p>"},{"location":"mcp/antigravity/","title":"Google Antigravity","text":"<p>Antigravity by Google.</p> <p>Antigravity MCP documentation</p>"},{"location":"mcp/antigravity/#interaction","title":"Interaction","text":"<p>My interaction history:</p> <ol> <li>tell me about the planexe mcp tool</li> <li>make 5 suggestions</li> <li>crisis response plan for yellow stone outbreak, please refine that</li> <li>I didn't meant outbreak, I meant vulcanic</li> <li>your prompt is a bit shorter than the example prompts</li> <li>go ahead create the plan</li> <li>stop that plan you are creating.</li> <li>now create the plan again, this time with ALL details. Last time you had FAST selected that would leave out most details.</li> <li>check status</li> <li>status</li> <li>status</li> <li>status</li> <li>download the report</li> <li>summarize the report</li> <li>does it correspond to your expectations?</li> </ol> <p>I had to manually ask about <code>check status</code> to get details how the plan creation was going. It's not something that Antigravity can do.</p> <p>The created plan is here: Yellowstone Evacuation</p>"},{"location":"mcp/antigravity/#prerequisites","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Antigravity if the PlanExe installation is incomplete.</p>"},{"location":"mcp/antigravity/#configuring-antigravity","title":"Configuring Antigravity","text":"<p>To configure Antigravity to use PlanExe, you need to add the MCP server configuration.</p> <ol> <li>Open Antigravity</li> <li>Click the \"...\" icon at the top of the Agent panel</li> <li>Select \"MCP Servers\"</li> <li>This opens the <code>mcp_config.json</code> file.</li> </ol> <p>Add the following <code>planexe</code> dictionary to your <code>mcpServers</code> configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet:</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Once you have saved the <code>mcp_config.json</code>. Then go to the <code>Manage MCP Servers</code> and click the refresh icon.</p> <p>If it doesn't work then ask on the PlanExe Discord for help.</p> <p>This is what it should look like: </p>"},{"location":"mcp/codex/","title":"OpenAI Codex","text":"<p>Guide for connecting codex with PlanExe via MCP.</p>"},{"location":"mcp/codex/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Codex.</li> <li>PlanExe MCP server reachable by Codex.</li> </ul>"},{"location":"mcp/codex/#quick-setup","title":"Quick setup","text":"<ol> <li>Start Codex.</li> <li>Ask for MCP tools.</li> <li>Call <code>prompt_examples</code> to get examples.</li> <li>Call <code>task_create</code> to start a plan.</li> </ol>"},{"location":"mcp/codex/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/codex/#success-criteria","title":"Success criteria","text":"<ul> <li>You can retrieve prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/codex/#interaction","title":"Interaction","text":"<p>In a terminal, start codex like this:</p> <pre><code>codex\n</code></pre> <p>Inside codex; these are my interactions:</p> <ol> <li>tell me about the mcp tools you have access to</li> <li>for planexe, get the prompt examples</li> <li>I want you to formulate a prompt about constructing a new variant of english where the worst inconsistencies have been fixed such as 11th vs 1st, 21st, 31st,   potentially eliminated such suffixes. And the pronounciation inconsistencies have been cleaned up. I want you to adhere to the planexe example prompts.</li> <li>it's not just the ordinals. try again</li> <li>go ahead create this plan</li> <li>status</li> <li>status</li> <li>status</li> <li>download both</li> <li>summarize the html file</li> </ol> <p>The created plan is here: Clean English</p>"},{"location":"mcp/codex/#prerequisites_1","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Cursor if the PlanExe installation is incomplete.</p>"},{"location":"mcp/codex/#configuring-codex","title":"Configuring Codex","text":"<p>OpenAI's MCP documentation</p> <p>This is the command template. Make sure you tweak it, before running it.</p> <pre><code>codex mcp add planexe --env PLANEXE_URL=\"http://localhost:8001/mcp\" --env PLANEXE_PATH=\"/Users/your-name/Desktop\" -- uv run --with mcp /path/to/PlanExe/mcp_local/planexe_mcp_local.py\n</code></pre> <p>Make these adjustments to the command line.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Verify that it's working.</p> <pre><code>codex mcp list  \nName Command Args Env Cwd Status Auth       \nplanexe  uv       run --with mcp /path/to/PlanExe/mcp_local/planexe_mcp_local.py  PLANEXE_PATH=*****, PLANEXE_URL=*****  -    enabled  Unsupported\n</code></pre>"},{"location":"mcp/cursor/","title":"Cursor","text":"<p>According to Cursor's wikipedia page:</p> <p>Several media outlets have described Cursor as a vibe coding app.</p> <p>And</p> <p>Cursor allows developers produce code from natural language instructions.</p>"},{"location":"mcp/cursor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cursor installed.</li> <li>PlanExe MCP server reachable by Cursor.</li> </ul>"},{"location":"mcp/cursor/#quick-setup","title":"Quick setup","text":"<ol> <li>Configure MCP in Cursor.</li> <li>Ask for prompt examples.</li> <li>Create a plan task and download the report.</li> </ol>"},{"location":"mcp/cursor/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/cursor/#success-criteria","title":"Success criteria","text":"<ul> <li>You can fetch prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/cursor/#video","title":"Video","text":"<p>Video (1m29s) - PlanExe inside Cursor</p> <p>Here I'm chatting with Cursor. Behind the scenes Cursor talks with PlanExe via MCP.</p> <p>In total it takes 18 minutes to create the plan. The boring parts have been cropped out.</p>"},{"location":"mcp/cursor/#interaction","title":"Interaction","text":"<p>My interaction with Cursor for creating a plan is like this:</p> <ol> <li>tell me about the planexe mcp tool you have access to</li> <li>I want you to come up with a good prompt</li> <li>I want something ala winter olympics in Italy 2026</li> <li>Slightly different idea. I want Denmark to switch from DKK to EUR. Use the persona of a person representing Denmark's ministers.</li> <li>go ahead create plan with all details</li> <li>wait for 18 minutes until the plan has been created</li> <li>download the plan</li> </ol> <p>Here is the created plan: DKK to EUR</p>"},{"location":"mcp/cursor/#prerequisites_1","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Cursor if the PlanExe installation is incomplete.</p>"},{"location":"mcp/cursor/#configuring-cursor","title":"Configuring Cursor","text":"<p>Go to <code>Cursor Settings</code> \u2192 <code>Tools &amp; MCP</code></p> <p>Click <code>New MCP Server</code>, which opens <code>.cursor/mcp.json</code></p> <p>Insert the following <code>planexe</code> dictionary inside the <code>mcpServers</code> dictionary. </p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Now Cursor is connected with PlanExe, and it looks like this. If it doesn't then ask on the PlanExe Discord for help.</p> <p></p>"},{"location":"mcp/lm_studio/","title":"LM Studio","text":"<p>LM Studio is available for Linux/macOS/Windows.</p> <p>You need a hefty computer for running models locally.</p>"},{"location":"mcp/lm_studio/#prerequisites","title":"Prerequisites","text":"<ul> <li>LM Studio installed.</li> <li>PlanExe MCP server reachable by LM Studio.</li> </ul>"},{"location":"mcp/lm_studio/#quick-setup","title":"Quick setup","text":"<ol> <li>Configure MCP in LM Studio.</li> <li>Ask for prompt examples.</li> <li>Create a plan task and download the report.</li> </ol>"},{"location":"mcp/lm_studio/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/lm_studio/#success-criteria","title":"Success criteria","text":"<ul> <li>You can fetch prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/lm_studio/#interaction","title":"Interaction","text":"<p>My interaction with LM Studio for creating a plan is like this:</p> <ol> <li>tell me about the planexe mcp tool</li> <li>fetch the example prompts</li> <li>based on the example prompts. I want you to create a plan prompt for a social media website inspired by Reddit, but instead of the target audience being humans, I want the target audience to be AI agents talking with other AI agents. And hanging out in different channels.</li> <li>go ahead create a plan</li> <li>check status</li> <li>what is progress now</li> <li>status</li> <li>how about now</li> <li>download the report</li> <li>also download the zip</li> </ol> <p>LM Studio cannot autonomously check status, so it's up to the user to ask for it to invoke the <code>task_status</code> tool.</p> <p>The created plan is here: AI AgentNet</p>"},{"location":"mcp/lm_studio/#prerequisites_1","title":"Prerequisites","text":"<p>Check that your LM Studio works with a model that support tools such as glm-4.7-flash.</p> <p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring LM Studio if the PlanExe installation is incomplete.</p>"},{"location":"mcp/lm_studio/#configuring-lm-studio","title":"Configuring LM Studio","text":"<p>Follow step 1, 2, 3, 4. This should open LM Studio's <code>mcp.json</code> editor.</p> <p></p> <p>Insert the following <code>planexe</code> dictionary inside the <code>mcpServers</code> dictionary. </p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Now LM Studio is connected with PlanExe. If it doesn't work then ask on the PlanExe Discord for help.</p>"},{"location":"mcp/mcp_details/","title":"PlanExe MCP Details","text":"<p>MCP is work-in-progress, and I (Simon Strandgaard, the developer) may change it as I see fit. If there is a particular tool you want. Write to me on the PlanExe Discord, and I will see what I can do.</p> <p>This document lists the MCP tools exposed by PlanExe and example prompts for agents.</p>"},{"location":"mcp/mcp_details/#overview","title":"Overview","text":"<ul> <li>The primary MCP server runs in the cloud (see <code>mcp_cloud</code>).</li> <li>The local MCP proxy (<code>mcp_local</code>) forwards calls to the server and adds a local download helper.</li> <li>Tool responses return JSON in both <code>content.text</code> and <code>structuredContent</code>.</li> </ul>"},{"location":"mcp/mcp_details/#tool-catalog-mcp_cloud","title":"Tool Catalog, <code>mcp_cloud</code>","text":""},{"location":"mcp/mcp_details/#prompt_examples","title":"prompt_examples","text":"<p>Returns around five example prompts that show what good prompts look like. Each sample is typically 300\u2013800 words: detailed context, requirements, and success criteria. Usually the AI does the heavy lifting: the user has a vague idea, the agent calls <code>prompt_examples</code>, then expands that idea into a high-quality prompt (300\u2013800 words). The prompt is shown to the user, who can ask for further changes or confirm it\u2019s good to go. When the user confirms, the agent then calls <code>task_create</code>. Shorter or vaguer prompts produce lower-quality plans.</p> <p>Example prompt: <pre><code>Get example prompts for creating a plan.\n</code></pre></p> <p>Example call: <pre><code>{}\n</code></pre></p> <p>Response includes <code>samples</code> (array of prompt strings, each 300\u2013800 words) and <code>message</code>.</p>"},{"location":"mcp/mcp_details/#task_create","title":"task_create","text":"<p>Create a new plan task.</p> <p>Example prompt:</p> <p>Create a plan for: Weekly meetup for humans where participants are randomly paired every 5 minutes...</p> <p>Example call: <pre><code>{\"prompt\": \"Weekly meetup for humans where participants are randomly paired every 5 minutes...\"}\n</code></pre></p> <p>Optional argument: <pre><code>speed_vs_detail: \"ping\" | \"fast\" | \"all\"\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_status","title":"task_status","text":"<p>Fetch status/progress and recent files for a task.</p> <p>Example prompt: <pre><code>Get status for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_stop","title":"task_stop","text":"<p>Request an active task to stop.</p> <p>Example prompt: <pre><code>Stop task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_file_info","title":"task_file_info","text":"<p>Return download metadata for report or zip artifacts.</p> <p>Example prompt: <pre><code>Get report info for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p> <p>Available artifacts: <pre><code>\"report\" | \"zip\"\n</code></pre></p>"},{"location":"mcp/mcp_details/#tool-catalog-mcp_local","title":"Tool Catalog, <code>mcp_local</code>","text":"<p>The local proxy exposes the same tools as the server, and adds:</p>"},{"location":"mcp/mcp_details/#task_download","title":"task_download","text":"<p>Download report or zip to a local path.</p> <p>Example prompt: <pre><code>Download the report for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#typical-flow","title":"Typical Flow","text":""},{"location":"mcp/mcp_details/#1-get-example-prompts","title":"1. Get example prompts","text":"<p>The user often starts with a vague idea. The AI calls <code>prompt_examples</code> first to see what good prompts look like (around five samples, 300\u2013800 words each), then expands the user\u2019s idea into a high-quality prompt and shows it to the user.</p> <p>Prompt: <pre><code>Get example prompts for creating a plan.\n</code></pre></p> <p>Tool call: <pre><code>{}\n</code></pre></p>"},{"location":"mcp/mcp_details/#2-create-a-plan","title":"2. Create a plan","text":"<p>The user reviews the prompt and either asks for further changes or confirms it\u2019s good to go. When the user confirms, the agent calls <code>task_create</code> with that prompt.</p> <p>Tool call: <pre><code>{\"prompt\": \"...\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#3-get-status","title":"3. Get status","text":"<p>Prompt: <pre><code>Get status for my latest task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#4-download-the-report","title":"4. Download the report","text":"<p>Prompt: <pre><code>Download the report for my task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mcp/mcp_setup/","title":"MCP setup","text":"<p>This is the shortest path to a working PlanExe MCP integration.</p>"},{"location":"mcp/mcp_setup/#1-understand-the-flow","title":"1. Understand the flow","text":"<ol> <li>Ask for prompt examples.</li> <li>Expand the user idea into a high\u2011quality prompt.</li> <li>Create the plan task.</li> <li>Poll for status.</li> <li>Download the report (HTML or zip).</li> </ol>"},{"location":"mcp/mcp_setup/#2-minimal-tool-usage","title":"2. Minimal tool usage","text":"<ol> <li><code>prompt_examples</code></li> <li><code>task_create</code></li> <li><code>task_status</code></li> <li><code>task_download</code></li> </ol>"},{"location":"mcp/mcp_setup/#3-success-criteria","title":"3. Success criteria","text":"<ul> <li>You can fetch example prompts.</li> <li>You can create a plan task.</li> <li>You can download the report artifact.</li> </ul>"},{"location":"mcp/mcp_setup/#next-steps","title":"Next steps","text":"<ul> <li>Full tool details: MCP details</li> <li>Reference schema: PlanExe MCP interface</li> <li>App setup guides: Cursor, Codex, LM Studio</li> </ul>"},{"location":"mcp/mcp_troubleshooting/","title":"MCP troubleshooting","text":"<p>Common MCP integration issues and fixes.</p>"},{"location":"mcp/mcp_troubleshooting/#cannot-create-a-plan","title":"Cannot create a plan","text":"<ul> <li>Ensure your prompt is detailed (300\u2013800 words).</li> <li>Some topics may be refused by the model (harmful, unethical, or dangerous requests).</li> <li>Try a smaller model or a more reliable paid model.</li> <li>Confirm the MCP server is reachable from your client.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#status-never-changes","title":"Status never changes","text":"<ul> <li>Long\u2011running plans are normal; retry after a few minutes.</li> <li>If it stalls, create a new task and compare behavior.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#download-fails","title":"Download fails","text":"<ul> <li>Confirm the download URL is reachable from your network.</li> <li>If you run locally, make sure any proxy or base URL is correct.</li> <li>Ensure <code>PLANEXE_PATH</code> is a valid directory and that you have write permissions.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#lowquality-output","title":"Low\u2011quality output","text":"<ul> <li>Increase prompt detail and constraints.</li> <li>Use a stronger model.</li> <li>Avoid \u201cvague\u201d or conflicting requirements.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#quota-or-billing-errors","title":"Quota or billing errors","text":"<p>If you see errors like:</p> <p>You exceeded your current quota, please check your plan and billing details</p> <p>Then:</p> <ul> <li>Verify your provider has active billing.</li> <li>Check usage limits or rate limits.</li> <li>Try a different model or provider.</li> </ul>"},{"location":"mcp/mcp_welcome/","title":"Welcome to PlanExe MCP","text":"<p>PlanExe MCP lets AI agents (and the tools you build) create strategic plans from a plain-English prompt. You send a goal; PlanExe produces a draft plan. The MCP user then chooses whether to download the HTML report or a zip of intermediary files (JSON, MD, CSV) used to build that report.</p> <p>No MCP experience is required to get started.</p>"},{"location":"mcp/mcp_welcome/#who-this-is-for","title":"Who this is for","text":"<ul> <li>You\u2019re an AI agent \u2014 You have access to PlanExe\u2019s tools and want to create a plan for the user.</li> <li>You\u2019re building an agent or integration \u2014 You want to connect your app or assistant to PlanExe and need a gentle overview before diving into technical details.</li> </ul>"},{"location":"mcp/mcp_welcome/#what-you-can-do","title":"What you can do","text":"<ul> <li>Get example prompts \u2014 See what good prompts look like (detailed, typically 300\u2013800 words). It is the caller\u2019s responsibility to take inspiration from these examples and ensure the prompt sent to PlanExe is of similar or better quality. The agent can refine a vague idea into a high-quality prompt and show it to the user for approval before creating the plan.</li> <li>Create a plan \u2014 Send a prompt; PlanExe starts creating the plan (takes about 15\u201320 minutes). If the input prompt is of low quality, the output plan will be crap too.</li> <li>Check progress \u2014 Ask for status and see how far the plan has gotten.</li> <li>Download the report \u2014 When the plan is ready, the user specifies whether to download the HTML report or the zip of intermediary files (JSON, MD, CSV).</li> </ul>"},{"location":"mcp/mcp_welcome/#what-you-get","title":"What you get","text":"<p>The MCP user chooses which artifact to download:</p> <ul> <li>HTML report (around 40 pages) \u2014 executive summary, Gantt chart, risks, next steps, and more. Opens in a browser.</li> <li>Zip \u2014 intermediary files (JSON, MD, CSV) used to build the HTML report, for deeper inspection.</li> </ul>"},{"location":"mcp/mcp_welcome/#next-steps","title":"Next steps","text":"<ul> <li>Setup \u2014 MCP setup: recommended path to a working integration.</li> <li>See the tools and a typical flow \u2014 MCP details: tool list, example prompts, and step-by-step flow without heavy protocol detail.</li> <li>Set up in Cursor \u2014 Cursor: video, prerequisites, and how to connect PlanExe to Cursor.</li> <li>Set up in Windsurf \u2014 Windsurf: setup steps and example interaction.</li> <li>Set up in LM Studio \u2014 LM Studio: setup steps and example interaction.</li> <li>Set up in Codex \u2014 Codex: setup steps and example interaction.</li> <li>Set up in Antigravity \u2014 Antigravity: setup steps and example interaction.</li> <li>Full technical specification \u2014 PlanExe MCP interface: for implementors; request/response schemas, state machine, error codes, and compatibility rules.</li> <li>Troubleshooting \u2014 MCP troubleshooting: common integration issues and fixes.</li> </ul>"},{"location":"mcp/mcp_welcome/#get-help","title":"Get help","text":"<p>If something doesn\u2019t work or you\u2019re unsure how to integrate, ask on the PlanExe Discord. Include what you tried, your setup, and any error output.</p>"},{"location":"mcp/planexe_mcp_interface/","title":"PlanExe MCP Interface Specification (v1.0)","text":""},{"location":"mcp/planexe_mcp_interface/#1-purpose","title":"1. Purpose","text":""},{"location":"mcp/planexe_mcp_interface/#11-what-is-planexe","title":"1.1 What is PlanExe","text":"<p>PlanExe is a service that generates rough-draft project plans from a natural-language prompt. You describe a large goal (e.g. open a clinic, launch a product, build a moon base)\u2014the kind of project that in reality takes months or years. PlanExe produces a structured draft: steps, documents, and deliverables. The plan is not executable in its current form; it is a draft to refine and act on. Creating a plan is a long-running task (100+ LLM inference calls): create a task with a prompt, poll status, then download the HTML report and zip when done.</p>"},{"location":"mcp/planexe_mcp_interface/#12-what-kind-of-plan-does-it-create","title":"1.2 What kind of plan does it create","text":"<p>The plan is a project plan: a DAG of steps (Luigi tasks) that produce artifacts including a Gantt chart, risk analysis, and other project management deliverables. The main output is a large HTML file (approx 700KB) containing many sections. There is also a zip file containing all intermediary files (md, json, csv). Plan quality depends on prompt quality; use the prompt_examples tool to see the baseline before calling task_create.</p>"},{"location":"mcp/planexe_mcp_interface/#121-agent-facing-summary-for-server-instructions-tool-descriptions","title":"1.2.1 Agent-facing summary (for server instructions / tool descriptions)","text":"<p>Implementors should expose the following to agents so they understand what PlanExe does:</p> <ul> <li>What: PlanExe turns a plain-English goal into a structured strategic-plan draft (executive summary, Gantt, risk register, governance, etc.) in ~15\u201320 min. The plan is a draft to refine, not an executable or final document.</li> <li>Required interaction order: Step 1 \u2014 Call prompt_examples to fetch example prompts. Step 2 \u2014 Formulate a good prompt (use examples as a baseline; similar structure; get user approval). Step 3 \u2014 Only then call task_create with the approved prompt. Then poll task_status; use task_download or task_file_info when complete. To stop, call task_stop with the task_id from task_create.</li> <li>Output: Large HTML report (~700KB) and optional zip of intermediate files (md, json, csv).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#13-scope-of-this-document","title":"1.3 Scope of this document","text":"<p>This document specifies a Model Context Protocol (MCP) interface for PlanExe that enables AI agents and client UIs to:</p> <ol> <li>Create and run long-running plan generation workflows.</li> <li>Receive real-time progress updates (task status, log output).</li> <li>List, read, and edit artifacts produced in an output directory.</li> <li>Stop and resume execution with Luigi-aware incremental recomputation.</li> </ol> <p>The interface is designed to support:</p> <ul> <li>interactive \"build systems\" behavior (like make / bazel),</li> <li>resumable DAG execution (Luigi),</li> <li>deterministic artifact management.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#2-goals","title":"2. Goals","text":""},{"location":"mcp/planexe_mcp_interface/#21-functional-goals","title":"2.1 Functional goals","text":"<ul> <li>Task-based orchestration: each run is associated with a task ID.</li> <li>Long-running execution: starts asynchronously; clients poll or subscribe to events.</li> <li>Artifact-first workflow: outputs are exposed as file-like artifacts.</li> <li>Stop / Resume with minimal recompute:</li> <li>on resume, only invalidated downstream tasks regenerate.</li> <li>Progress reporting:</li> <li>progress_percentage</li> <li>Editable artifacts:</li> <li>user edits a generated file</li> <li>pipeline continues from that point, producing dependent outputs</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#22-non-functional-goals","title":"2.2 Non-functional goals","text":"<ul> <li>Idempotency: repeated tool calls should not corrupt state.</li> <li>Observability: logs, state transitions, and artifacts must be inspectable.</li> <li>Concurrency safety: prevent conflicting writes and illegal resume patterns.</li> <li>Extensibility: future versions can add task graph browsing, caching backends, exports.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#3-non-goals","title":"3. Non-goals","text":"<ul> <li>Defining PlanExe's internal plan schema, content format, or prompt strategy.</li> <li>Providing remote code execution inside artifacts.</li> <li>Implementing a full Luigi UI clone in MCP v1 (optional later).</li> <li>Guaranteeing ETA estimates (allowed but must be optional / best-effort).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#31-mcp-tools-vs-mcp-tasks-run-as-task","title":"3.1 MCP tools vs MCP tasks (\"Run as task\")","text":"<p>The MCP specification defines two different mechanisms:</p> <ul> <li>MCP tools (e.g. task_create, task_status, task_stop): the server exposes named tools; the client calls them and receives a response. PlanExe's interface is tool-based: the agent calls task_create \u2192 receives task_id \u2192 polls task_status \u2192 uses task_download. This document specifies those tools.</li> <li>MCP tasks protocol (\"Run as task\" in some UIs): a separate mechanism where the client can run a tool \"as a task\" using RPC methods such as tasks/run, tasks/get, tasks/result, tasks/cancel, tasks/list, so the tool runs in the background and the client polls for results.</li> </ul> <p>PlanExe does not use or advertise the MCP tasks protocol. Implementors and clients should use the tools only. Do not enable \"Run as task\" for PlanExe; many clients (e.g. Cursor) and the Python MCP SDK do not support the tasks protocol properly. The intended flow is: Step 1 \u2014 call prompt_examples; Step 2 \u2014 formulate a good prompt (user approval); Step 3 \u2014 call task_create; then poll task_status and call task_download when complete.</p>"},{"location":"mcp/planexe_mcp_interface/#4-system-model","title":"4. System Model","text":""},{"location":"mcp/planexe_mcp_interface/#41-core-entities","title":"4.1 Core entities","text":""},{"location":"mcp/planexe_mcp_interface/#task","title":"Task","text":"<p>A long-lived container for a PlanExe project run.</p> <p>Key properties</p> <ul> <li>task_id: UUID returned by task_create for that task. Each task_create returns a new UUID. Use that exact UUID for all MCP calls; do not substitute ids from other services.</li> <li>output_dir: artifact root namespace for task</li> <li>config: immutable run configuration (models, runtime limits, Luigi params)</li> <li>created_at, updated_at</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#run","title":"Run","text":"<p>A single execution attempt inside a task (e.g., after a resume).</p> <p>Key properties</p> <ul> <li>state: running | stopped | completed | failed</li> <li>progress_percentage: computed progress percentage (float)</li> <li>started_at, ended_at</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#artifact","title":"Artifact","text":"<p>A file-like output managed by PlanExe.</p> <p>Key properties</p> <ul> <li>path: path relative to task output root</li> <li>size, updated_at</li> <li>content_type: text/markdown, text/html, application/json, etc.</li> <li>sha256: content hash for optimistic locking and invalidation</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#event","title":"Event","text":"<p>A typed message emitted during execution for UI/agent consumption.</p> <p>Key properties</p> <ul> <li>cursor: ordering token</li> <li>ts: timestamp</li> <li>type: event type</li> <li>data: event payload</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#5-state-machine","title":"5. State Machine","text":""},{"location":"mcp/planexe_mcp_interface/#51-task-states","title":"5.1 Task states","text":"<p>Tasks may exist independent of active runs.</p> <ul> <li>created: task initialized, no run started</li> <li>active: at least one run exists, may be running or stopped</li> <li>archived: optional; immutable, no new runs allowed</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#52-run-states","title":"5.2 Run states","text":"<ul> <li>running</li> <li>stopping (optional transitional state)</li> <li>stopped (user stopped, resumable)</li> <li>completed</li> <li>failed (resumable depending on failure type)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#53-allowed-transitions","title":"5.3 Allowed transitions","text":"<ul> <li>running \u2192 stopped via task_stop</li> <li>running \u2192 completed via normal success</li> <li>running \u2192 failed via error</li> </ul> <p>Invalid</p> <ul> <li>completed \u2192 running (new run must be triggered by creating a new task)</li> <li>running \u2192 running (no concurrent runs in v1)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#6-mcp-tools-v1-required","title":"6. MCP Tools (v1 Required)","text":"<p>All tool names below are normative.</p>"},{"location":"mcp/planexe_mcp_interface/#61-prompt_examples","title":"6.1 prompt_examples","text":"<p>Step 1 \u2014 Call this first. Returns example prompts that define the baseline for what a good prompt looks like. Do not call task_create yet. Correct flow: Step 1 \u2014 call this tool to fetch examples. Step 2 \u2014 Formulate a good prompt (use examples as a baseline; similar structure; get user approval). Step 3 \u2014 Only then call task_create with the approved prompt. If you call task_create before formulating and approving a prompt, the resulting plan will be lower quality than it could be.</p> <p>Request: no parameters (empty object).</p> <p>Response:</p> <pre><code>{\n  \"samples\": [\"prompt text 1\", \"prompt text 2\", \"...\"],\n  \"message\": \"...\"\n}\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#62-task_create","title":"6.2 task_create","text":"<p>Step 3 \u2014 Call only after prompt_examples (Step 1) and after you have formulated a good prompt and got user approval (Step 2). Start creating a new plan with the approved prompt. speed_vs_detail modes: 'all' runs the full pipeline with all details (slower, higher token usage/cost). 'fast' runs the full pipeline with minimal work per step (faster, fewer details), useful to verify the pipeline is working. 'ping' runs the pipeline entrypoint and makes a single LLM call to verify the worker_plan_database is processing tasks and can reach the LLM.</p> <p>Request</p> <p>Schema</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"prompt\": { \"type\": \"string\" },\n    \"speed_vs_detail\": {\n      \"type\": \"string\",\n      \"enum\": [\"ping\", \"fast\", \"all\"],\n      \"default\": \"ping\"\n    }\n  },\n  \"required\": [\"prompt\"]\n}\n</code></pre> <p>Example</p> <pre><code>{\n  \"prompt\": \"string\",\n  \"speed_vs_detail\": \"ping\"\n}\n</code></pre> <p>Prompt quality</p> <p>The <code>prompt</code> parameter should be a detailed description of what the plan should cover. Good prompts are typically 300\u2013800 words and include:</p> <ul> <li>Clear context: background, constraints, and goals</li> <li>Specific requirements: budget, timeline, location, or technical constraints</li> <li>Success criteria: what \"done\" looks like</li> <li>Banned words or approaches (if any)</li> </ul> <p>Short one-liners (e.g., \"Construct a bridge\") tend to produce poor output because they lack context for the planning pipeline. Important details are location, budget, time frame.</p> <p>Clients can call the MCP tool prompt_examples to retrieve example prompts. Use these as examples for task_create; they can also call task_create with any prompt\u2014short prompts produce less detailed plans.</p> <p>For the full catalog file:</p> <ul> <li><code>worker_plan/worker_plan_api/prompt/data/simple_plan_prompts.jsonl</code> \u2014 JSONL with <code>id</code>, <code>prompt</code>, optional <code>tags</code>, and optional <code>mcp_example</code> (true = curated for MCP).</li> </ul> <p>Response</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",\n  \"created_at\": \"2026-01-14T12:34:56Z\"\n}\n</code></pre> <p>Important</p> <ul> <li>task_id is a UUID returned by task_create. Use this exact UUID for task_status/task_stop/task_download.</li> </ul> <p>Behavior</p> <ul> <li>Must be idempotent only if client supplies an optional client_request_id (optional extension).</li> <li>Task config is immutable after creation in v1.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#63-task_status","title":"6.3 task_status","text":"<p>Returns run status and progress. Used for progress bars and UI states. Polling interval: call at reasonable intervals only (e.g. every 5 minutes); plan generation takes 15\u201320+ minutes and frequent polling is unnecessary.</p> <p>Request</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\"\n}\n</code></pre> <p>Input</p> <ul> <li>task_id: UUID returned by task_create. Use it to reference the plan being created.</li> </ul> <p>Response</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",\n  \"state\": \"running\",\n  \"progress_percentage\": 62.0,\n  \"timing\": {\n    \"started_at\": \"2026-01-14T12:35:10Z\",\n    \"elapsed_sec\": 512\n  },\n  \"files\": [\n    {\n      \"path\": \"plan.md\",\n      \"updated_at\": \"2026-01-14T12:43:11Z\"\n    }\n  ]\n}\n</code></pre> <p>Notes</p> <ul> <li>progress_percentage must be a float within [0,100].</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#64-task_stop","title":"6.4 task_stop","text":"<p>Requests the plan generation to stop. Pass the task_id (the UUID returned by task_create). This is a normal MCP tool call: call task_stop with that task_id.</p> <p>Request</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\"\n}\n</code></pre> <p>Input</p> <ul> <li>task_id: UUID returned by task_create. Use this same UUID when calling task_stop to request the run to stop.</li> </ul> <p>Response</p> <pre><code>{\n  \"state\": \"stopped\"\n}\n</code></pre> <p>Required semantics</p> <ul> <li>Must stop workers cleanly where possible.</li> <li>Must persist enough Luigi state to resume incrementally.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#65-download-flow-task_download-vs-task_file_info","title":"6.5 Download flow (task_download vs task_file_info)","text":"<p>If your client exposes task_download (e.g. mcp_local): use it to save the report or zip locally; it calls task_file_info under the hood, then fetches and writes to the local save path (e.g. PLANEXE_PATH).</p> <p>If you only have task_file_info (e.g. direct connection to mcp_cloud): call it with task_id and artifact (\"report\" or \"zip\"); use the returned download_url to fetch the file (e.g. GET with API key if configured).</p> <p>task_file_info input</p> <ul> <li>task_id: UUID returned by task_create. Use it to download the created plan.</li> <li>artifact: \"report\" or \"zip\" (default \"report\").</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#7-targets","title":"7. Targets","text":""},{"location":"mcp/planexe_mcp_interface/#71-standard-targets","title":"7.1 Standard targets","text":"<p>The following targets MUST be supported:</p> <ul> <li>build_plan</li> <li>validate_plan</li> <li>build_plan_and_validate</li> </ul> <p>Targets map to Luigi \"final tasks\".</p>"},{"location":"mcp/planexe_mcp_interface/#8-concurrency-locking","title":"8. Concurrency &amp; Locking","text":""},{"location":"mcp/planexe_mcp_interface/#81-single-active-run-per-task","title":"8.1 Single active run per task","text":"<p>In v1, tasks MUST enforce:</p> <ul> <li>at most one run in running state.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#9-error-model","title":"9. Error Model","text":"<p>Errors MUST return:</p> <ul> <li>code: stable machine-readable</li> <li>message: human-readable</li> <li>details: optional</li> </ul> <p>Example:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"RUN_ALREADY_ACTIVE\",\n    \"message\": \"A run is currently active for this task.\",\n    \"details\": { \"run_id\": \"run_0001\" }\n  }\n}\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#91-required-error-codes","title":"9.1 Required error codes","text":"<ul> <li>TASK_NOT_FOUND</li> <li>RUN_NOT_FOUND</li> <li>RUN_ALREADY_ACTIVE</li> <li>RUN_NOT_ACTIVE</li> <li>INVALID_TARGET</li> <li>INVALID_ARTIFACT_URI</li> <li>CONFLICT</li> <li>PERMISSION_DENIED</li> <li>RUNNING_READONLY</li> <li>INTERNAL_ERROR</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#10-security-isolation","title":"10. Security &amp; Isolation","text":""},{"location":"mcp/planexe_mcp_interface/#101-sandbox-constraints","title":"10.1 Sandbox constraints","text":"<ul> <li>All artifacts must live under task-scoped storage.</li> <li>Artifact URIs must not permit path traversal.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#102-access-control","title":"10.2 Access control","text":"<p>At minimum:</p> <ul> <li>task must be scoped to a user identity (metadata.user_id)</li> <li>callers without permission must receive PERMISSION_DENIED</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#103-sensitive-data-handling","title":"10.3 Sensitive data handling","text":"<ul> <li>logs may include model prompts/responses \u2192 treat logs as sensitive artifacts</li> <li>allow a config option to redact prompt content in event streaming</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#11-performance-requirements","title":"11. Performance Requirements","text":""},{"location":"mcp/planexe_mcp_interface/#111-responsiveness","title":"11.1 Responsiveness","text":"<ul> <li>task_status must return within &lt; 250ms under normal load.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#112-large-artifacts","title":"11.2 Large artifacts","text":"<ul> <li>server SHOULD impose max read size per call (e.g., 2\u201310MB)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#12-observability-requirements","title":"12. Observability Requirements","text":"<p>The server MUST persist:</p> <ul> <li>run lifecycle events</li> <li>stop reasons</li> <li>failure tracebacks as artifacts (e.g., run_error.json)</li> <li>luigi execution logs (run.log)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#13-reference-ui-integration-contract","title":"13. Reference UI Integration Contract","text":"<p>To match your UI behavior:</p> <p>Progress bars</p> <p>Use:</p> <ul> <li>task_status.progress_percentage</li> <li>or progress_updated events</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#14-compatibility-versioning","title":"14. Compatibility &amp; Versioning","text":""},{"location":"mcp/planexe_mcp_interface/#141-versioning-strategy","title":"14.1 Versioning strategy","text":"<ul> <li>MCP server exposes: planexe.version = \"1.0\"</li> <li>breaking changes require major bump</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#142-forward-compatibility","title":"14.2 Forward compatibility","text":"<p>Clients must ignore unknown fields and unknown event types.</p>"},{"location":"mcp/planexe_mcp_interface/#15-testing-strategy","title":"15. Testing Strategy","text":""},{"location":"mcp/planexe_mcp_interface/#151-contract-tests-required","title":"15.1 Contract tests (required)","text":"<ul> <li>Start/stop/resume loops</li> <li>Invalid transition errors</li> <li>Event cursor monotonicity</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#152-determinism-tests-recommended","title":"15.2 Determinism tests (recommended)","text":"<ul> <li>Given same inputs + same edits, ensure same downstream artifacts unless models are stochastic</li> <li>If models are stochastic, test pipeline correctness, not identical bytes</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#153-load-tests-recommended","title":"15.3 Load tests (recommended)","text":"<ul> <li>multiple tasks concurrently, one run each</li> <li>event streaming stability under heavy log output</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#16-future-extensions-mcp-resources","title":"16. Future Extensions (MCP Resources)","text":"<p>PlanExe is artifact-first, and MCP already has a native concept for that: resources. Today artifacts are exposed via download_url or via proxy download + saved_path. Future versions SHOULD expose artifacts as MCP resources so clients can fetch them via standard resource reads (and treat PlanExe as a first-class MCP server rather than a thin API wrapper).</p> <p>Proposed resource identifiers</p> <ul> <li>planexe://task//report <li>planexe://task//zip <p>Recommended resource metadata</p> <ul> <li>mime type (content_type)</li> <li>size (bytes)</li> <li>sha256 (content hash)</li> <li>generated_at (UTC timestamp)</li> </ul> <p>Notes</p> <ul> <li>Resources can be backed by existing HTTP endpoints internally; the MCP resource read returns the bytes + metadata.</li> <li>This enables richer MCP client UX (preview, caching, validation) without custom tool calls.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#17-future-tools-high-leverage-low-complexity","title":"17. Future Tools (High-Leverage, Low-Complexity)","text":"<p>The following tools remove common UX friction without expanding the core model.</p>"},{"location":"mcp/planexe_mcp_interface/#171-task_list-or-task_recent","title":"17.1 task_list (or task_recent)","text":"<p>Return a short list of recent tasks so agents can recover if they lost a task_id.</p> <p>Notes</p> <ul> <li>Default limit: 5\u201310 tasks.</li> <li>Include task_id, created_at, state, and prompt summary.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#172-task_wait","title":"17.2 task_wait","text":"<p>Blocking helper that polls internally until the task completes or times out. Returns the final task_status payload plus suggested next steps.</p> <p>Notes</p> <ul> <li>Inputs: task_id, timeout_sec (optional), poll_interval_sec (optional).</li> <li>Outputs: same as task_status + next_steps (string or list).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#173-task_get_latest","title":"17.3 task_get_latest","text":"<p>Simplest recovery: return the most recently created task for the caller.</p> <p>Notes</p> <ul> <li>Useful for single-user / single-session flows.</li> <li>Should be scoped to the caller/user_id when available.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#174-task_logs_tail-optional","title":"17.4 task_logs_tail (optional)","text":"<p>Return the tail of recent log lines for troubleshooting failures.</p> <p>Notes</p> <ul> <li>Inputs: task_id, max_lines (optional), since_cursor (optional).</li> <li>Useful when task_status shows failed but no context.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#appendix-a-example-end-to-end-flow","title":"Appendix A \u2014 Example End-to-End Flow","text":"<p>Create task</p> <pre><code>{ \"prompt\": \"...\" }\n</code></pre> <p>Start run</p> <pre><code>{ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }\n</code></pre> <p>Stop</p> <pre><code>{ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#appendix-b-optional-v11-extensions","title":"Appendix B \u2014 Optional v1.1 Extensions","text":"<p>If you want richer Luigi integration later:</p> <ul> <li>planexe.task.graph (nodes + edges + states)</li> <li>planexe.task.invalidate (rerun subtree)</li> <li>planexe.export.bundle (zip all artifacts)</li> <li>planexe.validate.only (audit without regeneration)</li> <li>planexe.task.archive (freeze task)</li> </ul>"},{"location":"mcp/windsurf/","title":"Windsurf","text":"<p>Windsurf.</p> <p>Windsurf MCP documentation</p> <p>Windsurf MCP tutorial</p>"},{"location":"mcp/windsurf/#interaction","title":"Interaction","text":"<p>My interaction history:</p> <ol> <li>get planexe example prompts</li> <li>I want you to suggest 5 prompts, based on the example prompts</li> <li>suggest something that fixes real world problems</li> <li>5 more</li> <li>I'm in europe. make 5 suggestions that fixes serious issues in europe</li> <li>I like your Heatwave mortality reduction idea. I want you to make a full prompt ala the planexe example prompts, and show me the prompt</li> <li>remove the heading \"Full PlanExe-style prompt: Heatwave mortality reduction (Europe)\". what do you think about the prompt?</li> <li>go ahead create this plan</li> <li>status</li> <li>status     Here windsurf went ahead and downloaded the created HTML report</li> <li>compare the created plan with the prompt you formulated</li> <li>also download the zip</li> </ol> <p>I had to manually ask about <code>check status</code> to get details how the plan creation was going. It's not something that Windsurf can do.</p>"},{"location":"mcp/windsurf/#prerequisites","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Windsurf if the PlanExe installation is incomplete.</p>"},{"location":"mcp/windsurf/#configuring-windsurf","title":"Configuring Windsurf","text":"<p>To configure Windsurf to use PlanExe, you need to add the MCP server configuration.</p> <p></p> <ol> <li>Open Windsurf</li> <li>Click the \"...\" icon at the top of the Agent panel, this opens a menu.</li> <li>Click the \"Open MCP Config File\" icon at the bottom of the menu.</li> <li>This opens the <code>mcp_config.json</code> file.</li> </ol> <p>Add the following <code>planexe</code> dictionary to your <code>mcpServers</code> configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet:</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Once you have saved the <code>mcp_config.json</code>. Then go to the <code>Manage MCP Servers</code> and click the refresh icon.</p> <p>If it doesn't work then ask on the PlanExe Discord for help.</p>"}]}