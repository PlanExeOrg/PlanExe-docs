{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to PlanExe Documentation","text":"<p>PlanExe is an open-source tool that turns a single plain-English goal into a ~40-page strategic plan in about 15 minutes, using local or cloud LLMs. It\u2019s an accelerator for first drafts\u2014not a replacement for human refinement. Typical output includes an executive summary, Gantt chart, governance structure, stakeholder maps, risk registers, and SWOT analyses. PlanExe removes most of the labor for the planning scaffold; the final 10\u201330% that makes a plan credible and defensible remains human work.</p> <p>See example plans (e.g. Minecraft escape room, Faraday cage company, Human-as-a-Service pilot). Use the navigation on the left to explore setup, usage, and reference.</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>PlanExe website</li> <li>GitHub repository</li> <li>Discord community</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>Install PlanExe, configure your AI provider, and generate your first plan. Browse the sections in the sidebar for setup guides, usage, and reference.</p>"},{"location":"AGENTS/","title":"How PlanExe-docs Builds and Publishes to docs.planexe.org","text":"<p>This document describes how the PlanExe-docs repository takes content from this directory (<code>PlanExe/docs/</code>) and publishes it to https://docs.planexe.org.</p>"},{"location":"AGENTS/#overview","title":"Overview","text":"<ul> <li>Content source: This directory (<code>PlanExe/docs/</code>). All Markdown files, images, and assets here become the published documentation.</li> <li>Build &amp; deploy: The PlanExe-docs repo. It holds MkDocs config, GitHub Actions workflow, and build scripts.</li> <li>Output: Static site served via GitHub Pages at docs.planexe.org.</li> </ul>"},{"location":"AGENTS/#pipeline-ci","title":"Pipeline (CI)","text":"<ol> <li>Trigger    The Deploy Documentation workflow runs when:</li> <li>There is a push to <code>main</code> on PlanExe-docs, or</li> <li>It is started manually (<code>workflow_dispatch</code>), or</li> <li>A <code>repository_dispatch</code> event <code>docs-updated</code> is sent (e.g. when PlanExe is updated and you want to redeploy docs).</li> </ol> <p>Pushing only to PlanExe does not by itself update docs.planexe.org. This repo has a workflow (<code>.github/workflows/docs-update.yml</code>) that runs on push to <code>main</code> when <code>docs/</code> changes and sends <code>repository_dispatch</code> to PlanExe-docs. For that to work you must add a secret in PlanExe (see below). Otherwise, after editing <code>PlanExe/docs/</code>, either run the Deploy workflow manually in PlanExe-docs, or push to PlanExe-docs <code>main</code> (e.g. after syncing content) to deploy.</p> <ol> <li>Checkout </li> <li>PlanExe-docs repo (workflow, <code>mkdocs.yml</code>, <code>requirements.txt</code>, etc.).  </li> <li> <p>PlanExe repo into <code>planexe-source/</code> (so this <code>docs/</code> directory is available).</p> </li> <li> <p>Build </p> </li> <li><code>mkdir -p docs</code> in the PlanExe-docs workspace.  </li> <li><code>cp -r planexe-source/docs/* docs/</code> \u2014 all content from this <code>PlanExe/docs/</code> directory is copied into PlanExe-docs\u2019 <code>docs/</code> folder.  </li> <li> <p><code>mkdocs build --site-dir site</code> \u2014 MkDocs (Material theme, config from <code>mkdocs.yml</code>) builds the site into <code>site/</code>.</p> </li> <li> <p>Deploy </p> </li> <li>The peaceiris/actions-gh-pages action publishes the <code>site/</code> directory to the gh-pages branch of PlanExe-docs.  </li> <li>Custom domain docs.planexe.org is set via <code>cname: docs.planexe.org</code> in the workflow.  </li> <li>GitHub Pages serves the site from that branch, so updates appear at https://docs.planexe.org.</li> </ol>"},{"location":"AGENTS/#key-files","title":"Key files","text":"What Where Doc content (you edit here) <code>PlanExe/docs/</code> (this directory) MkDocs config, theme, plugins PlanExe-docs <code>mkdocs.yml</code> Deploy workflow PlanExe-docs <code>.github/workflows/deploy.yml</code> Build dependencies PlanExe-docs <code>requirements.txt</code> Frontpage <code>PlanExe/docs/index.md</code> (used as site index)"},{"location":"AGENTS/#local-preview","title":"Local preview","text":"<p>To build and preview the same site locally:</p> <ol> <li>Clone both PlanExe and PlanExe-docs.  </li> <li>From PlanExe-docs, run <code>python build.py</code> (optionally set <code>PLANEXE_REPO</code> if PlanExe is not at <code>../PlanExe</code>).  </li> <li>This copies <code>PlanExe/docs/</code> into a temp <code>docs/</code> dir, runs <code>mkdocs build</code>, and writes output to <code>site/</code>.  </li> <li>Run <code>python serve.py</code> to serve <code>site/</code> at <code>http://127.0.0.1:18525/</code>.</li> </ol>"},{"location":"AGENTS/#auto-deploy-from-planexe-optional","title":"Auto-deploy from PlanExe (optional)","text":"<p>To have the live site update when you push to PlanExe <code>main</code> with changes under <code>docs/</code>:</p> <ol> <li>In PlanExe repo: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret.</li> <li>Name: <code>PLANEXE_DOCS_DISPATCH_TOKEN</code>. Value: a Personal Access Token (or fine-grained PAT) with repo scope for PlanExeOrg/PlanExe-docs (or at least permission to trigger workflows in PlanExe-docs).</li> <li>Push to PlanExe <code>main</code> with changes under <code>docs/</code>. The workflow <code>.github/workflows/docs-update.yml</code> runs and sends <code>repository_dispatch</code> to PlanExe-docs; PlanExe-docs then checks out PlanExe, copies <code>docs/</code>, builds, and deploys.</li> </ol> <p>If the secret is not set, the \"Notify docs deploy\" workflow in PlanExe will fail at the dispatch step. You can still update the live site by running the Deploy Documentation workflow manually in PlanExe-docs (Actions \u2192 Deploy Documentation \u2192 Run workflow), or by pushing to PlanExe-docs <code>main</code>.</p>"},{"location":"AGENTS/#summary","title":"Summary","text":"<p>Edits in PlanExe/docs/ are what get published. PlanExe-docs orchestrates copy \u2192 MkDocs build \u2192 GitHub Pages deploy to docs.planexe.org. Push to PlanExe-docs <code>main</code>, trigger the Deploy workflow manually in PlanExe-docs, or set up <code>PLANEXE_DOCS_DISPATCH_TOKEN</code> in PlanExe so pushes to <code>docs/</code> auto-trigger the deploy.</p>"},{"location":"docker/","title":"PlanExe uses Docker","text":"<p>I'm no fan of Docker and I tried to avoid it. I first tried dependency hell, and it was fragile. Incompatible packages, preventing installing/upgrading things. I had to choose between dependency hell or docker hell. Now I'm going with Docker. Hopefully it turns out to be less fragile.</p>"},{"location":"docker/#basic-lifecycle","title":"Basic lifecycle","text":"<ul> <li>Stop everything: <code>docker compose down</code></li> <li>Build fresh (no cache) after code moves: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user</code></li> <li>Start services: <code>docker compose up</code></li> <li>Stop services (leave images): <code>docker compose down</code></li> <li>Build fresh and start services: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user &amp;&amp; docker compose up</code></li> </ul>"},{"location":"docker/#while-developing","title":"While developing","text":"<ul> <li>Live rebuild/restart on changes: <code>docker compose watch</code> (requires Docker Desktop 4.28+).   If watch misses changes after file moves, rerun the no-cache build above.</li> <li>View logs: </li> <li><code>docker compose logs -f worker_plan</code></li> <li><code>docker compose logs -f frontend_single_user</code></li> <li><code>docker compose logs -f frontend_multi_user</code></li> </ul>"},{"location":"docker/#run-individual-files","title":"Run individual files","text":"<ul> <li>Rebuild the worker image when code or data files change: <code>docker compose build --no-cache worker_plan</code>.</li> <li>Run a one-off module inside the worker image (same deps/env as the API): <code>docker compose run --rm worker_plan python -m worker_plan_internal.fiction.fiction_writer</code> (swap the module path as needed). If containers are already up, use <code>docker compose exec worker_plan python -m ...</code> instead.</li> <li>For host Ollama access, set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:11434</code> (default Ollama port). On Linux, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> if that hostname is missing, or use your bridge IP.</li> <li>Ensure required env vars (e.g., <code>DEFAULT_LLM</code>) are available via <code>.env</code> or your shell before running the command.</li> </ul>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the pipeline stops immediately with missing module errors, rebuild with <code>--no-cache</code> so new files are inside the images.</li> <li>If you change environment variables (e.g., <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT</code>), restart: <code>docker compose down</code> then <code>docker compose up</code>.</li> <li>If <code>frontend_multi_user</code> can't start because host port 5000 is busy, map it elsewhere: <code>export PLANEXE_FRONTEND_MULTIUSER_PORT=5001</code> (or another free port) before <code>docker compose up</code>.</li> <li>To clean out containers, network, and orphans: <code>docker compose down --remove-orphans</code>.</li> <li>To reclaim disk space when builds start failing with <code>No space left on device</code>:</li> <li>See current usage: <code>docker system df</code></li> <li>Aggressively prune (images, caches, networks not in use): <code>docker system prune -a</code><ul> <li>Expect a confirmation prompt; this removed ~37 GB here by deleting unused images and build cache.</li> </ul> </li> <li>If needed, prune build cache separately: <code>docker builder prune</code></li> </ul>"},{"location":"docker/#port-5432-already-in-use-postgres-conflict","title":"Port 5432 already in use (Postgres conflict)","text":"<p>If <code>database_postgres</code> fails to start with a \"port already in use\" error, another PostgreSQL is likely running on your machine. This is common on developer machines where you have: - macOS: Postgres.app (a popular menu-bar Postgres), Homebrew PostgreSQL (<code>brew install postgresql</code>), or pgAdmin's bundled server - Linux: System PostgreSQL installed via <code>apt install postgresql</code> or similar - Windows: PostgreSQL installer, pgAdmin, or other database tools</p> <p>Solution: Set <code>PLANEXE_POSTGRES_PORT</code> to a different value: <pre><code>export PLANEXE_POSTGRES_PORT=5433\ndocker compose up\n</code></pre></p> <p>This only affects the HOST port (how you access Postgres from your machine). Inside Docker, containers always connect to each other on port 5432\u2014this is hardcoded and unaffected by <code>PLANEXE_POSTGRES_PORT</code>.</p> <p>To make this permanent, add to your <code>.env</code> file: <pre><code>PLANEXE_POSTGRES_PORT=5433\n</code></pre></p> <p>When connecting from your host machine (e.g., DBeaver, <code>psql</code>), use the port you set: <pre><code>psql -h localhost -p 5433 -U planexe -d planexe\n</code></pre></p>"},{"location":"docker/#environment-notes","title":"Environment notes","text":"<ul> <li>The worker exports logs to stdout when <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT=true</code> (set in <code>docker-compose.yml</code>).</li> <li>Shared volumes: <code>./run</code> is mounted into both services; <code>.env</code> and <code>llm_config.json</code> are mounted read-only. Ensure they exist on the host before starting.***</li> <li>Database: Postgres runs in <code>database_postgres</code> and listens on host <code>${PLANEXE_POSTGRES_PORT:-5432}</code> mapped to container <code>5432</code>; data is persisted in the named volume <code>database_postgres_data</code>.</li> <li>Multiuser UI: binds to container port <code>5000</code>, exposed on host <code>${PLANEXE_FRONTEND_MULTIUSER_PORT:-5001}</code>.</li> <li>MCP server downloads: set <code>PLANEXE_MCP_PUBLIC_BASE_URL</code> so clients receive a reachable <code>/download/...</code> URL (defaults to <code>http://localhost:8001</code> in compose).</li> </ul>"},{"location":"docker/#host-opener-open-output-dir","title":"Host opener (Open Output Dir)","text":"<p>Because Docker containers cannot launch host apps, the <code>Open Output Dir</code> button needs a host-side service.</p> <p>Set these environment variables before starting: - <code>PLANEXE_OPEN_DIR_SERVER_URL</code> so the container can reach the host opener:   - macOS/Windows (Docker Desktop): <code>http://host.docker.internal:5100</code>   - Linux: <code>http://172.17.0.1:5100</code> (or add <code>host.docker.internal</code> pointing to the bridge IP). - <code>PLANEXE_HOST_RUN_DIR</code>: optional; defaults to <code>PlanExe/run</code> on the host. Set an absolute path if you relocate the run directory.</p> <p>1) Start host opener before Docker (on the host): <pre><code>cd open_dir_server\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -r requirements.txt\npython app.py\n</code></pre> 2) Provide <code>PLANEXE_OPEN_DIR_SERVER_URL</code> via your shell env, <code>.env</code>, or docker compose environment for <code>frontend_single_user</code>.</p>"},{"location":"getting_started/","title":"Getting started with PlanExe","text":"<p>This guide shows new users how to launch the <code>frontend_single_user</code> UI with Docker using OpenRouter as the LLM provider. No local Python or pip setup is needed.</p>"},{"location":"getting_started/#1-prerequisites","title":"1. Prerequisites","text":"<p>Install Docker.</p> <p>Create an account on OpenRouter and top up around 5 USD in credits (paid models works, the free models are unreliable). It cost around 0.1 USD to generate a plan, when using PlanExe's default settings.</p>"},{"location":"getting_started/#2-clone-the-repo","title":"2. Clone the repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre>"},{"location":"getting_started/#3-configure-secrets","title":"3. Configure secrets","text":"<p>Copy <code>.env.docker-example</code> to <code>.env</code>.</p> <p>Add your OpenRouter key: <pre><code>OPENROUTER_API_KEY='sk-or-v1-your-key'\n</code></pre></p>"},{"location":"getting_started/#4-start-the-single-user-stack","title":"4. Start the single-user stack","text":"<pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> <p>Wait for http://localhost:7860 to become available.</p> <p>Stop with <code>Ctrl+C</code>.</p>"},{"location":"getting_started/#5-use-the-ui","title":"5. Use the UI","text":"<p>Open http://localhost:7860 in your browser. </p> <p>You can now submit your prompt.</p> <p>The generated plans are written to <code>run/&lt;timestamped-output-dir&gt;</code>.</p> <p></p>"},{"location":"getting_started/#troubleshooting-and-next-steps","title":"Troubleshooting and next steps","text":"<ul> <li>For Docker tips, see docker.md.</li> <li>For OpenRouter-specific notes, see openrouter.md.</li> <li>If the UI fails to load or plans don\u2019t start, check worker logs: <code>docker compose logs -f worker_plan</code>.</li> </ul>"},{"location":"getting_started/#community","title":"Community","text":"<p>Need help? Join the PlanExe Discord.</p>"},{"location":"install_developer/","title":"Installing PlanExe for developers","text":"<p>I assume that you are a python developer.</p> <p>You need several open terminals to do development on this project.</p>"},{"location":"install_developer/#step-1-clone-repo","title":"Step 1 - Clone repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\n</code></pre>"},{"location":"install_developer/#step-2-prepare-env-file","title":"Step 2 - Prepare <code>.env</code> file","text":"<p>Create a <code>.env</code> file from the <code>.env.developer-example</code> file.</p> <p>Update <code>OPENROUTER_API_KEY</code> with your open router api key.</p>"},{"location":"install_developer/#step-3-open_dir_server","title":"Step 3 - <code>open_dir_server</code>","text":"<p>In a new terminal:  Follow the <code>open_dir_server</code> instructions.</p>"},{"location":"install_developer/#step-4-worker_plan","title":"Step 4 - <code>worker_plan</code>","text":"<p>In a new terminal:  Follow the <code>worker_plan</code> instructions.</p>"},{"location":"install_developer/#step-5-frontend_single_user","title":"Step 5 - <code>frontend_single_user</code>","text":"<p>In a new terminal:  Follow the <code>frontend_single_user</code> instructions.</p>"},{"location":"install_developer/#step-6-database_postgres","title":"Step 6 - <code>database_postgres</code>","text":"<p>In a new terminal:  Follow the <code>database_postgres</code> instructions.</p>"},{"location":"install_developer/#step-7-worker_plan_database","title":"Step 7 - <code>worker_plan_database</code>","text":"<p>In a new terminal:  Follow the <code>worker_plan_database</code> instructions.</p>"},{"location":"install_developer/#step-8-frontend_multi_user","title":"Step 8 - <code>frontend_multi_user</code>","text":"<p>In a new terminal:  Follow the <code>frontend_multi_user</code> instructions.</p>"},{"location":"install_developer/#step-9-tests","title":"Step 9 - Tests","text":"<p>In a new terminal:  Run the tests to ensure that the project works correctly. <pre><code>PROMPT&gt; python test.py\nsnip lots of output snip\nRan 117 tests in 0.059s\n\nOK\n</code></pre></p>"},{"location":"install_developer/#now-planexe-have-been-installed","title":"Now PlanExe have been installed.","text":""},{"location":"lm_studio/","title":"Using PlanExe with LM Studio","text":"<p>LM Studio is an open source app for macOS/Windows/Linux for running LLMs on your own computer. It's great for troubleshooting.</p> <p>PlanExe processes more text than regular chat. You will need expensive hardware to run a LLM at a reasonable speed.</p>"},{"location":"lm_studio/#quickstart-docker","title":"Quickstart (Docker)","text":"<p>1) Install LM Studio on your host and download a small model inside LM Studio (e.g., <code>Qwen2.5-7B-Instruct-1M</code>, ~4.5 GB). 2) Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for LM Studio) and use the <code>lmstudio-...</code> entry in <code>llm_config.json</code>, setting <code>base_url</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Linux bridge IP. 3) Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</p>"},{"location":"lm_studio/#host-only-no-docker-for-advanced-users","title":"Host-only (no Docker) \u2014 for advanced users","text":"<ul> <li>Use the host entry (e.g., <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://127.0.0.1:1234</code>.</li> <li>Start your preferred PlanExe runner (e.g., a local Python environment) and make sure the LM Studio server is running before you submit jobs.</li> </ul>"},{"location":"lm_studio/#configuration","title":"Configuration","text":"<p>In the <code>llm_config.json</code> find a config that starts with <code>lmstudio-</code> such as <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>. Inside LM Studio, find the model with that exact id and download it. Here is the Qwen model on huggingface (~4.5 GB).</p> <p>Inside LM Studio, go to the <code>Developer</code> page (CMD+2 or Windows+2 or Ctrl+2). Start the server. The UI should show <code>Status: Running [x]</code> and <code>Reachable at: http://127.0.0.1:1234</code>.</p>"},{"location":"lm_studio/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with a ~7B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>Structured output matters: not all models return clean structured output. If you see malformed/JSON errors, try a nearby model or quantization.</li> </ul>"},{"location":"lm_studio/#run-lm-studio-locally-with-docker","title":"Run LM Studio locally with Docker","text":"<ul> <li>Containers cannot reach <code>127.0.0.1</code> on your host. Set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Docker bridge IP on Linux (often <code>http://172.17.0.1:1234</code>). Add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code> if that hostname is missing on Linux.</li> <li>Find your bridge IP on Linux:</li> </ul> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <ul> <li>If <code>docker0</code> is missing (alternate bridge names, Podman, etc.), inspect the default bridge gateway instead:</li> </ul> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <ul> <li>Example <code>llm_config.json</code> entry (add <code>base_url</code> when using Docker):</li> </ul> <pre><code>\"lmstudio-qwen2.5-7b-instruct-1m\": {\n    \"comment\": \"Runs via LM Studio on the host; PlanExe in Docker points to the host LM Studio server.\",\n    \"class\": \"LMStudio\",\n    \"arguments\": {\n        \"model_name\": \"qwen2.5-7b-instruct-1m\",\n        \"base_url\": \"http://host.docker.internal:1234/v1\",\n        \"temperature\": 0.2,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <ul> <li>After editing <code>llm_config.json</code>, rebuild or restart the worker/frontends: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config baked in).</li> </ul>"},{"location":"lm_studio/#troubleshooting","title":"Troubleshooting","text":"<p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p> <p>Where to look for logs: - Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container). - Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to LM Studio). - Structured-output failures: if you see JSON/parse errors or malformed outputs in <code>log.txt</code>, try a different model or quantization; not all models return structured output cleanly.</p>"},{"location":"lm_studio/#run-lm-studio-on-a-remote-computer","title":"Run LM Studio on a remote computer","text":"<p>Use a secure tunnel instead of exposing the server directly. Example from your local machine:</p> <pre><code>ssh -N -L 1234:localhost:1234 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:1234</code> while the tunnel is running.</p>"},{"location":"mcp/","title":"PlanExe MCP Tools - Experimental","text":"<p>MCP is work-in-progress, and I (Simon Strandgaard, the developer) may change it as I see fit. If there is a particular tool you want. Write to me on the PlanExe Discord, and I will see what I can do.</p> <p>This document lists the MCP tools exposed by PlanExe and example prompts for agents.</p>"},{"location":"mcp/#overview","title":"Overview","text":"<ul> <li>The primary MCP server runs in the cloud (see <code>mcp_cloud</code>).</li> <li>The local MCP proxy (<code>mcp_local</code>) forwards calls to the server and adds a local download helper.</li> <li>Tool responses return JSON in both <code>content.text</code> and <code>structuredContent</code>.</li> </ul>"},{"location":"mcp/#tool-catalog-mcp_cloud","title":"Tool Catalog, <code>mcp_cloud</code>","text":""},{"location":"mcp/#task_create","title":"task_create","text":"<p>Create a new plan task.</p> <p>Example prompt: <pre><code>Create a plan for: Weekly meetup for humans where participants are randomly paired every 5 minutes...\n</code></pre></p> <p>Example call: <pre><code>{\"idea\": \"Weekly meetup for humans where participants are randomly paired every 5 minutes...\"}\n</code></pre></p> <p>Optional argument: <pre><code>speed_vs_detail: \"ping\" | \"fast\" | \"all\"\n</code></pre></p>"},{"location":"mcp/#task_status","title":"task_status","text":"<p>Fetch status/progress and recent files for a task.</p> <p>Example prompt: <pre><code>Get status for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/#task_stop","title":"task_stop","text":"<p>Request an active task to stop.</p> <p>Example prompt: <pre><code>Stop task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/#task_file_info","title":"task_file_info","text":"<p>Return download metadata for report or zip artifacts.</p> <p>Example prompt: <pre><code>Get report info for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p> <p>Available artifacts: <pre><code>\"report\" | \"zip\"\n</code></pre></p>"},{"location":"mcp/#tool-catalog-mcp_local","title":"Tool Catalog, <code>mcp_local</code>","text":"<p>The local proxy exposes the same tools as the server, and adds:</p>"},{"location":"mcp/#task_download","title":"task_download","text":"<p>Download report or zip to a local path.</p> <p>Example prompt: <pre><code>Download the report for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mcp/#typical-flow","title":"Typical Flow","text":""},{"location":"mcp/#1-create-a-plan","title":"1) Create a plan","text":"<p>Prompt: <pre><code>Create a plan for this idea: Weekly meetup for humans where participants are randomly paired every 5 minutes...\n</code></pre></p>"},{"location":"mcp/#2-get-status","title":"2) Get status","text":"<p>Prompt: <pre><code>Get status for my latest task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\"}\n</code></pre></p>"},{"location":"mcp/#3-download-the-report","title":"3) Download the report","text":"<p>Prompt: <pre><code>Download the report for my task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mistral/","title":"Using PlanExe with Mistral","text":"<p>This is for advanced users that already have PlanExe working.</p> <p>If you are getting started with PlanExe, I (the developer of PlanExe) recommend following the openrouter.md instructions. Stick with the default settings. </p> <p>If you want to use Mistral, then OpenRouter has several mistral models. </p>"},{"location":"mistral/#docker-setup-for-mistral","title":"Docker setup for Mistral","text":"<p>Mistral support is not baked into the Docker image by default. You must add the Mistral LlamaIndex extension to the worker, rebuild the image, and supply your API key.</p> <p>1) Install Docker (with Docker Compose), then clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre> 2) Enable the Mistral client inside the worker image by editing <code>worker_plan/pyproject.toml</code>. Under <code>[project].dependencies</code>, add or uncomment these lines: <pre><code>\"llama-index-llms-mistralai==0.4.0\",\n\"mistralai==1.5.2\",\n</code></pre>    Without this step, the Docker image will not have the <code>MistralAI</code> class. 3) Copy <code>.env.docker-example</code> to <code>.env</code> and add your key: <pre><code>MISTRAL_API_KEY='INSERT-YOUR-SECRET-KEY-HERE'\n</code></pre> 4) Add (or keep) a Mistral entry in <code>llm_config.json</code> (example below). 5) Rebuild the images so the new dependencies are baked in: <pre><code>docker compose build --no-cache worker_plan frontend_single_user\n</code></pre> 6) Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> 7) Open http://localhost:7860, go to Settings, and pick your Mistral model (e.g., <code>mistral-paid-large</code>). If you later tweak only <code>llm_config.json</code>, just restart the containers (<code>docker compose restart worker_plan frontend_single_user</code>); rebuilds are only needed when dependencies change.</p>"},{"location":"mistral/#why-use-mistral","title":"Why use Mistral?","text":"<p>Mistral can have run your own fine tuned model in the cloud. If you have sensitive business data that you don't want to share with the world, then this is one way to do it.</p> <p>Create an account on the mistral.ai website and buy 10 EUR of credits.</p> <p>List of available models.</p> <p>Using the free models, and the API is rate limited to 1 request per second. PlanExe cannot deal with rate limiting and PlanExe does 70-100 requests, so it's likely going to yield errors.</p>"},{"location":"mistral/#create-api-key","title":"Create API key","text":"<ol> <li>Visit api-keys.</li> <li>Click <code>Create new key</code> and name the new key <code>PlanExe</code>.</li> <li>In the <code>.env</code> file in the root dir of the PlanExe repo, create a row named <code>MISTRAL_API_KEY</code>. Copy/paste the newly created api key into that row.</li> </ol> <p>The <code>.env</code> file should look something like the following, with your own key inserted. <pre><code>MISTRAL_API_KEY='AWkg3SxFTLWaPJClbASfv9h3VPItroof'\n</code></pre></p>"},{"location":"mistral/#edit-the-llm_configjson","title":"Edit the <code>llm_config.json</code>","text":"<p>The JSON should look something like this:</p> <pre><code>{\n    \"mistral-paid-large\": {\n        \"comment\": \"This is paid. Possible free to use for a limited time. Check the pricing before use.\",\n        \"class\": \"MistralAI\",\n        \"arguments\": {\n            \"model\": \"mistral-large-latest\",\n            \"api_key\": \"${MISTRAL_API_KEY}\",\n            \"temperature\": 1.0,\n            \"timeout\": 60.0,\n            \"max_tokens\": 8192,\n            \"max_retries\": 5\n        }\n    }\n}\n</code></pre>"},{"location":"mistral/#use-the-mistral-model","title":"Use the Mistral model","text":"<ol> <li>Restart PlanExe</li> <li>Go to the <code>Settings</code> tab</li> <li>Select the <code>mistral-paid-large</code> model.</li> </ol>"},{"location":"ollama/","title":"Using PlanExe with Ollama","text":"<p>Ollama is an open source app for macOS/Windows/Linux for running LLMs on your own computer (or on a remote computer).</p> <p>PlanExe processes more text than regular chat. You will need expensive hardware to run a LLM at a reasonable speed.</p>"},{"location":"ollama/#quickstart-docker","title":"Quickstart (Docker)","text":"<p>1) Install Ollama on your host and pull a small model: <code>ollama run llama3.1</code> (downloads ~4.9 GB and proves the host service works). 2) Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for Ollama) and pick the Docker entry in <code>llm_config.json</code> (snippet below) so <code>base_url</code> points to <code>http://host.docker.internal:11434</code> (Docker Desktop) or your Linux bridge IP. 3) Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</p>"},{"location":"ollama/#host-only-no-docker-for-advanced-users","title":"Host-only (no Docker) \u2014 for advanced users","text":"<ul> <li>Use the host entry (e.g., <code>\"ollama-llama3.1\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://localhost:11434</code>.</li> <li>Start your preferred PlanExe runner (e.g., a local Python environment) and ensure Ollama is already running on the host before you submit jobs.</li> </ul>"},{"location":"ollama/#configuration","title":"Configuration","text":"<p>In the <code>llm_config.json</code> find a config that starts with <code>ollama-</code> such as <code>\"ollama-llama3.1\"</code> (host) or <code>\"docker-ollama-llama3.1\"</code> (Docker). Use the <code>docker-</code> entry when PlanExe runs in Docker so requests reach the host.</p> <p>On the Ollama Search Models website. Find the corresponding model. Go to the info page for the model: ollama/library/llama3.1. The info page shows how to install the model on your computer, in this case <code>ollama run llama3.1</code>. To get started, go for a <code>8b</code> model that is <code>4.9GB</code>.</p>"},{"location":"ollama/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with an 8B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>If you need faster responses, move to a bigger GPU box or use a cloud model via OpenRouter instead of upsizing Ollama locally.</li> </ul>"},{"location":"ollama/#run-ollama-locally-with-docker","title":"Run Ollama locally with Docker","text":"<ul> <li>Make sure the container can reach Ollama on the host. On macOS/Windows (Docker Desktop) use the preconfigured entry in <code>llm_config.json</code> (snippet below) with <code>base_url</code> pointing to <code>http://host.docker.internal:11434</code>. On Linux, use your Docker bridge IP (often <code>http://172.17.0.1:11434</code>) and, if needed, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code>.</li> <li>Find your bridge IP on Linux:</li> </ul> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <ul> <li>If <code>docker0</code> is missing (alternate bridge names, Podman, etc.), inspect the default bridge gateway instead:</li> </ul> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <ul> <li>Example <code>llm_config.json</code> entry:</li> </ul> <pre><code>\"docker-ollama-llama3.1\": {\n    \"comment\": \"This runs on your own computer. It's free. Requires Ollama to be installed. PlanExe runs in a Docker container, and ollama is installed on the host the computer.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://host.docker.internal:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <ul> <li>Restart or rebuild the worker/frontends after updating <code>llm_config.json</code>: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config baked in).</li> </ul>"},{"location":"ollama/#troubleshooting","title":"Troubleshooting","text":"<p>Use the command line to compare Ollama's list of installed models with the configurations in your <code>llm_config.json</code> file. Run:</p> <pre><code>PROMPT&gt; ollama list\nNAME                                             ID              SIZE      MODIFIED       \nhf.co/unsloth/Llama-3.1-Tulu-3-8B-GGUF:Q4_K_M    08fe35cc5878    4.9 GB    19 minutes ago    \nphi4:latest                                      ac896e5b8b34    9.1 GB    6 weeks ago       \nqwen2.5-coder:latest                             2b0496514337    4.7 GB    2 months ago      \nllama3.1:latest                                  42182419e950    4.7 GB    5 months ago      \n</code></pre> <p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p> <p>Where to look for logs: - Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container). - Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to Ollama). - Structured-output failures: if you see JSON/parse errors or malformed outputs in <code>log.txt</code>, try a different Ollama model or quantization; not all models return structured output cleanly.</p>"},{"location":"ollama/#how-to-add-a-new-ollama-model-to-llm_configjson","title":"How to add a new Ollama model to <code>llm_config.json</code>","text":"<p>You can find models and installation instructions here: - Ollama \u2013 Overview of popular models, curated by the Ollama team. - Hugging Face \u2013 A vast collection of GGUF models.</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output. Not every model does this reliably; you may need to try a few nearby models (or quantizations) before finding one that cleanly returns the structured responses PlanExe expects.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Follow the instructions on Ollama or Hugging Face to install the model.</li> <li>Copy the model id from the <code>ollama list</code> command, such as <code>llama3.1:latest</code></li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"ollama/#run-ollama-on-a-remote-computer","title":"Run Ollama on a remote computer","text":"<p>In <code>llm_config.json</code>, insert <code>base_url</code> with the url to run on. Prefer a secure tunnel (example below) or a firewall-restricted host\u2014avoid exposing Ollama publicly.</p> <p>SSH tunnel example from your local machine:</p> <pre><code>ssh -N -L 11434:localhost:11434 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:11434</code> while the tunnel is running.</p> <pre><code>\"ollama-llama3.1\": {\n    \"comment\": \"This runs on on a remote computer. Requires Ollama to be installed.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://example.com:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre>"},{"location":"openrouter/","title":"Using PlanExe with OpenRouter","text":"<p>OpenRouter provides access to a large number of LLM models, that runs in the cloud.</p> <p>Unfortunately there is no <code>free</code> model that works reliable with PlanExe.</p> <p>In my experience, the <code>paid</code> models are the most reliable. Models like google/gemini-2.0-flash-001 and openai/gpt-4o-mini are cheap and faster than running models on my own computer and without risk of it overheating.</p> <p>I haven't been able to find a <code>free</code> model on OpenRouter that works well with PlanExe.</p>"},{"location":"openrouter/#quickstart-docker","title":"Quickstart (Docker)","text":"<p>1) Install Docker (with Docker Compose) \u2014 no local Python or pip is needed now. 2) Clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre> 3) Copy <code>.env.docker-example</code> to <code>.env</code>, then set your API key and pick a default OpenRouter profile so the worker uses the cloud model by default: <pre><code>OPENROUTER_API_KEY='sk-or-v1-...'\nDEFAULT_LLM='openrouter-paid-gemini-2.0-flash-001'   # or openrouter-paid-openai-gpt-4o-mini\n</code></pre>    The containers mount <code>.env</code> and <code>llm_config.json</code> automatically. 4) Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre>    - Wait for http://localhost:7860 to come up, submit a prompt, and watch progress with <code>docker compose logs -f worker_plan</code>.    - Outputs are written to <code>run/&lt;timestamped-output-dir&gt;</code> on the host (mounted from the containers). 5) Stop with <code>Ctrl+C</code> (or <code>docker compose down</code>). If you change <code>llm_config.json</code>, restart the containers so they reload it: <code>docker compose restart worker_plan frontend_single_user</code> (or <code>docker compose down &amp;&amp; docker compose up</code>). No rebuild is needed for config-only edits.</p>"},{"location":"openrouter/#configuration","title":"Configuration","text":"<p>Visit OpenRouter, create an account, purchase 5 USD in credits (plenty for making a several plans), and generate an API key.</p> <p>Copy <code>.env.docker-example</code> to a new file called <code>.env</code> (loaded by Docker at startup).</p> <p>Open the <code>.env</code> file in a text editor and insert your OpenRouter API key. Like this:</p> <pre><code>OPENROUTER_API_KEY='INSERT YOUR KEY HERE'\n</code></pre> <p>If you edit <code>llm_config.json</code> later, restart the worker/frontend containers to pick up the changes: <code>docker compose restart worker_plan frontend_single_user</code> (or stop/start). Rebuilds are only needed when dependencies change.</p>"},{"location":"openrouter/#troubleshooting","title":"Troubleshooting","text":"<p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>When running in Docker, also check the worker logs for 401/429 or connectivity errors:</p> <pre><code>docker compose logs -f worker_plan\n</code></pre> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p>"},{"location":"openrouter/#how-to-add-a-new-openrouter-model-to-llm_configjson","title":"How to add a new OpenRouter model to <code>llm_config.json</code>","text":"<p>The OpenRouter/rankings page shows an overview of the most popular models. New models are added frequently</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Copy the model id from the openrouter website.</li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"plan/","title":"Future plan for PlanExe","text":"<p>Using the \"5 Whys\" method:</p> <p>I want multiple-agents talking with each other, via zulip.</p> <p>Why?</p> <p>Can I \u201cexecute\u201d a plan from start to finish, with the agents doing all the labor. Zulip is open source. So I\u2019m not dependent on Discord/Teams/Slack.</p> <p>Why?</p> <p>When humans are doing the labor, they have to decompose the problem into tasks.  In my experience with PlanExe, AI can decompose sometimes better/worse than humans. Possible via MCP to interface with issue tracker. delegate parts of the plan to humans.</p> <p>Why?</p> <p>Companies spend lots of effort on planning and getting the right people to communicate with meetings, emails. Something I dislike about working in coding jobs. Wasting time and money on planning.</p> <p>Why?</p> <p>Cut cost and optimize speed.</p> <p>Why?</p> <p>To satisfy my own curiosity. I\u2019m curious to what kind of outcome it is. An AI organization/company, possible distributed network. Is it a global organism as seen in scifi movies that are controlled by AIs, that takes power away from politicians. My concerns are: will it be able to adapt to a changing world. Re-plan in real-time when a shipment is delayed, a machine breaks down, or an unexpected storm hits. quiet, compounding errors, security oversights, and cost blowouts.</p>"},{"location":"plan/#execute-the-plan","title":"Execute the plan","text":"<p>Currently it's up to humans to execute a plan. How can this be automated?</p> <p>Ideally take an entire plan and go with it.</p>"},{"location":"plan/#improve-plan","title":"Improve plan","text":"<p>Prompt optimizing with A/B testing: Make tiny tweaks to one system prompt at a time, and see how it compares to baseline. If most generated plans gets improved, then keep the new system prompt. Verify across multiple LLMs/reasoning models, that the new system prompt makes an improvement. Store the new system prompt in the repo. Find weaknesses that are common for the generated plans. Pick the earliest task in the pipeline that impact this weakness. Schedule this weakness for the next A/B test improvement iteration.</p> <p>Boost initial prompt: The <code>initial prompt</code> has the biggest impact on the generated plan, if it's bad then the final plan is bad. If it's well written, concise, there is a higher chance for a realistic/feasible plan. Currently I use AIs to write the initial prompt for me by first having a long conversation about the topic, and showing examples of other initial prompts that have worked well in the past. It may by a small tweak to the initial prompt and it yields a better plan. It may be an entire rewrite of the initial prompt. The user may have specified a vague prompt, or the user may not be domain expert, the prompt may be non-sense, or the prompt may be overly specific so PlanExe attends to the wrong things. Suggest changes to the initial prompt. This can be by picking a bigger budget, a different technology, a different set of levers, fixing typos.</p> <ul> <li>User specifies a budget of 0..100 USD. Which is unrealistic, when the plan is to hire a team, and work on it for months.</li> <li>User leaves out physical location(s). So PlanExe picks a random location in a different part of the world.</li> </ul> <p>Grid search: Currently PlanExe only generates a plan for 1 permutation of levers. A plan may have 10 levers with 3-5 settings. Here it could be interesting to create  100 full plans, each with a different combination of levers. Compare the generated plans against each other  and pick the most 3 promising plans.</p> <p>Multiple refinements: Currently PlanExe generates the first iteration of the plan. Usually issues arises when making the first iteration, that have to be incorporated into the timeline. In the future I want to do multiple iterations, until the plan is of a reasonable quality.</p> <p>Validate the plan with deep research: Currently there is no validation. It's up to humans to be skeptic about the plan, does this make sense, check everything. There may be issues with: assumptions, numbers, flaws.</p> <p>Money: Currently the LLMs make up numbers. Alternate between these: Tweak the plan. Tweak the budget. Repeat. Obtain latest market data. Obtain info about what resources the user has available. Populate a Cost-Breakdown-Structure.</p> <p>Gantt in parallel: Currently the Gantt is waterfall. For a team with several people it's possible to do tasks in parallel. Obtain info about what resources the user has available, and if they are willing to do tasks in parallel.</p>"},{"location":"plan/#secondary-issues","title":"Secondary issues","text":""},{"location":"plan/#luigi-can-run-tasks-in-parallel","title":"Luigi can run tasks in parallel","text":"<p>I'm not making use of it. </p> <p>Until 2026-jan-01 I had this limitation: The PythonAnywhere doesn't like long running child processes/threads, anything longer than 5 minutes gets killed. There are always-on workers, but these must not spawn long running processes/threads. I'm considering finding another provider. Starting from 2026-jan-01 I\u2019m using Docker and no longer using pythonanywhere, I can start looking into running parallel tasks within Luigi.</p>"},{"location":"plan/#mcp-on-railway","title":"MCP on Railway","text":"<p>Doing inference in the cloud cost money. If users are to use MCP in the cloud, they will have to pay for it.</p> <ul> <li>Scenario A: Users can buy credit via <code>PLANEXE_MCP_API_KEY</code>.</li> <li>Scenario B: Users can BYOK (Bring your own key).</li> </ul> <p>Both scenarios will need user management with login.</p>"},{"location":"plan/#tertiary-issues","title":"Tertiary issues","text":""},{"location":"plan/#capture-reasoning-response","title":"Capture reasoning response","text":"<p>Currently I only capture the final response, without any reasoning. I want to capture the reasoning, since it may be helpful for troubleshooting. Or for other AIs to assess the reasoning steps leading up to the response.</p>"},{"location":"plan/#token-counting","title":"Token counting","text":"<p>So that I can see how much does it cost to generate a plan. Reasoning models. How many tokens are spent on reasoning vs generating the final response.</p>"},{"location":"plan/#debugging","title":"Debugging","text":"<p>Get step-by-step debugging working again. Now that I have switched to Docker, I have multiple python projects in the same repo, that use different incompatible packages.</p>"},{"location":"plan/#github-ci-that-runs-tests","title":"GitHub CI that runs tests","text":"<p>The hard thing is getting the venv's working.</p>"},{"location":"plan/#table-of-content","title":"Table of content","text":"<p>Currently the generated report has expandable/collapsible sections. There is an overwhelming amount of content inside each sections. I'm considering having a table of content in the left sidebar, similar to this: https://docs.railway.com/guides/dockerfiles It uses Docusaurus which uses React. I'm no fan of React. I'm considering using mkdocs instead.</p>"},{"location":"plan/#eliminate-redundant-user-prompts-in-the-log-file","title":"Eliminate redundant user prompts in the log file","text":"<p>Get rid of some of the many user prompt logging statements, so the log.txt is less noisy. These user prompts are saved to the <code>track_activity.jsonl</code> file already. So having them in the log.txt is redundant.</p>"},{"location":"plan/#ssl-when-connecting-with-the-database","title":"SSL when connecting with the database","text":"<p>I can't afford the pro plan to have a dedicated Postgres server. Currently when connecting to Railway, it's via a TCP Proxy and it's unencrypted. Either upgrade to pro, or use SSL certificates within the \"database_postgres\" Dockerfile.</p>"},{"location":"planexe_mcp_interface/","title":"Planexe mcp interface","text":"<p>PlanExe MCP Interface Specification (v1.0)</p> <ol> <li>Purpose</li> </ol> <p>1.1 What is PlanExe</p> <p>PlanExe is a service that generates rough-draft project plans from a natural-language prompt. You describe a large goal (e.g. open a clinic, launch a product, build a moon base)\u2014the kind of project that in reality takes months or years. PlanExe produces a structured draft: steps, documents, and deliverables. The plan is not executable in its current form; it is a draft to refine and act on. Creating a plan is a long-running task (100+ LLM inference calls): create a task with a prompt, poll status, then download the HTML report and zip when done.</p> <p>1.2 What kind of plan does it create</p> <p>The plan is a project plan: a DAG of steps (Luigi tasks) that produce artifacts including a Gantt chart, risk analysis, and other project management deliverables. The main output is a large HTML file (approx 700KB) containing many sections. There is also a zip file containing all intermediary files (md, json, csv). Plan quality depends on prompt quality; use the prompt_examples tool to see the baseline before calling task_create.</p> <p>1.2.1 Agent-facing summary (for server instructions / tool descriptions)</p> <p>Implementors should expose the following to agents so they understand what PlanExe does:</p> <ul> <li>What: PlanExe turns a plain-English goal into a structured strategic-plan draft (executive summary, Gantt, risk register, governance, etc.) in ~15\u201320 min. The plan is a draft to refine, not an executable or final document.</li> <li>Flow: Call prompt_examples first, then task_create; poll task_status at reasonable intervals (e.g. every 5 min); use task_download or task_file_info when complete.</li> <li>Output: Large HTML report (~700KB) and optional zip of intermediate files (md, json, csv).</li> </ul> <p>1.3 Scope of this document</p> <p>This document specifies a Model Context Protocol (MCP) interface for PlanExe that enables AI agents and client UIs to:     1.  Create and run long-running plan generation workflows.     2.  Receive real-time progress updates (task status, log output).     3.  List, read, and edit artifacts produced in an output directory.     4.  Stop and resume execution with Luigi-aware incremental recomputation.</p> <p>The interface is designed to support:     \u2022   interactive \u201cbuild systems\u201d behavior (like make / bazel),     \u2022   resumable DAG execution (Luigi),     \u2022   deterministic artifact management.</p> <p>\u2e3b</p> <ol> <li>Goals</li> </ol> <p>2.1 Functional goals     \u2022   Task-based orchestration: each run is associated with a task ID.     \u2022   Long-running execution: starts asynchronously; clients poll or subscribe to events.     \u2022   Artifact-first workflow: outputs are exposed as file-like artifacts.     \u2022   Stop / Resume with minimal recompute:     \u2022   on resume, only invalidated downstream tasks regenerate.     \u2022   Progress reporting:     \u2022   progress_percentage     \u2022   Editable artifacts:     \u2022   user edits a generated file     \u2022   pipeline continues from that point, producing dependent outputs</p> <p>2.2 Non-functional goals     \u2022   Idempotency: repeated tool calls should not corrupt state.     \u2022   Observability: logs, state transitions, and artifacts must be inspectable.     \u2022   Concurrency safety: prevent conflicting writes and illegal resume patterns.     \u2022   Extensibility: future versions can add task graph browsing, caching backends, exports.</p> <p>\u2e3b</p> <ol> <li>Non-goals     \u2022   Defining PlanExe\u2019s internal plan schema, content format, or prompt strategy.     \u2022   Providing remote code execution inside artifacts.     \u2022   Implementing a full Luigi UI clone in MCP v1 (optional later).     \u2022   Guaranteeing ETA estimates (allowed but must be optional / best-effort).</li> </ol> <p>3.1 MCP tools vs MCP tasks (\u201cRun as task\u201d)     The MCP specification defines two different mechanisms:     \u2022   MCP tools (e.g. task_create, task_status, task_stop): the server exposes named tools; the client calls them and receives a response. PlanExe\u2019s interface is tool-based: the agent calls task_create \u2192 receives task_id \u2192 polls task_status \u2192 uses task_download. This document specifies those tools.     \u2022   MCP tasks protocol (\u201cRun as task\u201d in some UIs): a separate mechanism where the client can run a tool \u201cas a task\u201d using RPC methods such as tasks/run, tasks/get, tasks/result, tasks/cancel, tasks/list, so the tool runs in the background and the client polls for results.     PlanExe does not use or advertise the MCP tasks protocol. Implementors and clients should use the tools only. Do not enable \u201cRun as task\u201d for PlanExe; many clients (e.g. Cursor) and the Python MCP SDK do not support the tasks protocol properly. The intended flow is: call task_create, poll task_status, then call task_download when complete.</p> <p>\u2e3b</p> <ol> <li>System Model</li> </ol> <p>4.1 Core entities</p> <p>Task A long-lived container for a PlanExe project run.</p> <p>Key properties     \u2022   task_id: stable unique identifier (UUID, matches TaskItem.id)     \u2022   output_dir: artifact root namespace for task     \u2022   config: immutable run configuration (models, runtime limits, Luigi params)     \u2022   created_at, updated_at</p> <p>Run A single execution attempt inside a task (e.g., after a resume).</p> <p>Key properties     \u2022   state: running | stopped | completed | failed     \u2022   progress_percentage: computed progress percentage (float)     \u2022   started_at, ended_at</p> <p>Artifact A file-like output managed by PlanExe.</p> <p>Key properties     \u2022   path: path relative to task output root     \u2022   size, updated_at     \u2022   content_type: text/markdown, text/html, application/json, etc.     \u2022   sha256: content hash for optimistic locking and invalidation</p> <p>Event A typed message emitted during execution for UI/agent consumption.</p> <p>Key properties     \u2022   cursor: ordering token     \u2022   ts: timestamp     \u2022   type: event type     \u2022   data: event payload</p> <p>\u2e3b</p> <ol> <li>State Machine</li> </ol> <p>5.1 Task states</p> <p>Tasks may exist independent of active runs.     \u2022   created: task initialized, no run started     \u2022   active: at least one run exists, may be running or stopped     \u2022   archived: optional; immutable, no new runs allowed</p> <p>5.2 Run states     \u2022   running     \u2022   stopping (optional transitional state)     \u2022   stopped (user stopped, resumable)     \u2022   completed     \u2022   failed (resumable depending on failure type)</p> <p>5.3 Allowed transitions     \u2022   running \u2192 stopped via task_stop     \u2022   running \u2192 completed via normal success     \u2022   running \u2192 failed via error</p> <p>Invalid     \u2022   completed \u2192 running (new run must be triggered by creating a new task)     \u2022   running \u2192 running (no concurrent runs in v1)</p> <p>\u2e3b</p> <ol> <li>MCP Tools (v1 Required)</li> </ol> <p>All tool names below are normative.</p> <p>6.1 prompt_examples</p> <p>Returns example prompts that define the baseline for a good prompt. Call this tool before task_create and refine your prompt until it matches that quality. If you create a task with a weaker prompt, the resulting plan will be lower quality than it could be.</p> <p>Request: no parameters (empty object).</p> <p>Response: <code>{ \"samples\": [ \"prompt text 1\", \"prompt text 2\", ... ], \"message\": \"...\" }</code>.</p> <p>\u2e3b</p> <p>6.2 task_create</p> <p>Start creating a new plan. speed_vs_detail modes: 'all' runs the full pipeline with all details (slower, higher token usage/cost). 'fast' runs the full pipeline with minimal work per step (faster, fewer details), useful to verify the pipeline is working. 'ping' runs the pipeline entrypoint and makes a single LLM call to verify the worker_plan_database is processing tasks and can reach the LLM.</p> <p>Request</p> <p>Schema</p> <p>{   \"type\": \"object\",   \"properties\": {     \"prompt\": { \"type\": \"string\" },     \"speed_vs_detail\": {       \"type\": \"string\",       \"enum\": [\"ping\", \"fast\", \"all\"],       \"default\": \"ping\"     }   },   \"required\": [\"prompt\"] }</p> <p>Example</p> <p>{   \"prompt\": \"string\",   \"speed_vs_detail\": \"ping\" }</p> <p>Prompt quality</p> <p>The <code>prompt</code> parameter should be a detailed description of what the plan should cover. Good prompts are typically 300\u2013800 words and include:     \u2022   Clear context: background, constraints, and goals     \u2022   Specific requirements: budget, timeline, location, or technical constraints     \u2022   Success criteria: what \"done\" looks like     \u2022   Banned words or approaches (if any)</p> <p>Short one-liners (e.g., \"Construct a bridge\") tend to produce poor output because they lack context for the planning pipeline. Important details are location, budget, time frame.</p> <p>Clients can call the MCP tool prompt_examples to retrieve example prompts. Use these as examples for task_create; they can also call task_create with any prompt\u2014short prompts produce less detailed plans.</p> <p>For the full catalog file:     \u2022   <code>worker_plan/worker_plan_api/prompt/data/simple_plan_prompts.jsonl</code> \u2014 JSONL with <code>id</code>, <code>prompt</code>, optional <code>tags</code>, and optional <code>mcp_example</code> (true = curated for MCP).</p> <p>Response</p> <p>{   \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",   \"created_at\": \"2026-01-14T12:34:56Z\" }</p> <p>Behavior     \u2022   Must be idempotent only if client supplies an optional client_request_id (optional extension).     \u2022   Task config is immutable after creation in v1.</p> <p>\u2e3b</p> <p>6.3 task_status</p> <p>Returns run status and progress. Used for progress bars and UI states. Polling interval: call at reasonable intervals only (e.g. every 5 minutes); plan generation takes 15\u201320+ minutes and frequent polling is unnecessary.</p> <p>Request</p> <p>{   \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }</p> <p>Response</p> <p>{   \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",   \"state\": \"running\",   \"progress_percentage\": 62.0,   \"timing\": {     \"started_at\": \"2026-01-14T12:35:10Z\",     \"elapsed_sec\": 512   },   \"files\": [     {       \"path\": \"plan.md\",       \"updated_at\": \"2026-01-14T12:43:11Z\"     }   ] }</p> <p>Notes     \u2022   progress_percentage must be a float within [0,100].</p> <p>\u2e3b</p> <p>6.4 task_stop</p> <p>Stops the active run.</p> <p>Request</p> <p>{   \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }</p> <p>Response</p> <p>{   \"state\": \"stopped\" }</p> <p>Required semantics     \u2022   Must stop workers cleanly where possible.     \u2022   Must persist enough Luigi state to resume incrementally.</p> <p>\u2e3b</p> <p>6.5 Download flow (task_download vs task_file_info)</p> <p>If your client exposes task_download (e.g. mcp_local): use it to save the report or zip locally; it calls task_file_info under the hood, then fetches and writes to the local save path (e.g. PLANEXE_PATH).</p> <p>If you only have task_file_info (e.g. direct connection to mcp_cloud): call it with task_id and artifact (\"report\" or \"zip\"); use the returned download_url to fetch the file (e.g. GET with API key if configured).</p> <p>\u2e3b</p> <ol> <li>Targets</li> </ol> <p>8.1 Standard targets</p> <p>The following targets MUST be supported:     \u2022   build_plan     \u2022   validate_plan     \u2022   build_plan_and_validate</p> <p>Targets map to Luigi \u201cfinal tasks\u201d.</p> <ol> <li>Concurrency &amp; Locking</li> </ol> <p>10.1 Single active run per task</p> <p>In v1, tasks MUST enforce:     \u2022   at most one run in running state.</p> <p>\u2e3b</p> <ol> <li>Error Model</li> </ol> <p>Errors MUST return:     \u2022   code: stable machine-readable     \u2022   message: human-readable     \u2022   details: optional</p> <p>Example:</p> <p>{   \"error\": {     \"code\": \"RUN_ALREADY_ACTIVE\",     \"message\": \"A run is currently active for this task.\",     \"details\": { \"run_id\": \"run_0001\" }   } }</p> <p>11.1 Required error codes     \u2022   TASK_NOT_FOUND     \u2022   RUN_NOT_FOUND     \u2022   RUN_ALREADY_ACTIVE     \u2022   RUN_NOT_ACTIVE     \u2022   INVALID_TARGET     \u2022   INVALID_ARTIFACT_URI     \u2022   CONFLICT     \u2022   PERMISSION_DENIED     \u2022   RUNNING_READONLY     \u2022   INTERNAL_ERROR</p> <p>\u2e3b</p> <ol> <li>Security &amp; Isolation</li> </ol> <p>12.1 Sandbox constraints     \u2022   All artifacts must live under task-scoped storage.     \u2022   Artifact URIs must not permit path traversal.</p> <p>12.2 Access control</p> <p>At minimum:     \u2022   task must be scoped to a user identity (metadata.user_id)     \u2022   callers without permission must receive PERMISSION_DENIED</p> <p>12.3 Sensitive data handling     \u2022   logs may include model prompts/responses \u2192 treat logs as sensitive artifacts     \u2022   allow a config option to redact prompt content in event streaming</p> <p>\u2e3b</p> <ol> <li>Performance Requirements</li> </ol> <p>13.1 Responsiveness     \u2022   task_status must return within &lt; 250ms under normal load.</p> <p>13.2 Large artifacts     \u2022   server SHOULD impose max read size per call (e.g., 2\u201310MB)</p> <p>\u2e3b</p> <ol> <li>Observability Requirements</li> </ol> <p>The server MUST persist:     \u2022   run lifecycle events     \u2022   stop reasons     \u2022   failure tracebacks as artifacts (e.g., run_error.json)     \u2022   luigi execution logs (run.log)</p> <p>\u2e3b</p> <ol> <li>Reference UI Integration Contract</li> </ol> <p>To match your UI behavior:</p> <p>Progress bars</p> <p>Use:     \u2022   task_status.progress_percentage     \u2022   or progress_updated events</p> <p>\u2e3b</p> <ol> <li>Compatibility &amp; Versioning</li> </ol> <p>16.1 Versioning strategy     \u2022   MCP server exposes: planexe.version = \"1.0\"     \u2022   breaking changes require major bump</p> <p>16.2 Forward compatibility</p> <p>Clients must ignore unknown fields and unknown event types.</p> <p>\u2e3b</p> <ol> <li>Testing Strategy</li> </ol> <p>17.1 Contract tests (required)     \u2022   Start/stop/resume loops     \u2022   Invalid transition errors     \u2022   Event cursor monotonicity</p> <p>17.2 Determinism tests (recommended)     \u2022   Given same inputs + same edits, ensure same downstream artifacts unless models are stochastic     \u2022   If models are stochastic, test pipeline correctness, not identical bytes</p> <p>17.3 Load tests (recommended)     \u2022   multiple tasks concurrently, one run each     \u2022   event streaming stability under heavy log output</p> <p>\u2e3b</p> <ol> <li>Future Extensions (MCP Resources)</li> </ol> <p>PlanExe is artifact-first, and MCP already has a native concept for that: resources. Today artifacts are exposed via download_url or via proxy download + saved_path. Future versions SHOULD expose artifacts as MCP resources so clients can fetch them via standard resource reads (and treat PlanExe as a first-class MCP server rather than a thin API wrapper).</p> <p>Proposed resource identifiers     \u2022   planexe://task//report     \u2022   planexe://task//zip <p>Recommended resource metadata     \u2022   mime type (content_type)     \u2022   size (bytes)     \u2022   sha256 (content hash)     \u2022   generated_at (UTC timestamp)</p> <p>Notes     \u2022   Resources can be backed by existing HTTP endpoints internally; the MCP         resource read returns the bytes + metadata.     \u2022   This enables richer MCP client UX (preview, caching, validation) without         custom tool calls.</p> <p>\u2e3b</p> <ol> <li>Future Tools (High-Leverage, Low-Complexity)</li> </ol> <p>The following tools remove common UX friction without expanding the core model.</p> <p>19.1 task_list (or task_recent) Return a short list of recent tasks so agents can recover if they lost a task_id.</p> <p>Notes     \u2022   Default limit: 5\u201310 tasks.     \u2022   Include task_id, created_at, state, and prompt summary.</p> <p>19.2 task_wait Blocking helper that polls internally until the task completes or times out. Returns the final task_status payload plus suggested next steps.</p> <p>Notes     \u2022   Inputs: task_id, timeout_sec (optional), poll_interval_sec (optional).     \u2022   Outputs: same as task_status + next_steps (string or list).</p> <p>19.3 task_get_latest Simplest recovery: return the most recently created task for the caller.</p> <p>Notes     \u2022   Useful for single-user / single-session flows.     \u2022   Should be scoped to the caller/user_id when available.</p> <p>19.4 task_logs_tail (optional) Return the tail of recent log lines for troubleshooting failures.</p> <p>Notes     \u2022   Inputs: task_id, max_lines (optional), since_cursor (optional).     \u2022   Useful when task_status shows failed but no context.</p> <p>\u2e3b</p> <p>Appendix A \u2014 Example End-to-End Flow</p> <p>Create task</p> <p>task_create({ \"prompt\": \"...\" })</p> <p>Start run</p> <p>task_status({ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" })</p> <p>Stop</p> <p>task_stop({ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" })</p> <p>\u2e3b</p> <p>Appendix B \u2014 Optional v1.1 Extensions</p> <p>If you want richer Luigi integration later:     \u2022   planexe.task.graph (nodes + edges + states)     \u2022   planexe.task.invalidate (rerun subtree)     \u2022   planexe.export.bundle (zip all artifacts)     \u2022   planexe.validate.only (audit without regeneration)     \u2022   planexe.task.archive (freeze task)</p>"},{"location":"railway/","title":"PlanExe on Railway - Experimental","text":"<p>As of 2026-Jan-04, I'm experimenting with Railway. Currently the <code>frontend_multi_user</code> UI is an ugly MVP. I recommend going with the <code>frontend_single_user</code>, that doesn't use database.</p> <p>In this project, the files named <code>railway.md</code> or <code>railway.toml</code>, are related to how things are configured in my Railway setup.</p>"},{"location":"railway/#project-settings","title":"Project Settings","text":""},{"location":"railway/#environments","title":"Environments","text":"<p>Create these environments: - <code>production</code> - <code>staging</code></p>"},{"location":"railway/#shared-variables-production","title":"Shared variables - production","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than staging\n</code></pre>"},{"location":"railway/#shared-variables-staging","title":"Shared variables - staging","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than production\n</code></pre>"},{"location":"railway/#using-shared-variables-in-services","title":"Using Shared Variables in Services","text":"<p>Each service that connects to the database must reference the shared password variable in its own environment variables.</p> <p>In Railway, go to each service \u2192 Variables and add:</p> <pre><code>PLANEXE_POSTGRES_PASSWORD=\"${{shared.PLANEXE_POSTGRES_PASSWORD}}\"\n</code></pre> <p>Services that need this variable: - <code>database_postgres</code> - <code>frontend_multi_user</code> - <code>worker_plan_database</code></p> <p>This ensures all services use the same password, and you only need to update it in one place (the shared variables) when rotating credentials.</p>"},{"location":"recent_changes/","title":"Recent Changes in PlanExe","text":""},{"location":"recent_changes/#2025-dec-31","title":"2025-dec-31","text":"<p>PlanExe is now using Docker.</p> <p>So you no longer have to be python developer to install it on your own computer.</p> <p>Over the last month I have migrated PlanExe to Docker.</p> <p>So that I can deploy PlanExe on Railway and similar web providers.</p> <p>Previously I have been using PythonAnywhere, and I was stuck in a dependency hell, where I couldn't add packages without breaking other packages.</p> <p>Now with docker, I don't have these incompatibility issues. However docker have its own issues.</p> <p>The last version BEFORE the transition to docker is available here https://github.com/PlanExeOrg/PlanExe/releases/tag/2025-dec-31</p> <p>The main branch will be docker from now on. https://github.com/PlanExeOrg/PlanExe/tree/main</p>"},{"location":"troubleshooting_stuck_pipeline/","title":"Troubleshooting a stuck pipeline","text":"<p>The gradio app (<code>app_text2plan.py</code>) starts the <code>run_plan_pipeline</code> process via a <code>Popen</code> call. </p> <ul> <li>Environment, if the gradio app runs in a slightly different environment than when running via commandline <code>python -m worker_plan_internal.plan.run_plan_pipeline</code>, then the child process may behave differently. I have verified that the parent process and child process runs with the same environment variables.</li> <li>Buffering, if the parent process isn't reading stdout/stderr fast enough, the child process may freeze. I have reworked the <code>Popen</code> code so the stdout/stderr goes to <code>/dev/null</code>.</li> <li>Other issues, if the pipeline still hangs, let me know, it may be some issue I'm not aware of.</li> </ul>"},{"location":"troubleshooting_stuck_pipeline/#manually-resuming-a-stuck-pipeline","title":"Manually resuming a stuck pipeline","text":"<p>In the UI copy/paste the run_id that is stuck, eg: <code>20250209_030626</code></p> <p>Insert it on commandline, and run the pipeline, like this:</p> <pre><code>PROMPT&gt; RUN_ID=20250209_030626 python -m worker_plan_internal.plan.run_plan_pipeline\n</code></pre>"},{"location":"troubleshooting_stuck_pipeline/#why-does-the-pipeline-get-stuck","title":"Why does the pipeline get stuck?","text":"<p>The <code>log.txt</code> contains the output from the logger with <code>DEBUG</code> level, the most detailed. Alas the <code>log.txt</code> have little info about what exactly went wrong.  The exceptions rarely have useful info.</p> <ul> <li>Censorship, if it's a sensitive topic, then the LLM may refuse to answer.</li> <li>Timeout, that happens often when using AI providers in the cloud.</li> <li>Invalid json, responds from the server that doesn't adhere to the json schema. Too high a temperature setting may cause the LLM to be too creative and diverge from the json schema. Try use a lower temperature.</li> <li>Too long answer, if the respond from the server gets too long so it gets truncated, so it's invalid json.</li> <li>Other, there may be other reasons that I'm not aware of, please let me know if you encounter such a scenario.</li> </ul>"}]}