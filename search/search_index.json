{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome!","text":"<p>PlanExe turns a single plain-English goal into a ~40-page strategic plan in ~15 minutes using local or cloud LLMs. It\u2019s an accelerator for first drafts \u2014 not a replacement for human refinement. PlanExe removes most of the labor for the planning scaffold; the final 10\u201330% that makes a plan credible and defensible remains human work.</p> <p>Try it first, then decide if you want to run it locally.</p> <ul> <li>Open a full sample report (HTML): Minecraft Escape sample report</li> <li>Try PlanExe in your browser (no installation): planexe.org</li> <li>See more examples: planexe.org/examples</li> </ul>"},{"location":"#start-here-pick-your-path","title":"Start here (pick your path)","text":"<p>Use the short decision guide: Start here</p>"},{"location":"#core-guides","title":"Core guides","text":"<ul> <li>Prompt writing guide</li> <li>Plan output anatomy</li> <li>Costs and models</li> </ul>"},{"location":"#what-you-get","title":"What you get","text":"<p>PlanExe generates a single HTML report (a self-contained artifact you can open in a browser). See the sample report here: Minecraft Escape sample report</p>"},{"location":"#2-minute-tour","title":"2-minute tour","text":"<p>Open the sample report and do this:</p> <ol> <li>Read Executive Summary to see the top-level deliverables, budget, risks, and next steps.</li> <li>Jump to Gantt Interactive to see how the goal gets broken down into many concrete tasks.</li> <li>Open Premortem to see what could go wrong and what to do about it.</li> </ol>"},{"location":"#get-help","title":"Get help","text":"<p>If you run into issues, join the PlanExe Discord \u2014 the community and maintainers are there to help. When you ask, include what you tried, your setup (OS, Docker vs local, model/LLM), and any error output so others can help you quickly.</p> <p>Join the PlanExe Discord \u2192</p>"},{"location":"#links","title":"Links","text":"<ul> <li>Website: planexe.org</li> <li>GitHub: PlanExeOrg/PlanExe</li> <li>Discord: planexe.org/discord</li> </ul>"},{"location":"AGENTS/","title":"How PlanExe-docs Builds and Publishes to docs.planexe.org","text":"<p>This document describes how the PlanExe-docs repository takes content from this directory (<code>PlanExe/docs/</code>) and publishes it to https://docs.planexe.org.</p>"},{"location":"AGENTS/#overview","title":"Overview","text":"<ul> <li>Content source: This directory (<code>PlanExe/docs/</code>). All Markdown files, images, and assets here become the published documentation.</li> <li>Build &amp; deploy: The PlanExe-docs repo. It holds MkDocs config, GitHub Actions workflow, and build scripts.</li> <li>Output: Static site served via GitHub Pages at docs.planexe.org.</li> </ul>"},{"location":"AGENTS/#pipeline-ci","title":"Pipeline (CI)","text":"<ol> <li>Trigger    The Deploy Documentation workflow runs when:</li> <li>There is a push to <code>main</code> on PlanExe-docs, or</li> <li>It is started manually (<code>workflow_dispatch</code>), or</li> <li>A <code>repository_dispatch</code> event <code>docs-updated</code> is sent (e.g. when PlanExe is updated and you want to redeploy docs).</li> </ol> <p>Pushing only to PlanExe does not by itself update docs.planexe.org. This repo has a workflow (<code>.github/workflows/docs-update.yml</code>) that runs on push to <code>main</code> when <code>docs/</code> changes and sends <code>repository_dispatch</code> to PlanExe-docs. For that to work you must add a secret in PlanExe (see below). Otherwise, after editing <code>PlanExe/docs/</code>, either run the Deploy workflow manually in PlanExe-docs, or push to PlanExe-docs <code>main</code> (e.g. after syncing content) to deploy.</p> <ol> <li>Checkout </li> <li>PlanExe-docs repo (workflow, <code>mkdocs.yml</code>, <code>requirements.txt</code>, etc.).  </li> <li> <p>PlanExe repo into <code>planexe-source/</code> (so this <code>docs/</code> directory is available).</p> </li> <li> <p>Build </p> </li> <li><code>mkdir -p docs</code> in the PlanExe-docs workspace.  </li> <li><code>cp -r planexe-source/docs/* docs/</code> \u2014 all content from this <code>PlanExe/docs/</code> directory is copied into PlanExe-docs\u2019 <code>docs/</code> folder.  </li> <li> <p><code>mkdocs build --site-dir site</code> \u2014 MkDocs (Material theme, config from <code>mkdocs.yml</code>) builds the site into <code>site/</code>.</p> </li> <li> <p>Deploy </p> </li> <li>The peaceiris/actions-gh-pages action publishes the <code>site/</code> directory to the gh-pages branch of PlanExe-docs.  </li> <li>Custom domain docs.planexe.org is set via <code>cname: docs.planexe.org</code> in the workflow.  </li> <li>GitHub Pages serves the site from that branch, so updates appear at https://docs.planexe.org.</li> </ol>"},{"location":"AGENTS/#key-files","title":"Key files","text":"What Where Doc content (you edit here) <code>PlanExe/docs/</code> (this directory) MkDocs config, theme, plugins PlanExe-docs <code>mkdocs.yml</code> Deploy workflow PlanExe-docs <code>.github/workflows/deploy.yml</code> Build dependencies PlanExe-docs <code>requirements.txt</code> Frontpage <code>PlanExe/docs/index.md</code> (used as site index)"},{"location":"AGENTS/#linking-between-documentation-pages","title":"Linking between documentation pages","text":"<p>When adding or editing links from one doc file to another in <code>PlanExe/docs/</code>, use paths that MkDocs (used by PlanExe-docs <code>build.py</code>) can resolve. Otherwise the build will report \"unrecognized relative link\" and leave the URL as-is on the published site.</p> <p>Do:</p> <ul> <li>Use the <code>.md</code> extension in relative links to other docs in this directory.</li> <li>Same directory: <code>[MCP](mcp/mcp_details.md)</code>, <code>[Getting started](getting_started.md)</code>.</li> <li>Subdirectory: <code>[Extra](guides/extra.md)</code> (if you have <code>docs/guides/extra.md</code>).</li> </ul> <p>Do not:</p> <ul> <li>Use trailing slashes for doc-to-doc links: <code>[MCP](mcp/)</code> is not resolved by MkDocs and will trigger a build warning.</li> </ul> <p>Examples (in any file under <code>PlanExe/docs/</code>):</p> <pre><code>[PlanExe MCP interface](mcp/planexe_mcp_interface.md)\n[Docker](docker.md)\n[OpenRouter](ai_providers/openrouter.md)\n</code></pre> <p>External links (e.g. <code>https://planexe.org/</code>) are unchanged; this applies only to links between documentation <code>.md</code> files in this repo.</p>"},{"location":"AGENTS/#documentation-conventions","title":"Documentation conventions","text":"<ul> <li>Tone: keep it factual and direct; avoid marketing terms like \u201cquickstart,\u201d \u201cfastest,\u201d or \u201cseamless.\u201d</li> <li>Style guide: follow <code>docs_style_guide.md</code> for structure and terminology.</li> <li>Social cards: if a page needs a specific social card title, add front matter:   <pre><code>---\ntitle: Your page title\n---\n</code></pre></li> <li>Links: prefer Markdown links for URLs in prose, not bare URLs.</li> <li>AI providers: provider docs live under <code>ai_providers/</code> (e.g. <code>ai_providers/openrouter.md</code>).</li> <li>MCP setup: the MCP setup guide is <code>mcp/mcp_setup.md</code> (avoid \u201cquickstart\u201d).</li> </ul>"},{"location":"AGENTS/#local-preview","title":"Local preview","text":"<p>To build and preview the same site locally:</p> <ol> <li>Clone both PlanExe and PlanExe-docs.  </li> <li>From PlanExe-docs, run <code>python build.py</code> (optionally set <code>PLANEXE_REPO</code> if PlanExe is not at <code>../PlanExe</code>).  </li> <li>This copies <code>PlanExe/docs/</code> into a temp <code>docs/</code> dir, runs <code>mkdocs build</code>, and writes output to <code>site/</code>.  </li> <li>Run <code>python serve.py</code> to serve <code>site/</code> at <code>http://127.0.0.1:18525/</code>.</li> </ol>"},{"location":"AGENTS/#auto-deploy-from-planexe-optional","title":"Auto-deploy from PlanExe (optional)","text":"<p>To have the live site update when you push to PlanExe <code>main</code> with changes under <code>docs/</code>:</p> <ol> <li>In PlanExe repo: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret.</li> <li>Name: <code>PLANEXE_DOCS_DISPATCH_TOKEN</code>. Value: a Personal Access Token (or fine-grained PAT) with repo scope for PlanExeOrg/PlanExe-docs (or at least permission to trigger workflows in PlanExe-docs).</li> <li>Push to PlanExe <code>main</code> with changes under <code>docs/</code>. The workflow <code>.github/workflows/docs-update.yml</code> runs and sends <code>repository_dispatch</code> to PlanExe-docs; PlanExe-docs then checks out PlanExe, copies <code>docs/</code>, builds, and deploys.</li> </ol> <p>If the secret is not set, the \"Notify docs deploy\" workflow in PlanExe will fail at the dispatch step. You can still update the live site by running the Deploy Documentation workflow manually in PlanExe-docs (Actions \u2192 Deploy Documentation \u2192 Run workflow), or by pushing to PlanExe-docs <code>main</code>.</p>"},{"location":"AGENTS/#summary","title":"Summary","text":"<p>Edits in PlanExe/docs/ are what get published. PlanExe-docs orchestrates copy \u2192 MkDocs build \u2192 GitHub Pages deploy to docs.planexe.org. Push to PlanExe-docs <code>main</code>, trigger the Deploy workflow manually in PlanExe-docs, or set up <code>PLANEXE_DOCS_DISPATCH_TOKEN</code> in PlanExe so pushes to <code>docs/</code> auto-trigger the deploy.</p>"},{"location":"PLANEXE_SPRINT_PLAN/","title":"PlanExe Sprint Plan (2\u2011Week Outline from Simon + Bot)","text":"<p>Source: #openclaw channel responses from Simon + Simon Strandgaard Bot (2026\u201102\u201108)</p>"},{"location":"PLANEXE_SPRINT_PLAN/#executive-summary","title":"Executive Summary","text":"<p>Simon wants to shift PlanExe toward a ranking/metrics pipeline for plans and a production\u2011ready MCP cloud with a Stripe top\u2011up flow. The bot supplied a detailed technical plan for KPI extraction + Elo ranking, including DB schema changes, endpoints, and a light UI/exports. Simon\u2019s core concern: don\u2019t let plans game a static weighted sum. Use a 2\u2011phase LLM evaluation (raw KPI extraction \u2192 pairwise comparison) to produce Elo updates.</p>"},{"location":"PLANEXE_SPRINT_PLAN/#stated-priorities-from-simon","title":"Stated Priorities (from Simon)","text":"<p>Priority 1: Get <code>mcp_cloud</code> live on Railway Priority 2: Stripe top\u2011up flow + credit UI (quick win) Priority 3: KPI extraction scaffolding (foundation for ranking)</p>"},{"location":"PLANEXE_SPRINT_PLAN/#twoweek-sprint-outline-bot","title":"Two\u2011Week Sprint Outline (Bot)","text":""},{"location":"PLANEXE_SPRINT_PLAN/#week-1","title":"Week 1","text":"<ul> <li>Mon: Implement Stripe top\u2011up flow + credit UI</li> <li>Tue: Add <code>/api/credit</code> and webhook handler</li> <li>Wed: Write minimal \u201cmetrics viewer\u201d page for debugging</li> <li>Thu: Refactor <code>mcp_cloud</code> to call KPI module after plan generation and store metrics</li> <li>Fri: Deploy to Railway; smoke test (create plan \u2192 credit deducted)</li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#week-2","title":"Week 2","text":"<ul> <li>Mon: Start KPI extraction module from plan JSON</li> <li>Tue: Add <code>plan_metrics</code> API endpoint; unit tests</li> <li>Wed: Expand ranking (Elo), leaderboard UI</li> <li>Thu: Add export endpoint (<code>/api/export</code>) for top\u2011N plans</li> <li>Fri: Documentation + demo video</li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#ranking-engine-design-key-takeaways","title":"Ranking Engine Design (Key Takeaways)","text":""},{"location":"PLANEXE_SPRINT_PLAN/#why-prevent-gaming","title":"Why: Prevent Gaming","text":"<p>Static linear weights can be gamed. Solution: two\u2011phase LLM evaluation.</p>"},{"location":"PLANEXE_SPRINT_PLAN/#phase-1-raw-kpi-extraction-01-continuous","title":"Phase 1 \u2013 Raw KPI Extraction (0\u20111 continuous)","text":"<p>KPI examples: - novelty_score (embedding similarity) - prompt_quality (LLM rating) - technical_completeness (WBS depth + dependencies) - feasibility (cost vs budget) - impact_estimate (ROI placeholder)</p>"},{"location":"PLANEXE_SPRINT_PLAN/#phase-2-pairwise-llm-comparison","title":"Phase 2 \u2013 Pairwise LLM Comparison","text":"<p>LLM compares KPI vectors for Plan A vs Plan B \u2192 returns Likert preference. Use that to update Elo for both plans.</p>"},{"location":"PLANEXE_SPRINT_PLAN/#sampling-strategy","title":"Sampling Strategy","text":"<p>Avoid O(N\u00b2) comparisons. - Bucket\u2011wise tournament (100 plans per bucket) - Optional ANN nearest\u2011neighbor pre\u2011filter (PGVector/Faiss) - Sample ~10\u201320 neighbors per new plan</p>"},{"location":"PLANEXE_SPRINT_PLAN/#concrete-implementation-items","title":"Concrete Implementation Items","text":""},{"location":"PLANEXE_SPRINT_PLAN/#db","title":"DB","text":"<ul> <li><code>plan_corpus</code> table (seeded from PlanExe-web examples)</li> <li><code>plan_metrics</code> table with KPI columns + <code>elo</code> + optional comment</li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#backend-endpoints","title":"Backend Endpoints","text":"<ul> <li><code>POST /api/rank</code> (rank a plan, update Elo)</li> <li><code>GET /api/leaderboard</code> (top plans by Elo)</li> <li><code>GET /api/export</code> (JSON dump of top\u2011N)</li> <li>(optional) <code>GET /metrics/&lt;plan_id&gt;</code></li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#ui","title":"UI","text":"<ul> <li><code>rankings.html</code> (simple Jinja table pulling from leaderboard)</li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#testing","title":"Testing","text":"<ul> <li><code>docker-compose.test.yml</code> to smoke test <code>/api/rank \u2192 /api/leaderboard</code></li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#gaps-open-questions-to-resolve","title":"Gaps / Open Questions to Resolve","text":"<ol> <li>Which priority gets staffed first? (MCP cloud vs Stripe vs ranking)</li> <li>LLM provider for KPI extraction? (OpenAI/Anthropic/Gemini) and budget limits</li> <li>Plan corpus source of truth? (PlanExe-web YAML vs DB vs MCP outputs)</li> <li>Vector store decision? (PGVector vs Faiss vs none)</li> <li>KPI definitions (weights, normalizations, thresholds) \u2013 final spec needed</li> <li>Human\u2011in\u2011the\u2011loop? (store LLM justification + manual audit of random samples)</li> <li>Security &amp; abuse: Rate\u2011limit <code>/api/rank</code> so users can\u2019t spam Elo</li> <li>Deployment: Who owns Railway creds and rollout timeline?</li> </ol>"},{"location":"PLANEXE_SPRINT_PLAN/#suggested-next-actions-for-larry","title":"Suggested Next Actions (for Larry)","text":"<ul> <li>Confirm with Simon which priority is highest to start coding first</li> <li>Ask whether LLM scoring should use OpenAI or Claude</li> <li>Decide if vector store is required for v1 or can be deferred</li> <li>Draft the migration scripts + <code>plan_metrics</code> schema</li> <li>Build the ranking MVP (KPI stub + Elo updates + leaderboard API)</li> </ul>"},{"location":"PLANEXE_SPRINT_PLAN/#notes","title":"Notes","text":"<ul> <li>Simon\u2019s concern: weighted sum can be gamed \u2192 must use LLM\u2011derived KPIs + pairwise comparison.</li> <li>Bot offered to provide ready\u2011made diffs for ranking engine components.</li> </ul>"},{"location":"costs_and_models/","title":"Costs and models","text":"<p>PlanExe makes many LLM calls per plan. Model choice affects cost, speed, and quality.</p>"},{"location":"costs_and_models/#guidance","title":"Guidance","text":"<ul> <li>Most reliable: paid cloud models via OpenRouter.</li> <li>Lowest cost: older, smaller models (quality can drop).</li> <li>Local models: require strong hardware and are slower.</li> <li>Speed matters: tokens per second can be the difference between minutes and hours.</li> </ul>"},{"location":"costs_and_models/#typical-costs","title":"Typical costs","text":"<p>Costs vary by model and prompt size. PlanExe can use 100+ calls per plan, so avoid expensive models unless you need the highest quality.</p>"},{"location":"costs_and_models/#speed-and-iteration","title":"Speed and iteration","text":"<p>Fast models can complete a plan in roughly 10\u201320 minutes. Slow models may take hours. In practice, it is often better to iterate quickly and generate several candidate plans than to wait for one slow run.</p>"},{"location":"costs_and_models/#choosing-a-provider","title":"Choosing a provider","text":"<ul> <li>OpenRouter: easiest path for most users.</li> <li>Ollama / LM Studio: good for local experimentation.</li> </ul> <p>See the provider guides: - OpenRouter - Ollama - LM Studio</p>"},{"location":"deployment_hosting/","title":"Deployment and hosting","text":"<p>This page summarizes ways to deploy PlanExe.</p>"},{"location":"deployment_hosting/#local-recommended-for-most-users","title":"Local (recommended for most users)","text":"<ul> <li>Use Docker with the single\u2011user UI.</li> <li>See: Getting started</li> </ul>"},{"location":"deployment_hosting/#railway","title":"Railway","text":"<ul> <li>See the experimental guide: Railway</li> <li>Expect to tune env vars and ports for production use.</li> </ul>"},{"location":"deployment_hosting/#mcp-deployments","title":"MCP deployments","text":"<ul> <li>MCP server lives in <code>mcp_cloud</code>.</li> <li>Local proxy lives in <code>mcp_local</code>.</li> <li>Start with: MCP setup</li> </ul>"},{"location":"docker/","title":"PlanExe uses Docker","text":"<p>Docker is the supported way to run PlanExe locally and in most deployments. This page covers common Docker workflows and troubleshooting.</p>"},{"location":"docker/#basic-lifecycle","title":"Basic lifecycle","text":"<ul> <li>Stop everything: <code>docker compose down</code></li> <li>Build fresh (no cache) after code moves: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user</code></li> <li>Start services: <code>docker compose up</code></li> <li>Stop services (leave images): <code>docker compose down</code></li> <li>Build fresh and start services: <code>docker compose build --no-cache database_postgres worker_plan frontend_single_user frontend_multi_user &amp;&amp; docker compose up</code></li> </ul>"},{"location":"docker/#while-developing","title":"While developing","text":"<ul> <li>Live rebuild/restart on changes: <code>docker compose watch</code> (requires Docker Desktop 4.28+).   If watch misses changes after file moves, rerun the no-cache build above.</li> <li>View logs: </li> <li><code>docker compose logs -f worker_plan</code></li> <li><code>docker compose logs -f frontend_single_user</code></li> <li><code>docker compose logs -f frontend_multi_user</code></li> </ul>"},{"location":"docker/#run-individual-files","title":"Run individual files","text":"<ul> <li>Rebuild the worker image when code or data files change: <code>docker compose build --no-cache worker_plan</code>.</li> <li>Run a one-off module inside the worker image (same deps/env as the API): <code>docker compose run --rm worker_plan python -m worker_plan_internal.fiction.fiction_writer</code> (swap the module path as needed). If containers are already up, use <code>docker compose exec worker_plan python -m ...</code> instead.</li> <li>For host Ollama access, set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:11434</code> (default Ollama port). On Linux, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> if that hostname is missing, or use your bridge IP.</li> <li>Ensure required env vars (e.g., <code>DEFAULT_LLM</code>) are available via <code>.env</code> or your shell before running the command.</li> </ul>"},{"location":"docker/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the pipeline stops immediately with missing module errors, rebuild with <code>--no-cache</code> so new files are inside the images.</li> <li>If you change environment variables (e.g., <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT</code>), restart: <code>docker compose down</code> then <code>docker compose up</code>.</li> <li>If <code>frontend_multi_user</code> can't start because host port 5000 is busy, map it elsewhere: <code>export PLANEXE_FRONTEND_MULTIUSER_PORT=5001</code> (or another free port) before <code>docker compose up</code>.</li> <li>To clean out containers, network, and orphans: <code>docker compose down --remove-orphans</code>.</li> <li>To reclaim disk space when builds start failing with <code>No space left on device</code>:</li> <li>See current usage: <code>docker system df</code></li> <li>Aggressively prune (images, caches, networks not in use): <code>docker system prune -a</code><ul> <li>Expect a confirmation prompt; this removed ~37 GB here by deleting unused images and build cache.</li> </ul> </li> <li>If needed, prune build cache separately: <code>docker builder prune</code></li> </ul>"},{"location":"docker/#port-5432-already-in-use-postgres-conflict","title":"Port 5432 already in use (Postgres conflict)","text":"<p>If <code>database_postgres</code> fails to start with a \"port already in use\" error, another PostgreSQL is likely running on your machine. This is common on developer machines where you have: - macOS: Postgres.app (a popular menu-bar Postgres), Homebrew PostgreSQL (<code>brew install postgresql</code>), or pgAdmin's bundled server - Linux: System PostgreSQL installed via <code>apt install postgresql</code> or similar - Windows: PostgreSQL installer, pgAdmin, or other database tools</p> <p>Solution: Set <code>PLANEXE_POSTGRES_PORT</code> to a different value: <pre><code>export PLANEXE_POSTGRES_PORT=5433\ndocker compose up\n</code></pre></p> <p>This only affects the HOST port (how you access Postgres from your machine). Inside Docker, containers always connect to each other on port 5432\u2014this is hardcoded and unaffected by <code>PLANEXE_POSTGRES_PORT</code>.</p> <p>To make this permanent, add to your <code>.env</code> file: <pre><code>PLANEXE_POSTGRES_PORT=5433\n</code></pre></p> <p>When connecting from your host machine (e.g., DBeaver, <code>psql</code>), use the port you set: <pre><code>psql -h localhost -p 5433 -U planexe -d planexe\n</code></pre></p>"},{"location":"docker/#environment-notes","title":"Environment notes","text":"<ul> <li>The worker exports logs to stdout when <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT=true</code> (set in <code>docker-compose.yml</code>).</li> <li>Shared volumes: <code>./run</code> is mounted into both services; <code>.env</code> and <code>llm_config.json</code> are mounted read-only. Ensure they exist on the host before starting.***</li> <li>Database: Postgres runs in <code>database_postgres</code> and listens on host <code>${PLANEXE_POSTGRES_PORT:-5432}</code> mapped to container <code>5432</code>; data is persisted in the named volume <code>database_postgres_data</code>.</li> <li>Multiuser UI: binds to container port <code>5000</code>, exposed on host <code>${PLANEXE_FRONTEND_MULTIUSER_PORT:-5001}</code>.</li> <li>MCP server downloads: set <code>PLANEXE_MCP_PUBLIC_BASE_URL</code> so clients receive a reachable <code>/download/...</code> URL (defaults to <code>http://localhost:8001</code> in compose).</li> </ul>"},{"location":"docker/#host-opener-open-output-dir","title":"Host opener (Open Output Dir)","text":"<p>Because Docker containers cannot launch host apps, the <code>Open Output Dir</code> button needs a host-side service.</p> <p>Set these environment variables before starting: - <code>PLANEXE_OPEN_DIR_SERVER_URL</code> so the container can reach the host opener:   - macOS/Windows (Docker Desktop): <code>http://host.docker.internal:5100</code>   - Linux: <code>http://172.17.0.1:5100</code> (or add <code>host.docker.internal</code> pointing to the bridge IP). - <code>PLANEXE_HOST_RUN_DIR</code>: optional; defaults to <code>PlanExe/run</code> on the host. Set an absolute path if you relocate the run directory.</p> <p>1) Start host opener before Docker (on the host): <pre><code>cd open_dir_server\npython -m venv venv\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\npip install -r requirements.txt\npython app.py\n</code></pre> 2) Provide <code>PLANEXE_OPEN_DIR_SERVER_URL</code> via your shell env, <code>.env</code>, or docker compose environment for <code>frontend_single_user</code>.</p>"},{"location":"docs_style_guide/","title":"Docs style guide","text":"<p>Short, consistent rules for PlanExe docs.</p>"},{"location":"docs_style_guide/#tone-and-voice","title":"Tone and voice","text":"<ul> <li>Be direct and practical.</li> <li>Be factual and specific.</li> <li>Avoid marketing terms like: quickstart, fastest.</li> <li>Avoid long personal stories in setup guides.</li> <li>Prefer short sentences.</li> </ul>"},{"location":"docs_style_guide/#structure","title":"Structure","text":"<ul> <li>Start with a 1\u20132 sentence summary.</li> <li>Use numbered steps for setup.</li> <li>End with \u201cNext steps\u201d or \u201cTroubleshooting.\u201d</li> </ul>"},{"location":"docs_style_guide/#terminology","title":"Terminology","text":"<ul> <li>Use plan for the output.</li> <li>Use report for the HTML artifact.</li> <li>Use MCP server for the PlanExe MCP service.</li> </ul>"},{"location":"docs_style_guide/#code-blocks","title":"Code blocks","text":"<ul> <li>Use fenced code blocks with language when possible.</li> <li>Keep commands copy\u2011paste friendly.</li> </ul>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-planexe-a-finished-plan","title":"Is PlanExe a finished plan?","text":"<p>No. It creates a strong draft and structure, but the final plan still needs human review and refinement.</p>"},{"location":"faq/#how-long-does-a-plan-take","title":"How long does a plan take?","text":"<p>Depends on provider and model. Expect minutes, not seconds.</p>"},{"location":"faq/#what-does-it-cost","title":"What does it cost?","text":"<p>Costs depend on the model. Paid cloud models are more reliable; local models are slower but can be cheaper.</p>"},{"location":"faq/#why-is-the-output-vague-sometimes","title":"Why is the output vague sometimes?","text":"<p>Prompts that are too short or unclear usually produce vague output. Use the Prompt writing guide.</p>"},{"location":"faq/#can-i-run-it-locally","title":"Can I run it locally?","text":"<p>Yes. Follow Getting started to run with Docker.</p>"},{"location":"faq/#where-are-the-outputs-saved","title":"Where are the outputs saved?","text":"<p>On local runs, outputs are written to <code>run/</code> in the repo root.</p>"},{"location":"getting_started/","title":"Getting started with PlanExe","text":"<p>This guide shows new users how to launch the <code>frontend_single_user</code> UI with Docker using OpenRouter as the LLM provider. No local Python or pip setup is needed.</p>"},{"location":"getting_started/#1-prerequisites","title":"1. Prerequisites","text":"<p>Install Docker.</p> <p>Create an account on OpenRouter and top up around 5 USD in credits (paid models works, the free models are unreliable). It cost around 0.1 USD to generate a plan, when using PlanExe's default settings.</p>"},{"location":"getting_started/#2-clone-the-repo","title":"2. Clone the repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre>"},{"location":"getting_started/#3-configure-secrets","title":"3. Configure secrets","text":"<p>Copy <code>.env.docker-example</code> to <code>.env</code>.</p> <p>Add your OpenRouter key: <pre><code>OPENROUTER_API_KEY='sk-or-v1-your-key'\n</code></pre></p>"},{"location":"getting_started/#4-start-the-single-user-stack","title":"4. Start the single-user stack","text":"<pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> <p>Wait for http://localhost:7860 to become available.</p> <p>Stop with <code>Ctrl+C</code>.</p>"},{"location":"getting_started/#5-use-the-ui","title":"5. Use the UI","text":"<p>Open http://localhost:7860 in your browser. </p> <p>You can now submit your prompt.</p> <p>The generated plans are written to <code>run/&lt;timestamped-output-dir&gt;</code>.</p> <p></p>"},{"location":"getting_started/#verification","title":"Verification","text":"<ul> <li>You can open the UI at http://localhost:7860.</li> <li>A plan run creates a new folder in <code>run/</code>.</li> </ul>"},{"location":"getting_started/#troubleshooting-and-next-steps","title":"Troubleshooting and next steps","text":"<ul> <li>For Docker tips, see docker.md.</li> <li>For OpenRouter-specific notes, see openrouter.md.</li> <li>If the UI fails to load or plans don\u2019t start, check worker logs: <code>docker compose logs -f worker_plan</code>.</li> <li>Learn how to write better prompts: Prompt writing guide</li> </ul>"},{"location":"getting_started/#community","title":"Community","text":"<p>Need help? Join the PlanExe Discord.</p>"},{"location":"install_developer/","title":"Installing PlanExe for developers","text":"<p>I assume that you are a python developer.</p> <p>You need several open terminals to do development on this project.</p>"},{"location":"install_developer/#clone-repo","title":"Clone repo","text":"<pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\n</code></pre>"},{"location":"install_developer/#prepare-env-file","title":"Prepare <code>.env</code> file","text":"<p>Create a <code>.env</code> file from the <code>.env.developer-example</code> file.</p> <p>Update <code>OPENROUTER_API_KEY</code> with your open router api key.</p>"},{"location":"install_developer/#open_dir_server","title":"<code>open_dir_server</code>","text":"<p>In a new terminal:  Follow the open_dir_server instructions.</p>"},{"location":"install_developer/#worker_plan","title":"<code>worker_plan</code>","text":"<p>In a new terminal:  Follow the worker_plan instructions.</p>"},{"location":"install_developer/#frontend_single_user","title":"<code>frontend_single_user</code>","text":"<p>In a new terminal:  Follow the frontend_single_user instructions.</p>"},{"location":"install_developer/#database_postgres","title":"<code>database_postgres</code>","text":"<p>In a new terminal:  Follow the database_postgres instructions.</p>"},{"location":"install_developer/#worker_plan_database","title":"<code>worker_plan_database</code>","text":"<p>In a new terminal:  Follow the worker_plan_database instructions.</p>"},{"location":"install_developer/#frontend_multi_user","title":"<code>frontend_multi_user</code>","text":"<p>In a new terminal:  Follow the frontend_multi_user instructions.</p>"},{"location":"install_developer/#tests","title":"Tests","text":"<p>In a new terminal:  Run the tests to ensure that the project works correctly. <pre><code>PROMPT&gt; python test.py\nsnip lots of output snip\nRan 117 tests in 0.059s\n\nOK\n</code></pre></p>"},{"location":"install_developer/#now-planexe-have-been-installed","title":"Now PlanExe have been installed.","text":""},{"location":"llm_config/","title":"LLM config (llm_config.json)","text":"<p>This file defines which LLM providers and models PlanExe can use. Each top\u2011level key is a model id used in the UI and pipeline.</p> <p><code>llm_config.json</code> lives in the PlanExe repo root and is read at runtime. Environment variables are substituted from <code>.env</code>.</p>"},{"location":"llm_config/#file-structure","title":"File structure","text":"<pre><code>{\n  \"model-id\": {\n    \"comment\": \"Human description\",\n    \"priority\": 1,\n    \"luigi_workers\": 4,\n    \"class\": \"OpenRouter\",\n    \"arguments\": {\n      \"model\": \"google/gemini-2.0-flash-001\",\n      \"api_key\": \"${OPENROUTER_API_KEY}\",\n      \"temperature\": 0.1,\n      \"timeout\": 60.0,\n      \"is_function_calling_model\": false,\n      \"is_chat_model\": true,\n      \"max_tokens\": 8192,\n      \"max_retries\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"llm_config/#top-level-fields","title":"Top-level fields","text":"<ul> <li>comment: Plain\u2011text description for humans. Optional.</li> <li>priority: Lower number = higher priority when <code>auto</code> is selected. Optional.</li> <li>luigi_workers: Number of Luigi workers used for this model. Use <code>1</code> for local models (Ollama/LM Studio).</li> <li>class: Provider class name (e.g., <code>OpenRouter</code>, <code>OpenAI</code>, <code>Ollama</code>, <code>LMStudio</code>, <code>OpenAILike</code>).</li> <li>arguments: Provider\u2011specific settings passed to the LLM client.</li> </ul>"},{"location":"llm_config/#common-arguments","title":"Common arguments","text":"<p>These keys are common across most providers:</p> <ul> <li>model / model_name: Provider model identifier.</li> <li>api_key: API key reference (usually <code>${ENV_VAR}</code>).</li> <li>base_url / api_base: Override the provider base URL.</li> <li>temperature: Controls randomness. Lower is more deterministic.</li> <li>timeout / request_timeout: Max time per request in seconds.</li> <li>max_tokens / max_completion_tokens: Output token limit (provider specific).</li> <li>max_retries: Retry count on transient errors.</li> <li>is_function_calling_model: Whether the model supports structured/tool output.</li> <li>is_chat_model: Whether the model uses chat format.</li> </ul>"},{"location":"llm_config/#choosing-values","title":"Choosing values","text":"<ul> <li>Use luigi_workers = 1 for local models (Ollama / LM Studio).</li> <li>Use luigi_workers &gt; 1 for cloud models if you want parallel tasks.</li> <li>Keep timeout higher for slower models.</li> </ul>"},{"location":"llm_config/#notes","title":"Notes","text":"<ul> <li>If <code>llm_config.json</code> is missing, PlanExe logs a warning and proceeds with defaults.</li> <li>Changes to <code>llm_config.json</code> require a container restart (or rebuild if baked into the image).</li> </ul>"},{"location":"plan/","title":"Future plan for PlanExe","text":"<p>Using the \"5 Whys\" method:</p> <p>I want multiple-agents talking with each other, via zulip.</p> <p>Why?</p> <p>Can I \u201cexecute\u201d a plan from start to finish, with the agents doing all the labor. Zulip is open source. So I\u2019m not dependent on Discord/Teams/Slack.</p> <p>Why?</p> <p>When humans are doing the labor, they have to decompose the problem into tasks.  In my experience with PlanExe, AI can decompose sometimes better/worse than humans. Possible via MCP to interface with issue tracker. delegate parts of the plan to humans.</p> <p>Why?</p> <p>Companies spend lots of effort on planning and getting the right people to communicate with meetings, emails. Something I dislike about working in coding jobs. Wasting time and money on planning.</p> <p>Why?</p> <p>Cut cost and optimize speed.</p> <p>Why?</p> <p>To satisfy my own curiosity. I\u2019m curious to what kind of outcome it is. An AI organization/company, possible distributed network. Is it a global organism as seen in scifi movies that are controlled by AIs, that takes power away from politicians. My concerns are: will it be able to adapt to a changing world. Re-plan in real-time when a shipment is delayed, a machine breaks down, or an unexpected storm hits. quiet, compounding errors, security oversights, and cost blowouts.</p>"},{"location":"plan/#execute-the-plan","title":"Execute the plan","text":"<p>Currently it's up to humans to execute a plan. How can this be automated?</p> <p>Ideally take an entire plan and go with it.</p>"},{"location":"plan/#improve-plan","title":"Improve plan","text":"<p>Prompt optimizing with A/B testing: Make tiny tweaks to one system prompt at a time, and see how it compares to baseline. If most generated plans gets improved, then keep the new system prompt. Verify across multiple LLMs/reasoning models, that the new system prompt makes an improvement. Store the new system prompt in the repo. Find weaknesses that are common for the generated plans. Pick the earliest task in the pipeline that impact this weakness. Schedule this weakness for the next A/B test improvement iteration.</p> <p>Boost initial prompt: The <code>initial prompt</code> has the biggest impact on the generated plan, if it's bad then the final plan is bad. If it's well written, concise, there is a higher chance for a realistic/feasible plan. Currently I use AIs to write the initial prompt for me by first having a long conversation about the topic, and showing examples of other initial prompts that have worked well in the past. It may by a small tweak to the initial prompt and it yields a better plan. It may be an entire rewrite of the initial prompt. The user may have specified a vague prompt, or the user may not be domain expert, the prompt may be non-sense, or the prompt may be overly specific so PlanExe attends to the wrong things. Suggest changes to the initial prompt. This can be by picking a bigger budget, a different technology, a different set of levers, fixing typos.</p> <ul> <li>User specifies a budget of 0..100 USD. Which is unrealistic, when the plan is to hire a team, and work on it for months.</li> <li>User leaves out physical location(s). So PlanExe picks a random location in a different part of the world.</li> </ul> <p>Dynamic plugins: Have AI's rewrite PlanExe as they see fit, depending on what the user have prompted it with. So if it's a software project, it writes PlanExe plugins that are going to be needed. And then proceeds to creating the plan. In the middle of the plan creation, it may be necessary to create more PlanExe plugins as issues shows up.</p> <p>Grid search: Currently PlanExe only generates a plan for 1 permutation of levers. A plan may have 10 levers with 3-5 settings. Here it could be interesting to create  100 full plans, each with a different combination of levers. Compare the generated plans against each other  and pick the most 3 promising plans.</p> <p>Multiple refinements: Currently PlanExe generates the first iteration of the plan. Usually issues arises when making the first iteration, that have to be incorporated into the timeline. In the future I want to do multiple iterations, until the plan is of a reasonable quality.</p> <p>Validate the plan with deep research: Currently there is no validation. It's up to humans to be skeptic about the plan, does this make sense, check everything. There may be issues with: assumptions, numbers, flaws.</p> <p>Money: Currently the LLMs make up numbers. Alternate between these: Tweak the plan. Tweak the budget. Repeat. Obtain latest market data. Obtain info about what resources the user has available. Populate a Cost-Breakdown-Structure.</p> <p>Gantt in parallel: Currently the Gantt is waterfall. For a team with several people it's possible to do tasks in parallel. Obtain info about what resources the user has available, and if they are willing to do tasks in parallel.</p>"},{"location":"plan/#secondary-issues","title":"Secondary issues","text":""},{"location":"plan/#luigi-can-run-tasks-in-parallel","title":"Luigi can run tasks in parallel","text":"<p>I'm not making use of it. </p> <p>Until 2026-jan-01 I had this limitation: The PythonAnywhere doesn't like long running child processes/threads, anything longer than 5 minutes gets killed. There are always-on workers, but these must not spawn long running processes/threads. I'm considering finding another provider. Starting from 2026-jan-01 I\u2019m using Docker and no longer using pythonanywhere, I can start looking into running parallel tasks within Luigi.</p>"},{"location":"plan/#mcp-on-railway","title":"MCP on Railway","text":"<p>Doing inference in the cloud cost money. If users are to use MCP in the cloud, they will have to pay for it.</p> <ul> <li>Scenario A: Users can buy credit via <code>PLANEXE_MCP_API_KEY</code>.</li> <li>Scenario B: Users can BYOK (Bring your own key).</li> </ul> <p>Both scenarios will need user management with login.</p>"},{"location":"plan/#tertiary-issues","title":"Tertiary issues","text":""},{"location":"plan/#capture-reasoning-response","title":"Capture reasoning response","text":"<p>Currently I only capture the final response, without any reasoning. I want to capture the reasoning, since it may be helpful for troubleshooting. Or for other AIs to assess the reasoning steps leading up to the response.</p>"},{"location":"plan/#token-counting","title":"Token counting","text":"<p>So that I can see how much does it cost to generate a plan. Reasoning models. How many tokens are spent on reasoning vs generating the final response.</p>"},{"location":"plan/#debugging","title":"Debugging","text":"<p>Get step-by-step debugging working again. Now that I have switched to Docker, I have multiple python projects in the same repo, that use different incompatible packages.</p>"},{"location":"plan/#github-ci-that-runs-tests","title":"GitHub CI that runs tests","text":"<p>The hard thing is getting the venv's working.</p>"},{"location":"plan/#table-of-content","title":"Table of content","text":"<p>Currently the generated report has expandable/collapsible sections. There is an overwhelming amount of content inside each sections. I'm considering having a table of content in the left sidebar, similar to this: Railway Dockerfiles guide It uses Docusaurus which uses React. I'm no fan of React. I'm considering using mkdocs instead.</p>"},{"location":"plan/#eliminate-redundant-user-prompts-in-the-log-file","title":"Eliminate redundant user prompts in the log file","text":"<p>Get rid of some of the many user prompt logging statements, so the log.txt is less noisy. These user prompts are saved to the <code>track_activity.jsonl</code> file already. So having them in the log.txt is redundant.</p>"},{"location":"plan/#ssl-when-connecting-with-the-database","title":"SSL when connecting with the database","text":"<p>I can't afford the pro plan to have a dedicated Postgres server. Currently when connecting to Railway, it's via a TCP Proxy and it's unencrypted. Either upgrade to pro, or use SSL certificates within the \"database_postgres\" Dockerfile.</p>"},{"location":"plan_output_anatomy/","title":"Plan output anatomy","text":"<p>PlanExe produces a single HTML report. This page explains what each section is for and how to use it.</p>"},{"location":"plan_output_anatomy/#executive-summary","title":"Executive summary","text":"<ul> <li>Use it to validate scope, deliverables, and assumptions.</li> <li>If this is wrong, the rest will be wrong too.</li> </ul>"},{"location":"plan_output_anatomy/#gantt-chart","title":"Gantt chart","text":"<ul> <li>A draft timeline with dependencies.</li> <li>Validate durations and sequencing with domain experts.</li> </ul>"},{"location":"plan_output_anatomy/#governance-structure","title":"Governance structure","text":"<ul> <li>Roles, decision rights, and accountability.</li> <li>Useful for stakeholder alignment early.</li> </ul>"},{"location":"plan_output_anatomy/#risk-register","title":"Risk register","text":"<ul> <li>A first pass at risks and mitigations.</li> <li>Expect to add domain\u2011specific risks.</li> </ul>"},{"location":"plan_output_anatomy/#swot-analysis","title":"SWOT analysis","text":"<ul> <li>A strategic snapshot of strengths/weaknesses/opportunities/threats.</li> <li>Good for framing strategy, not for execution details.</li> </ul>"},{"location":"plan_output_anatomy/#next-steps","title":"Next steps","text":"<ul> <li>A prioritized list of actions to start implementation.</li> <li>Use it to build a real project backlog.</li> </ul>"},{"location":"plan_output_anatomy/#how-to-use-the-plan","title":"How to use the plan","text":"<ol> <li>Validate scope (Executive summary).</li> <li>Correct timelines (Gantt).</li> <li>Stress\u2011test risks with stakeholders.</li> <li>Refine budgets (PlanExe budgets are headline\u2011only).</li> <li>Turn next steps into tickets.</li> </ol>"},{"location":"prompt_writing_guide/","title":"Prompt writing guide","text":"<p>PlanExe creates better plans when the input prompt is detailed and specific. Aim for 300\u2013800 words.</p>"},{"location":"prompt_writing_guide/#what-a-good-prompt-includes","title":"What a good prompt includes","text":"<ul> <li>Goal and scope: what you want, and what you explicitly do not want.</li> <li>Audience: who the plan is for (customers, users, stakeholders).</li> <li>Constraints: budget, timeline, geography.</li> <li>Location(s): country/region/city. If you have it, include the exact street address. Regulations and feasibility change by location.</li> <li>Success criteria: what success looks like, and how it is measured.</li> <li>Resources: team size, skills, existing assets, tools.</li> </ul>"},{"location":"prompt_writing_guide/#good-vs-weak-prompts","title":"Good vs. weak prompts","text":"<p>Weak</p> <p>Construct a bridge.</p> <p>Better</p> <p>Construct a bridge between Spain and Morocco across the Strait of Gibraltar. Target a feasibility\u2011to\u2011groundbreak timeline of 5 years with a total budget range of 8\u201312B EUR. The plan should cover geotechnical surveys, environmental impact assessments, maritime traffic coordination, and cross\u2011border permitting. Include options for rail + road, and describe staging for phase 1 (single rail) and phase 2 (road expansion). Exclude toll\u2011system design and focus on structural, logistics, and governance planning.</p>"},{"location":"prompt_writing_guide/#recommended-structure","title":"Recommended structure","text":"<pre><code>Goal:\n\nContext / background:\n\nTarget users / customers:\n\nScope (in/out):\n\nConstraints (budget, timeline, geography):\n\nLocation(s):\n\nSuccess criteria:\n\nTeam / resources available:\n</code></pre>"},{"location":"prompt_writing_guide/#budget-and-money","title":"Budget and money","text":"<p>Budget can be natural language. Examples:</p> <ul> <li>A range: \u201c$200k\u2013$400k total.\u201d</li> <li>Phased: \u201c$10M for phase 1, $5M for phase 2.\u201d</li> <li>With constraints: \u201cCapex only, exclude staffing.\u201d</li> <li>Use standard currencies (EUR, DKK, RUB, BRL, etc.). Crypto budgets (BTC, ETH) are not supported.</li> </ul>"},{"location":"prompt_writing_guide/#budget-mistakes-to-avoid","title":"Budget mistakes to avoid","text":"<ul> <li>Setting the budget to 0/none/N/A when the goal is serious and requires resources.</li> <li>Using a currency code as the budget value (e.g., budget = DKK) instead of a real budget.</li> </ul>"},{"location":"prompt_writing_guide/#common-mistakes-to-avoid","title":"Common mistakes to avoid","text":"<ul> <li>Being too vague (one sentence).</li> <li>Missing constraints (budget, time, scope).</li> <li>Leaving out the location(s), which makes regulations and feasibility assumptions unreliable.</li> <li>Conflicting requirements (e.g., \u201claunch in 2 weeks\u201d + \u201centerprise compliance\u201d).</li> </ul>"},{"location":"railway/","title":"PlanExe on Railway - Experimental","text":"<p>As of 2026-Jan-04, I'm experimenting with Railway. Currently the <code>frontend_multi_user</code> UI is an ugly MVP. I recommend going with the <code>frontend_single_user</code>, that doesn't use database.</p> <p>In this project, the files named <code>railway.md</code> or <code>railway.toml</code>, are related to how things are configured in my Railway setup.</p>"},{"location":"railway/#project-settings","title":"Project Settings","text":""},{"location":"railway/#environments","title":"Environments","text":"<p>Create these environments: - <code>production</code> - <code>staging</code></p>"},{"location":"railway/#shared-variables-production","title":"Shared variables - production","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than staging\n</code></pre>"},{"location":"railway/#shared-variables-staging","title":"Shared variables - staging","text":"<pre><code>PLANEXE_POSTGRES_PASSWORD=unique random text, different than production\n</code></pre>"},{"location":"railway/#using-shared-variables-in-services","title":"Using Shared Variables in Services","text":"<p>Each service that connects to the database must reference the shared password variable in its own environment variables.</p> <p>In Railway, go to each service \u2192 Variables and add:</p> <pre><code>PLANEXE_POSTGRES_PASSWORD=\"${{shared.PLANEXE_POSTGRES_PASSWORD}}\"\n</code></pre> <p>Services that need this variable: - <code>database_postgres</code> - <code>frontend_multi_user</code> - <code>worker_plan_database</code></p> <p>This ensures all services use the same password, and you only need to update it in one place (the shared variables) when rotating credentials.</p>"},{"location":"recent_changes/","title":"Recent Changes in PlanExe","text":""},{"location":"recent_changes/#2025-dec-31","title":"2025-dec-31","text":"<p>PlanExe is now using Docker.</p> <p>So you no longer have to be python developer to install it on your own computer.</p> <p>Over the last month I have migrated PlanExe to Docker.</p> <p>So that I can deploy PlanExe on Railway and similar web providers.</p> <p>Previously I have been using PythonAnywhere, and I was stuck in a dependency hell, where I couldn't add packages without breaking other packages.</p> <p>Now with docker, I don't have these incompatibility issues. However docker have its own issues.</p> <p>The last version BEFORE the transition to docker is available here: PlanExe 2025-dec-31 release</p> <p>The main branch will be docker from now on: PlanExe main branch</p>"},{"location":"start_here/","title":"Start here","text":"<p>Pick the path that matches your goal. Each path is short and points to a deeper guide.</p>"},{"location":"start_here/#path-1-i-want-to-use-planexe-without-local-setup","title":"Path 1: I want to use PlanExe without local setup","text":"<ol> <li>Skim a real report: Minecraft Escape sample report</li> <li>Open the web app and create a report: planexe.org</li> <li>If you like the output, go local for full control: Getting started</li> </ol>"},{"location":"start_here/#path-2-i-want-to-run-planexe-locally-docker","title":"Path 2: I want to run PlanExe locally (Docker)","text":"<ol> <li>Follow: Getting started</li> <li>Learn the Docker lifecycle: Docker</li> <li>Pick an AI provider: OpenRouter (recommended)</li> </ol>"},{"location":"start_here/#path-3-i-want-to-integrate-planexe-via-mcp","title":"Path 3: I want to integrate PlanExe via MCP","text":"<ol> <li>Read the overview: MCP welcome</li> <li>Follow the setup guide: MCP setup</li> <li>See tool details: MCP details</li> </ol>"},{"location":"start_here/#path-4-i-want-to-develop-on-planexe","title":"Path 4: I want to develop on PlanExe","text":"<ol> <li>Install the dev setup: Developer install</li> <li>Read a component doc to understand the architecture: Open dir server</li> <li>Use Docker for local services: Docker</li> </ol>"},{"location":"statistics/","title":"PlanExe Statistics","text":""},{"location":"statistics/#github-stars","title":"GitHub stars","text":"<p>The badge above shows the current star count for PlanExeOrg/PlanExe and links to the repository.</p>"},{"location":"statistics/#star-history","title":"Star history","text":"<p>The chart above shows star count over time. Click it to open the interactive chart on star-history.com.</p>"},{"location":"statistics/#commit-history","title":"Commit history","text":"<p>PlanExe commit activity \u2014 commits per week over the last year (GitHub Insights).</p>"},{"location":"statistics/#activity-on-openrouter","title":"Activity on OpenRouter","text":"<p>OpenRouter shows app analytics (usage, top models) for apps that use their API. PlanExe was originally under a personal GitHub account and is now under the PlanExeOrg organization, so there are two app pages:</p> <ul> <li>Current (PlanExeOrg): PlanExe on OpenRouter \u2014 PlanExeOrg/PlanExe</li> <li>Legacy (neoneye): PlanExe on OpenRouter \u2014 neoneye/PlanExe</li> </ul>"},{"location":"statistics/#discord","title":"Discord","text":"<p>PlanExe community server (ID <code>1337721703534690317</code>).</p> <ul> <li>Join: planexe.org/discord</li> <li>Widget JSON API: discord.com/api/guilds/1337721703534690317/widget.json \u2014 returns server name, invite, channels, members online, and <code>presence_count</code></li> </ul> <p>Live widget:</p>"},{"location":"stripe/","title":"Stripe (credits and local testing)","text":"<p>PlanExe uses Stripe Checkout for buying credits. This page explains how the flow works, why credits may not update when running locally, and how to test without real money.</p>"},{"location":"stripe/#how-credits-are-applied","title":"How credits are applied","text":"<ol> <li>User clicks \"Pay with Stripe\" on the Account page and completes checkout on Stripe\u2019s site.</li> <li>Stripe redirects the user back to your app (e.g. <code>/account?stripe=success</code>).</li> <li>Credits are added only when Stripe sends a webhook. Stripe calls your app at <code>/billing/stripe/webhook</code> with a <code>checkout.session.completed</code> event; the app then creates a <code>PaymentRecord</code> and a <code>CreditHistory</code> entry and updates <code>UserAccount.credits_balance</code>.</li> </ol> <p>So the redirect back to <code>/account</code> does not by itself add credits. The webhook does.</p>"},{"location":"stripe/#why-credits-stay-0-on-localhost","title":"Why credits stay 0 on localhost","text":"<p>When the app runs on <code>localhost</code> (e.g. <code>http://localhost:5001</code>), Stripe\u2019s servers cannot reach your machine. They need to POST to your webhook URL; <code>localhost</code> is only reachable from your own computer. So the <code>checkout.session.completed</code> webhook never hits your app, and credits are never applied.</p> <p>Fix: use the Stripe CLI to forward webhooks from Stripe to your local server.</p>"},{"location":"stripe/#stripe-cli-forward-webhooks-to-localhost","title":"Stripe CLI (forward webhooks to localhost)","text":"<p>The Stripe CLI is a separate developer tool (not listed with the Stripe SDKs). It can tunnel webhook events to your local app.</p>"},{"location":"stripe/#where-to-find-it","title":"Where to find it","text":"<ul> <li>Install: Install the Stripe CLI</li> <li>Overview: Stripe CLI</li> </ul>"},{"location":"stripe/#install-macos-homebrew","title":"Install (macOS, Homebrew)","text":"<pre><code>brew install stripe/stripe-cli/stripe\n</code></pre> <p>Other platforms: see the install guide.</p>"},{"location":"stripe/#use-it-for-webhooks","title":"Use it for webhooks","text":"<ol> <li>Log in (opens browser with a pairing code):</li> </ol> <pre><code>stripe login\n</code></pre> <ol> <li>Start forwarding webhooks to your app (adjust port if needed):</li> </ol> <pre><code>stripe listen --forward-to localhost:5001/billing/stripe/webhook\n</code></pre> <ol> <li>The CLI prints a webhook signing secret (<code>whsec_...</code>). Add it to your environment:</li> </ol> <pre><code>PLANEXE_STRIPE_WEBHOOK_SECRET='whsec_xxxxx'\n</code></pre> <ol> <li>Restart the PlanExe frontend so it loads the new secret. Keep <code>stripe listen</code> running while you test payments.</li> </ol> <p>Events sent to the CLI are forwarded to your local <code>/billing/stripe/webhook</code> and signed with the secret the CLI showed you. Your app can then verify the signature and apply credits.</p>"},{"location":"stripe/#testing-without-real-money-test-mode","title":"Testing without real money (test mode)","text":"<p>Use Stripe test mode so no real charges are made.</p>"},{"location":"stripe/#1-use-test-api-keys","title":"1. Use test API keys","text":"<p>In the Stripe Dashboard, turn on Test mode (toggle top right).</p> <ul> <li>Go to Developers \u2192 API keys (dashboard.stripe.com/test/apikeys).</li> <li>Use the Secret key that starts with <code>sk_test_...</code> (not <code>sk_live_...</code>).</li> </ul> <p>In your <code>.env</code> (or environment) for local/dev:</p> <pre><code>PLANEXE_STRIPE_SECRET_KEY='sk_test_...'\n</code></pre> <p>Use the test key only for development; keep the live key for production.</p>"},{"location":"stripe/#2-test-card-numbers","title":"2. Test card numbers","text":"<p>At checkout, use Stripe\u2019s test card numbers. No real payment is processed.</p> Result Card number Success <code>4242 4242 4242 4242</code> Card declined <code>4000 0000 0000 0002</code> Requires auth <code>4000 0025 0000 3155</code> <ul> <li>Expiry: any future date (e.g. <code>12/34</code>).</li> <li>CVC: any 3 digits (e.g. <code>123</code>).</li> <li>ZIP: any value (e.g. <code>12345</code>).</li> </ul>"},{"location":"stripe/#3-webhook-secret-when-using-the-cli","title":"3. Webhook secret when using the CLI","text":"<p>When you run <code>stripe listen</code>, the signing secret it prints is for test events. Put that value in <code>PLANEXE_STRIPE_WEBHOOK_SECRET</code>. In production you will configure a separate webhook endpoint in the Stripe Dashboard and use that endpoint\u2019s secret.</p>"},{"location":"stripe/#environment-variables","title":"Environment variables","text":"Variable Purpose <code>PLANEXE_STRIPE_SECRET_KEY</code> Stripe secret key (<code>sk_test_...</code> or <code>sk_live_...</code>). Required for checkout and webhooks. <code>PLANEXE_STRIPE_WEBHOOK_SECRET</code> Webhook signing secret (<code>whsec_...</code>). Required to verify that webhook requests come from Stripe. For local dev, use the secret from <code>stripe listen</code>. <code>PLANEXE_STRIPE_CURRENCY</code> Currency for Checkout (default: <code>usd</code>). <code>PLANEXE_CREDIT_PRICE_CENTS</code> Price per credit in cents (default: <code>100</code>). <code>PLANEXE_FRONTEND_MULTIUSER_PUBLIC_URL</code> Public base URL used for Stripe success/cancel redirects (e.g. <code>http://localhost:5001</code> or your production URL)."},{"location":"stripe/#see-also","title":"See also","text":"<ul> <li>User accounts and billing (database) \u2014 tables and flows for credits, payments, and refunds.</li> </ul>"},{"location":"troubleshooting_stuck_pipeline/","title":"Troubleshooting a stuck pipeline","text":"<p>The gradio app (<code>app_text2plan.py</code>) starts the <code>run_plan_pipeline</code> process via a <code>Popen</code> call. </p> <ul> <li>Environment, if the gradio app runs in a slightly different environment than when running via commandline <code>python -m worker_plan_internal.plan.run_plan_pipeline</code>, then the child process may behave differently. I have verified that the parent process and child process runs with the same environment variables.</li> <li>Buffering, if the parent process isn't reading stdout/stderr fast enough, the child process may freeze. I have reworked the <code>Popen</code> code so the stdout/stderr goes to <code>/dev/null</code>.</li> <li>Other issues, if the pipeline still hangs, let me know, it may be some issue I'm not aware of.</li> </ul>"},{"location":"troubleshooting_stuck_pipeline/#manually-resuming-a-stuck-pipeline","title":"Manually resuming a stuck pipeline","text":"<p>In the UI copy/paste the run_id that is stuck, eg: <code>20250209_030626</code></p> <p>Insert it on commandline, and run the pipeline, like this:</p> <pre><code>PROMPT&gt; RUN_ID=20250209_030626 python -m worker_plan_internal.plan.run_plan_pipeline\n</code></pre>"},{"location":"troubleshooting_stuck_pipeline/#why-does-the-pipeline-get-stuck","title":"Why does the pipeline get stuck?","text":"<p>The <code>log.txt</code> contains the output from the logger with <code>DEBUG</code> level, the most detailed. Alas the <code>log.txt</code> have little info about what exactly went wrong.  The exceptions rarely have useful info.</p> <ul> <li>Censorship, if it's a sensitive topic, then the LLM may refuse to answer.</li> <li>Timeout, that happens often when using AI providers in the cloud.</li> <li>Invalid json, responds from the server that doesn't adhere to the json schema. Too high a temperature setting may cause the LLM to be too creative and diverge from the json schema. Try use a lower temperature.</li> <li>Too long answer, if the respond from the server gets too long so it gets truncated, so it's invalid json.</li> <li>Other, there may be other reasons that I'm not aware of, please let me know if you encounter such a scenario.</li> </ul>"},{"location":"user_accounts_and_billing/","title":"User accounts and billing (database)","text":"<p>These tables support OAuth logins, API keys, and credit\u2011based billing.</p>"},{"location":"user_accounts_and_billing/#useraccount","title":"UserAccount","text":"<p>Represents a user in PlanExe.</p> <p>Fields: - <code>id</code> (UUID, primary key) - <code>email</code>, <code>name</code>, <code>given_name</code>, <code>family_name</code> - <code>locale</code>, <code>avatar_url</code> - <code>is_admin</code> (bool) - <code>free_plan_used</code> (bool) - <code>credits_balance</code> (int) - <code>last_login_at</code>, <code>created_at</code>, <code>updated_at</code></p>"},{"location":"user_accounts_and_billing/#userprovider","title":"UserProvider","text":"<p>Links a user to an OAuth provider identity.</p> <p>Fields: - <code>id</code> (UUID, primary key) - <code>user_id</code> (UUID, foreign key to UserAccount) - <code>provider</code> (string, e.g. google/github/discord) - <code>provider_user_id</code> (string) - <code>email</code> (string) - <code>raw_profile</code> (JSON) - <code>created_at</code>, <code>last_login_at</code></p>"},{"location":"user_accounts_and_billing/#userapikey","title":"UserApiKey","text":"<p>API key record for MCP usage.</p> <p>Fields: - <code>id</code> (UUID, primary key) - <code>user_id</code> (UUID, foreign key) - <code>key_hash</code> (sha256 hash) - <code>key_prefix</code> (short prefix for display) - <code>created_at</code>, <code>last_used_at</code>, <code>revoked_at</code></p> <p>Notes: - Only the hash is stored. The full key is shown once at creation.</p>"},{"location":"user_accounts_and_billing/#credithistory","title":"CreditHistory","text":"<p>Append\u2011only ledger of credit changes.</p> <p>Fields: - <code>id</code> (UUID, primary key) - <code>user_id</code> (UUID, foreign key) - <code>delta</code> (int, positive or negative) - <code>reason</code> (string) - <code>source</code> (string, e.g. stripe/telegram/mcp/web) - <code>external_id</code> (string) - <code>created_at</code></p>"},{"location":"user_accounts_and_billing/#paymentrecord","title":"PaymentRecord","text":"<p>Stores completed payment details.</p> <p>Fields: - <code>id</code> (UUID, primary key) - <code>user_id</code> (UUID, foreign key) - <code>provider</code> (string, stripe/telegram) - <code>provider_payment_id</code> (string) - <code>credits</code> (int) - <code>amount</code> (int, minor units) - <code>currency</code> (string) - <code>status</code> (string) - <code>raw_payload</code> (JSON) - <code>created_at</code></p>"},{"location":"user_accounts_and_billing/#payment-and-refund-flows","title":"Payment and refund flows","text":""},{"location":"user_accounts_and_billing/#buy-credits-stripe","title":"Buy credits (Stripe)","text":"<ol> <li>User opens Account and chooses credits.</li> <li>Stripe Checkout is created.</li> <li>Stripe sends <code>checkout.session.completed</code> webhook.</li> <li>App creates a <code>PaymentRecord</code> and a CreditHistory entry (+credits).</li> </ol>"},{"location":"user_accounts_and_billing/#buy-credits-telegram-stars","title":"Buy credits (Telegram Stars)","text":"<ol> <li>User opens Account and chooses credits.</li> <li>App creates an invoice link via Telegram.</li> <li>Telegram sends <code>successful_payment</code> webhook.</li> <li>App creates a <code>PaymentRecord</code> and a CreditHistory entry (+credits).</li> </ol>"},{"location":"user_accounts_and_billing/#spend-credits-create-a-plan","title":"Spend credits (create a plan)","text":"<ol> <li>User submits a plan.</li> <li>App deducts 1 credit.</li> <li>A CreditHistory entry is created (delta = -1, reason = plan_created).</li> </ol>"},{"location":"user_accounts_and_billing/#close-account-user-wants-money-back","title":"Close account (user wants money back)","text":"<p>Typical approach: - If credits are unused, issue a refund in Stripe/Telegram. - Add a CreditHistory entry to remove credits (negative delta) or to zero the balance. - Keep the ledger history intact (do not delete rows).</p>"},{"location":"user_accounts_and_billing/#refund-correction","title":"Refund / correction","text":"<p>If something went wrong: - Process the refund with the payment provider (Stripe/Telegram). This is the only step that moves real money. - Add a CreditHistory entry that reverses the original credit grant. This only changes internal credits. - Optionally update <code>PaymentRecord.status</code> (e.g., refunded).</p>"},{"location":"ai_providers/lm_studio/","title":"Using PlanExe with LM Studio","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>LM Studio is an open source app for macOS, Windows, and Linux for running LLMs on your own computer. It is useful for local troubleshooting.</p> <p>PlanExe processes more text than regular chat. You will need capable hardware to run an LLM at a reasonable speed.</p>"},{"location":"ai_providers/lm_studio/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install LM Studio on your host and download a small model inside LM Studio (e.g. <code>Qwen2.5-7B-Instruct-1M</code>, ~4.5 GB).</li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for LM Studio) and use the <code>lmstudio-...</code> entry in <code>llm_config.json</code>, setting <code>base_url</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Linux bridge IP.</li> <li>Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</li> </ol>"},{"location":"ai_providers/lm_studio/#host-only-no-docker","title":"Host-only (no Docker)","text":"<p>For advanced users: use the host entry (e.g. <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://127.0.0.1:1234</code>. Start your preferred PlanExe runner (e.g. a local Python environment) and ensure the LM Studio server is running before you submit jobs.</p>"},{"location":"ai_providers/lm_studio/#configuration","title":"Configuration","text":"<p>In <code>llm_config.json</code>, find a config that starts with <code>lmstudio-</code> (e.g. <code>\"lmstudio-qwen2.5-7b-instruct-1m\"</code>). In LM Studio, find the model with that exact id and download it. The Qwen model is on Hugging Face (~4.5 GB).</p> <p>In LM Studio, go to the Developer page (Cmd+2 / Ctrl+2 / Windows+2), start the server, and confirm the UI shows Status: Running and Reachable at: http://127.0.0.1:1234.</p>"},{"location":"ai_providers/lm_studio/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with a ~7B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>Structured output matters: not all models return clean structured output. If you see malformed or JSON errors, try a nearby model or quantization.</li> </ul>"},{"location":"ai_providers/lm_studio/#run-lm-studio-locally-with-docker","title":"Run LM Studio locally with Docker","text":"<p>Containers cannot reach <code>127.0.0.1</code> on your host. Set <code>base_url</code> in <code>llm_config.json</code> to <code>http://host.docker.internal:1234</code> (Docker Desktop) or your Docker bridge IP on Linux (often <code>http://172.17.0.1:1234</code>). On Linux, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code> if that hostname is missing.</p> <p>To find your bridge IP on Linux:</p> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <p>If <code>docker0</code> is missing (e.g. with Podman or alternate bridge names), inspect the default bridge gateway:</p> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <p>Example <code>llm_config.json</code> entry (add <code>base_url</code> when using Docker):</p> <pre><code>\"lmstudio-qwen2.5-7b-instruct-1m\": {\n    \"comment\": \"Runs via LM Studio on the host; PlanExe in Docker points to the host LM Studio server.\",\n    \"class\": \"LMStudio\",\n    \"arguments\": {\n        \"model_name\": \"qwen2.5-7b-instruct-1m\",\n        \"base_url\": \"http://host.docker.internal:1234/v1\",\n        \"temperature\": 0.2,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <p>After editing <code>llm_config.json</code>, rebuild or restart the worker and frontends: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config).</p>"},{"location":"ai_providers/lm_studio/#troubleshooting","title":"Troubleshooting","text":"<p>When you click Submit in PlanExe, a new output directory is created containing <code>log.txt</code>. Open that file and scroll to the bottom for error messages.</p> <p>Report issues on Discord. Include system info (e.g. \u201cI\u2019m on macOS with M1 Max, 64 GB\u201d).</p> <p>Where to look for logs:</p> <ul> <li>Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container).</li> <li>Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to LM Studio).</li> <li>Structured-output failures: If you see JSON/parse errors or malformed output in <code>log.txt</code>, try a different model or quantization; not all models return structured output cleanly.</li> </ul>"},{"location":"ai_providers/lm_studio/#run-lm-studio-on-a-remote-computer","title":"Run LM Studio on a remote computer","text":"<p>Use a secure tunnel instead of exposing the server directly. From your local machine:</p> <pre><code>ssh -N -L 1234:localhost:1234 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:1234</code> while the tunnel is running.</p>"},{"location":"ai_providers/lm_studio/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/mistral/","title":"Using PlanExe with Mistral","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>If you want to use Mistral, OpenRouter has several mistral models. </p>"},{"location":"ai_providers/mistral/#docker-setup-for-mistral","title":"Docker setup for Mistral","text":"<p>Mistral support is not baked into the Docker image by default. You must add the Mistral LlamaIndex extension to the worker, rebuild the image, and supply your API key.</p> <ol> <li>Install Docker (with Docker Compose), then clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre></li> <li>Enable the Mistral client inside the worker image by editing <code>worker_plan/pyproject.toml</code>. Under <code>[project].dependencies</code>, add or uncomment these lines: <pre><code>\"llama-index-llms-mistralai==0.4.0\",\n\"mistralai==1.5.2\",\n</code></pre>    Without this step, the Docker image will not have the <code>MistralAI</code> class.</li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> and add your key: <pre><code>MISTRAL_API_KEY='INSERT-YOUR-SECRET-KEY-HERE'\n</code></pre></li> <li>Add (or keep) a Mistral entry in <code>llm_config.json</code> (example below).</li> <li>Rebuild the images so the new dependencies are baked in: <pre><code>docker compose build --no-cache worker_plan frontend_single_user\n</code></pre></li> <li>Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre> 7) Open http://localhost:7860, go to Settings, and pick your Mistral model (e.g., <code>mistral-paid-large</code>). If you later tweak only <code>llm_config.json</code>, just restart the containers (<code>docker compose restart worker_plan frontend_single_user</code>); rebuilds are only needed when dependencies change.</li> </ol>"},{"location":"ai_providers/mistral/#why-use-mistral","title":"Why use Mistral?","text":"<p>Mistral can have run your own fine tuned model in the cloud. If you have sensitive business data that you don't want to share with the world, then this is one way to do it.</p> <p>Create an account on the mistral.ai website and buy 10 EUR of credits.</p> <p>List of available models.</p> <p>Using the free models, and the API is rate limited to 1 request per second. PlanExe cannot deal with rate limiting and PlanExe does 70-100 requests, so it's likely going to yield errors.</p>"},{"location":"ai_providers/mistral/#create-api-key","title":"Create API key","text":"<ol> <li>Visit api-keys.</li> <li>Click <code>Create new key</code> and name the new key <code>PlanExe</code>.</li> <li>In the <code>.env</code> file in the root dir of the PlanExe repo, create a row named <code>MISTRAL_API_KEY</code>. Copy/paste the newly created api key into that row.</li> </ol> <p>The <code>.env</code> file should look something like the following, with your own key inserted. <pre><code>MISTRAL_API_KEY='AWkg3SxFTLWaPJClbASfv9h3VPItroof'\n</code></pre></p>"},{"location":"ai_providers/mistral/#edit-the-llm_configjson","title":"Edit the <code>llm_config.json</code>","text":"<p>The JSON should look something like this:</p> <pre><code>{\n    \"mistral-paid-large\": {\n        \"comment\": \"This is paid. Possible free to use for a limited time. Check the pricing before use.\",\n        \"class\": \"MistralAI\",\n        \"arguments\": {\n            \"model\": \"mistral-large-latest\",\n            \"api_key\": \"${MISTRAL_API_KEY}\",\n            \"temperature\": 1.0,\n            \"timeout\": 60.0,\n            \"max_tokens\": 8192,\n            \"max_retries\": 5\n        }\n    }\n}\n</code></pre>"},{"location":"ai_providers/mistral/#use-the-mistral-model","title":"Use the Mistral model","text":"<ol> <li>Restart PlanExe</li> <li>Go to the <code>Settings</code> tab</li> <li>Select the <code>mistral-paid-large</code> model.</li> </ol>"},{"location":"ai_providers/mistral/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/ollama/","title":"Using PlanExe with Ollama","text":"<p>This is for advanced users that already have PlanExe working. If you're new to PlanExe, start with OpenRouter instead.</p> <p>Ollama is an open source app for macOS/Windows/Linux for running LLMs on your own computer (or on a remote computer).</p> <p>PlanExe processes more text than regular chat. You will need expensive hardware to run a LLM at a reasonable speed.</p>"},{"location":"ai_providers/ollama/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install Ollama on your host and pull a small model: <code>ollama run llama3.1</code> (downloads ~4.9 GB and proves the host service works).  </li> <li>Copy <code>.env.docker-example</code> to <code>.env</code> (even if you leave keys empty for Ollama) and pick the Docker entry in <code>llm_config.json</code> (snippet below) so <code>base_url</code> points to <code>http://host.docker.internal:11434</code> (Docker Desktop) or your Linux bridge IP.  </li> <li>Start PlanExe: <code>docker compose up worker_plan frontend_single_user</code>. Open http://localhost:7860, submit a prompt, and watch <code>docker compose logs -f worker_plan</code> for progress.</li> </ol>"},{"location":"ai_providers/ollama/#host-only-no-docker-for-advanced-users","title":"Host-only (no Docker) \u2014 for advanced users","text":"<ul> <li>Use the host entry (e.g., <code>\"ollama-llama3.1\"</code>) in <code>llm_config.json</code> so <code>base_url</code> stays on <code>http://localhost:11434</code>.</li> <li>Start your preferred PlanExe runner (e.g., a local Python environment) and ensure Ollama is already running on the host before you submit jobs.</li> </ul>"},{"location":"ai_providers/ollama/#configuration","title":"Configuration","text":"<p>In the <code>llm_config.json</code> find a config that starts with <code>ollama-</code> such as <code>\"ollama-llama3.1\"</code> (host) or <code>\"docker-ollama-llama3.1\"</code> (Docker). Use the <code>docker-</code> entry when PlanExe runs in Docker so requests reach the host.</p> <p>On the Ollama Search Models website. Find the corresponding model. Go to the info page for the model: ollama/library/llama3.1. The info page shows how to install the model on your computer, in this case <code>ollama run llama3.1</code>. To get started, go for a <code>8b</code> model that is <code>4.9GB</code>.</p>"},{"location":"ai_providers/ollama/#minimum-viable-setup","title":"Minimum viable setup","text":"<ul> <li>Start with an 8B model (\u22485 GB download). Expect workable speeds on a 16 GB RAM laptop or a GPU with \u22658 GB VRAM; larger models slow sharply without more hardware.</li> <li>If you need faster responses, move to a bigger GPU box or use a cloud model via OpenRouter instead of upsizing Ollama locally.</li> </ul>"},{"location":"ai_providers/ollama/#run-ollama-locally-with-docker","title":"Run Ollama locally with Docker","text":"<ul> <li>Make sure the container can reach Ollama on the host. On macOS/Windows (Docker Desktop) use the preconfigured entry in <code>llm_config.json</code> (snippet below) with <code>base_url</code> pointing to <code>http://host.docker.internal:11434</code>. On Linux, use your Docker bridge IP (often <code>http://172.17.0.1:11434</code>) and, if needed, add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> under <code>worker_plan</code> in <code>docker-compose.yml</code>.</li> <li>Find your bridge IP on Linux:</li> </ul> <pre><code>ip addr show docker0 | awk '/inet /{print $2}'\n</code></pre> <ul> <li>If <code>docker0</code> is missing (alternate bridge names, Podman, etc.), inspect the default bridge gateway instead:</li> </ul> <pre><code>docker network inspect bridge | awk -F'\"' '/Gateway/{print $4}'\n</code></pre> <ul> <li>Example <code>llm_config.json</code> entry:</li> </ul> <pre><code>\"docker-ollama-llama3.1\": {\n    \"comment\": \"This runs on your own computer. It's free. Requires Ollama to be installed. PlanExe runs in a Docker container, and ollama is installed on the host the computer.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://host.docker.internal:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre> <ul> <li>Restart or rebuild the worker/frontends after updating <code>llm_config.json</code>: <code>docker compose up worker_plan frontend_single_user</code> (add <code>--build</code> or run <code>docker compose build worker_plan frontend_single_user</code> if the image needs the new config baked in).</li> </ul>"},{"location":"ai_providers/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>Use the command line to compare Ollama's list of installed models with the configurations in your <code>llm_config.json</code> file. Run:</p> <pre><code>PROMPT&gt; ollama list\nNAME                                             ID              SIZE      MODIFIED       \nhf.co/unsloth/Llama-3.1-Tulu-3-8B-GGUF:Q4_K_M    08fe35cc5878    4.9 GB    19 minutes ago    \nphi4:latest                                      ac896e5b8b34    9.1 GB    6 weeks ago       \nqwen2.5-coder:latest                             2b0496514337    4.7 GB    2 months ago      \nllama3.1:latest                                  42182419e950    4.7 GB    5 months ago      \n</code></pre> <p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p> <p>Where to look for logs: - Host filesystem: <code>run/&lt;timestamped-output-dir&gt;/log.txt</code> (mounted from the container). - Container logs: <code>docker compose logs -f worker_plan</code> (watch for connection errors to Ollama). - Structured-output failures: if you see JSON/parse errors or malformed outputs in <code>log.txt</code>, try a different Ollama model or quantization; not all models return structured output cleanly.</p>"},{"location":"ai_providers/ollama/#how-to-add-a-new-ollama-model-to-llm_configjson","title":"How to add a new Ollama model to <code>llm_config.json</code>","text":"<p>You can find models and installation instructions here: - Ollama \u2013 Overview of popular models, curated by the Ollama team. - Hugging Face \u2013 A vast collection of GGUF models.</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output. Not every model does this reliably; you may need to try a few nearby models (or quantizations) before finding one that cleanly returns the structured responses PlanExe expects.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Follow the instructions on Ollama or Hugging Face to install the model.</li> <li>Copy the model id from the <code>ollama list</code> command, such as <code>llama3.1:latest</code></li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"ai_providers/ollama/#run-ollama-on-a-remote-computer","title":"Run Ollama on a remote computer","text":"<p>In <code>llm_config.json</code>, insert <code>base_url</code> with the url to run on. Prefer a secure tunnel (example below) or a firewall-restricted host\u2014avoid exposing Ollama publicly.</p> <p>SSH tunnel example from your local machine:</p> <pre><code>ssh -N -L 11434:localhost:11434 user@remote-host\n</code></pre> <p>Then set <code>base_url</code> to <code>http://localhost:11434</code> while the tunnel is running.</p> <pre><code>\"ollama-llama3.1\": {\n    \"comment\": \"This runs on on a remote computer. Requires Ollama to be installed.\",\n    \"class\": \"Ollama\",\n    \"arguments\": {\n        \"model\": \"llama3.1:latest\",\n        \"base_url\": \"http://example.com:11434\",\n        \"temperature\": 0.5,\n        \"request_timeout\": 120.0,\n        \"is_function_calling_model\": false\n    }\n}\n</code></pre>"},{"location":"ai_providers/ollama/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"ai_providers/openrouter/","title":"Using PlanExe with OpenRouter","text":"<p>For new users, OpenRouter is the recommended starting point. When you have have generated a few plans via OpenRouter, then you can try switch to other AI providers.</p> <p>OpenRouter provides access to a large number of LLM models, that runs in the cloud.</p> <p>Unfortunately there is no <code>free</code> model that works reliable with PlanExe.</p> <p>In my experience, the <code>paid</code> models are the most reliable. Models like google/gemini-2.0-flash-001. and openai/gpt-4o-mini are cheap and faster than running models on my own computer and without risk of it overheating.</p> <p>I haven't been able to find a <code>free</code> model on OpenRouter that works well with PlanExe.</p> <p>Avoid pricey <code>paid</code> models. PlanExe does more than 100 LLM inference calls per plan, so each run uses many tokens. With a cheap model, creating a full plan costs less than 0.30 USD; with one of the newest models, the price can exceed 20 USD. To keep PlanExe affordable for as many users as possible, the defaults use older, cheaper models.</p>"},{"location":"ai_providers/openrouter/#quickstart-docker","title":"Quickstart (Docker)","text":"<ol> <li>Install Docker (with Docker Compose) \u2014 no local Python or pip is needed now.</li> <li>Clone the repo and enter it: <pre><code>git clone https://github.com/PlanExeOrg/PlanExe.git\ncd PlanExe\n</code></pre></li> <li>Copy <code>.env.docker-example</code> to <code>.env</code>, then set your API key and pick a default OpenRouter profile so the worker uses the cloud model by default: <pre><code>OPENROUTER_API_KEY='sk-or-v1-...'\nDEFAULT_LLM='openrouter-paid-gemini-2.0-flash-001'   # or openrouter-paid-openai-gpt-4o-mini\n</code></pre>    The containers mount <code>.env</code> and <code>llm_config.json</code> automatically.</li> <li>Start PlanExe: <pre><code>docker compose up worker_plan frontend_single_user\n</code></pre></li> <li>Wait for http://localhost:7860 to come up, submit a prompt, and watch progress with <code>docker compose logs -f worker_plan</code>.</li> <li>Outputs are written to <code>run/&lt;timestamped-output-dir&gt;</code> on the host (mounted from the containers).</li> <li>Stop with <code>Ctrl+C</code> (or <code>docker compose down</code>). If you change <code>llm_config.json</code>, restart the containers so they reload it: <code>docker compose restart worker_plan frontend_single_user</code> (or <code>docker compose down &amp;&amp; docker compose up</code>). No rebuild is needed for config-only edits.</li> </ol>"},{"location":"ai_providers/openrouter/#configuration","title":"Configuration","text":"<p>Visit OpenRouter, create an account, purchase 5 USD in credits (plenty for making a several plans), and generate an API key.</p> <p>Copy <code>.env.docker-example</code> to a new file called <code>.env</code> (loaded by Docker at startup).</p> <p>Open the <code>.env</code> file in a text editor and insert your OpenRouter API key. Like this:</p> <pre><code>OPENROUTER_API_KEY='INSERT YOUR KEY HERE'\n</code></pre> <p>If you edit <code>llm_config.json</code> later, restart the worker/frontend containers to pick up the changes: <code>docker compose restart worker_plan frontend_single_user</code> (or stop/start). Rebuilds are only needed when dependencies change.</p>"},{"location":"ai_providers/openrouter/#troubleshooting","title":"Troubleshooting","text":"<p>Inside PlanExe, when clicking <code>Submit</code>, a new <code>Output Dir</code> should be created containing a <code>log.txt</code>. Open that file and scroll to the bottom, see if there are any error messages about what is wrong.</p> <p>When running in Docker, also check the worker logs for 401/429 or connectivity errors:</p> <pre><code>docker compose logs -f worker_plan\n</code></pre> <p>Report your issue on Discord. Please include info about your system, such as: \"I'm on macOS with M1 Max with 64 GB.\".</p>"},{"location":"ai_providers/openrouter/#how-to-add-a-new-openrouter-model-to-llm_configjson","title":"How to add a new OpenRouter model to <code>llm_config.json</code>","text":"<p>The OpenRouter/rankings page shows an overview of the most popular models. New models are added frequently</p> <p>For a model to work with PlanExe, it must meet the following criteria:</p> <ul> <li>Minimum 8192 output tokens.</li> <li>Support structured output.</li> <li>Reliable. Avoid fragile setups where it works one day, but not the next day. If it's a beta version, be aware that it may stop working.</li> <li>Low latency.</li> </ul> <p>Steps to add a model:</p> <ol> <li>Copy the model id from the openrouter website.</li> <li>Paste the model id into the <code>llm_config.json</code>.</li> <li>Restart PlanExe to apply the changes.</li> </ol>"},{"location":"ai_providers/openrouter/#next-steps","title":"Next steps","text":"<ul> <li>Learn prompt quality: Prompt writing guide</li> <li>Understand output sections: Plan output anatomy</li> </ul>"},{"location":"developer/database_postgres/","title":"Database Postgres","text":"<p>Database container for PlanExe. Used as a queue mechanism for planning tasks. The <code>worker_plan_database</code> listens for an incoming task, and runs PlanExe and then goes back to listen for more incoming tasks.</p> <p>In a single user environment, then this is overkill. The file system is sufficient.</p> <p>In a multi user environment, then there are many moving parts, and here a database is relevant.</p> <ul> <li>Build/run via <code>docker compose up database_postgres</code> (or <code>docker compose build database_postgres</code>).</li> <li>Defaults: <code>PLANEXE_POSTGRES_USER=planexe</code>, <code>PLANEXE_POSTGRES_PASSWORD=planexe</code>, <code>PLANEXE_POSTGRES_DB=planexe</code> (override with env or <code>.env</code>).</li> <li>Ports: <code>${PLANEXE_POSTGRES_PORT:-5432}</code> on the host mapped to <code>5432</code> in the container. Set <code>PLANEXE_POSTGRES_PORT</code> in <code>.env</code> or your shell to avoid clashes.</li> <li>Data: persisted in the named volume <code>database_postgres_data</code>.</li> </ul>"},{"location":"developer/database_postgres/#choose-a-host-port","title":"Choose a host port","text":"<p>The default PostgreSQL port is 5432. On developer machines, this port is often already occupied by a local PostgreSQL installation:</p> <ul> <li>macOS: Postgres.app (a popular menu-bar Postgres that auto-starts), Homebrew PostgreSQL (<code>brew install postgresql</code>), or pgAdmin's bundled server</li> <li>Linux: System PostgreSQL installed via <code>apt install postgresql</code>, <code>dnf install postgresql-server</code>, etc.</li> <li>Windows: PostgreSQL installer, pgAdmin, or other database tools</li> </ul> <p>If port 5432 is in use, Docker will fail to start <code>database_postgres</code> with a \"port already in use\" error.</p> <p>Solution: Set <code>PLANEXE_POSTGRES_PORT</code> to a different value before starting the container:</p> <pre><code>export PLANEXE_POSTGRES_PORT=5433\ndocker compose up database_postgres\n</code></pre> <p>Or add it to your <code>.env</code> file to make it permanent: <pre><code>PLANEXE_POSTGRES_PORT=5433\n</code></pre></p> <p>Replace <code>5433</code> with any free host port you prefer.</p> <p>Important: This only affects the HOST port mapping (how you access Postgres from your machine). Inside Docker, containers always communicate with each other on the internal port 5432\u2014this is hardcoded and not affected by <code>PLANEXE_POSTGRES_PORT</code>.</p>"},{"location":"developer/database_postgres/#verify-the-container","title":"Verify the container","text":"<ul> <li>Check status: <code>docker compose ps database_postgres</code></li> <li>Shell in to confirm Postgres is the right one: <code>docker compose exec database_postgres psql -U planexe -d planexe</code></li> </ul>"},{"location":"developer/database_postgres/#dbeaver","title":"DBeaver","text":"<p>For managing the database, I recommend using the <code>DBeaver Community</code> app, which is open source.</p> <p>https://github.com/dbeaver/dbeaver</p> <p>Connect with host <code>localhost</code>, port <code>${PLANEXE_POSTGRES_PORT:-5432}</code>, database <code>planexe</code>, user <code>planexe</code>, password <code>planexe</code> (or whatever you set in <code>.env</code>).</p>"},{"location":"developer/database_postgres/#railway-dbeaver","title":"Railway + DBeaver","text":"<p>DBeaver cannot connect via the Railway CLI tunnel (<code>railway ssh</code>/<code>connect</code>), because the CLI does not provide a traditional TCP port forward. Instead, use Railway's TCP Proxy feature.</p>"},{"location":"developer/database_postgres/#1-enable-tcp-proxy-in-railway","title":"1. Enable TCP Proxy in Railway","text":"<ol> <li>Go to your Railway dashboard \u2192 <code>database_postgres</code> service</li> <li>Navigate to Settings \u2192 Networking \u2192 Public Networking</li> <li>Add a TCP Proxy with port <code>5432</code></li> <li>Railway will assign a hostname and port, e.g., <code>subsubdomain.subdomain.example.com:12345</code></li> </ol> <p>Warning: Only enable TCP Proxy after setting a secure password (see below).</p> <p>Warning: The TCP Proxy connection is unencrypted. Railway's TCP Proxy forwards raw TCP traffic without adding TLS, and the <code>postgres:16-alpine</code> image doesn't have SSL enabled by default. Your password and data travel in plain text. Consider disabling TCP Proxy when not in use, or configure SSL on the PostgreSQL container for production use.</p>"},{"location":"developer/database_postgres/#2-set-a-secure-password","title":"2. Set a secure password","text":"<p>The default password <code>planexe</code> is too easy to guess. PostgreSQL only sets the password on first initialization, so if the database already exists:</p> <ol> <li>Connect with the current password</li> <li>Run: <code>ALTER USER planexe WITH PASSWORD 'your-secure-password';</code></li> <li>Update <code>POSTGRES_PASSWORD</code> in Railway's environment variables to match</li> </ol>"},{"location":"developer/database_postgres/#3-connect-with-dbeaver","title":"3. Connect with DBeaver","text":"<p>In DBeaver, create a new PostgreSQL connection with \"Connect by: Host\":</p> Field Value Host Your TCP Proxy hostname (e.g., <code>subsubdomain.subdomain.example.com</code>) Port Your assigned port (e.g., <code>12345</code>, NOT 5432) Database <code>planexe</code> Username <code>planexe</code> Password Your secure password <p>Click Test Connection to verify.</p>"},{"location":"developer/database_postgres/#4-security-check","title":"4. Security check","text":"<p>Try connecting with password <code>planexe</code>. If it succeeds, the password hasn't been changed yet\u2014go back to step 2.</p> <p>See <code>railway.md</code> for more details.</p>"},{"location":"developer/database_postgres/#ssl-future-plan","title":"SSL (Future Plan)","text":"<p>The current setup uses unencrypted connections. For production use with public TCP Proxy exposure, SSL/TLS should be enabled to encrypt traffic between clients and the database.</p>"},{"location":"developer/database_postgres/#whats-needed","title":"What's needed","text":""},{"location":"developer/database_postgres/#1-generate-ssl-certificates","title":"1. Generate SSL certificates","text":"<p>You'll need a certificate and private key. Options: - Self-signed: Quick for internal use, but clients must trust the certificate manually - Let's Encrypt: Free, but requires domain validation (complex for raw TCP) - Commercial CA: Trusted by default, but costs money</p> <p>Example self-signed certificate generation:</p> <pre><code>openssl req -new -x509 -days 365 -nodes \\\n  -out server.crt \\\n  -keyout server.key \\\n  -subj \"/CN=database_postgres\"\n</code></pre>"},{"location":"developer/database_postgres/#2-update-the-dockerfile","title":"2. Update the Dockerfile","text":"<p>Add the certificates and configure PostgreSQL to use them:</p> <pre><code>FROM postgres:16-alpine\n\n# ... existing ENV statements ...\n\n# Copy SSL certificates\nCOPY server.crt /var/lib/postgresql/server.crt\nCOPY server.key /var/lib/postgresql/server.key\n\n# Set correct permissions (required by PostgreSQL)\nRUN chmod 600 /var/lib/postgresql/server.key &amp;&amp; \\\n    chown postgres:postgres /var/lib/postgresql/server.crt /var/lib/postgresql/server.key\n\n# Enable SSL in PostgreSQL\nRUN echo \"ssl = on\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample &amp;&amp; \\\n    echo \"ssl_cert_file = '/var/lib/postgresql/server.crt'\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample &amp;&amp; \\\n    echo \"ssl_key_file = '/var/lib/postgresql/server.key'\" &gt;&gt; /usr/local/share/postgresql/postgresql.conf.sample\n</code></pre>"},{"location":"developer/database_postgres/#3-configure-dbeaver-for-ssl","title":"3. Configure DBeaver for SSL","text":"<p>In DBeaver's connection settings:</p> <ol> <li>Go to the SSL tab</li> <li>Check \"Use SSL\"</li> <li>Set SSL mode:</li> <li><code>require</code> \u2014 Encrypt connection, don't verify certificate</li> <li><code>verify-ca</code> \u2014 Encrypt and verify certificate against a CA</li> <li><code>verify-full</code> \u2014 Encrypt, verify certificate, and check hostname</li> <li>For self-signed certs, you may need to import the CA/certificate or set \"Trust all certificates\"</li> </ol>"},{"location":"developer/database_postgres/#4-enforce-ssl-on-the-server-optional","title":"4. Enforce SSL on the server (optional)","text":"<p>To reject unencrypted connections, add to <code>pg_hba.conf</code>:</p> <pre><code># Require SSL for all remote connections\nhostssl all all 0.0.0.0/0 scram-sha-256\n</code></pre>"},{"location":"developer/database_postgres/#resources","title":"Resources","text":"<ul> <li>PostgreSQL SSL Documentation</li> <li>pg_hba.conf Documentation</li> </ul>"},{"location":"developer/database_postgres/#railway-backup-to-local-file","title":"Railway backup to local file","text":"<p>Use <code>database_postgres/download_backup.py</code> to stream a compressed dump from the Railway <code>database_postgres</code> service to your machine.</p> <p>Prereq: Railway CLI installed and logged in.</p> <pre><code>python database_postgres/download_backup.py\n</code></pre> <ul> <li>Runs <code>railway link</code> (skip with <code>--skip-link</code> if already linked).</li> <li>Streams <code>pg_dump -F c -Z9</code> via <code>railway ssh</code> and writes <code>YYYYMMDD-HHMM.dump</code> in the current directory.</li> <li>Options:</li> <li><code>--user</code> Postgres user (default: <code>$PLANEXE_POSTGRES_USER</code> or <code>planexe</code>)</li> <li><code>--db</code> Postgres database (default: <code>$PLANEXE_POSTGRES_DB</code> or <code>planexe</code>)</li> <li><code>--output-dir path</code> Directory for the dump file</li> <li><code>--filename name.dump</code> Override dump filename</li> <li><code>--service other_service</code> Railway service name</li> <li><code>--skip-link</code> Skip <code>railway link</code> if already linked</li> </ul>"},{"location":"developer/database_postgres/#restore-a-backup-locally","title":"Restore a backup locally","text":"<p>Run a Postgres you can reach (for example <code>docker compose up database_postgres</code> on your machine), then restore the custom-format dump:</p> <pre><code>PGPASSWORD=planexe pg_restore \\\n  -h localhost \\\n  -p 5432 \\\n  -U planexe \\\n  -d planexe \\\n  /path/to/19841231-2359.dump\n</code></pre> <ul> <li>The dump is custom format (<code>pg_dump -F c</code>), so use <code>pg_restore</code>, not <code>psql</code>.</li> <li>Ensure the target database exists; add <code>-c</code> to drop objects before recreating them if you want a clean restore.</li> <li>If you changed credentials/DB name in <code>.env</code> or Railway, use those here.</li> </ul>"},{"location":"developer/frontend_multi_user/","title":"Frontend multi user - Experimental","text":"<p>My recommendation: Avoid this, instead go with <code>frontend_single_user</code>. This multi user UI is the bare minimum, unpolished. It has a queue mechanism, admin UI, but it has no user account management. I use it for handling multiple users. It requires lots of setup to get working. It's not something that simply works out of the box. Save yourself the trouble, go with <code>frontend_single_user</code> instead.</p> <p>Flask-based multi-user UI for PlanExe. Runs in Docker, uses Postgres (defaults to the <code>database_postgres</code> service), and only needs the lightweight <code>worker_plan_api</code> helpers (no full <code>worker_plan</code> install).</p>"},{"location":"developer/frontend_multi_user/#quickstart-with-docker","title":"Quickstart with Docker","text":"<ul> <li>Ensure <code>.env</code> and <code>llm_config.json</code> exist in the repo root (they are mounted into the container).</li> <li><code>docker compose up frontend_multi_user</code></li> <li>Open http://localhost:${PLANEXE_FRONTEND_MULTIUSER_PORT:-5001}/ (container listens on 5000). Health endpoint: <code>/healthcheck</code>.</li> </ul>"},{"location":"developer/frontend_multi_user/#config-env","title":"Config (env)","text":"<ul> <li><code>PLANEXE_FRONTEND_MULTIUSER_DB_HOST|PORT|NAME|USER|PASSWORD</code>: Postgres target (defaults follow <code>database_postgres</code> / <code>planexe</code> values).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_ADMIN_USERNAME</code> / <code>PLANEXE_FRONTEND_MULTIUSER_ADMIN_PASSWORD</code>: Admin login for the UI; must be set (service fails to start if missing).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_HOST</code>: bind address inside the container (default 0.0.0.0).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_PORT</code>: Flask port inside the container (default 5000).</li> <li><code>PLANEXE_FRONTEND_MULTIUSER_DEBUG</code>: set <code>true</code> to enable Flask debug.</li> <li><code>PLANEXE_CONFIG_PATH</code>: defaults to <code>/app</code> so PlanExe picks up <code>.env</code> + <code>llm_config.json</code> that compose mounts.</li> </ul>"},{"location":"developer/frontend_multi_user/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>frontend_multi_user</code> so its dependencies stay isolated:</p> <pre><code>cd frontend_multi_user\npython3 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e .\nexport PYTHONPATH=$PWD/..:$PWD/../worker_plan:$PYTHONPATH\npython src/app.py\n</code></pre> <p>Run <code>deactivate</code> when you are done with the venv.</p> <p>The <code>PYTHONPATH</code> makes <code>worker_plan_api</code> and <code>database_api</code> importable without installing the full <code>worker_plan</code> package (which has fragile dependencies in <code>worker_plan_internal</code>).</p>"},{"location":"developer/frontend_single_user/","title":"Frontend Single User","text":"<p>This directory contains the PlanExe single-user Gradio frontend.</p>"},{"location":"developer/frontend_single_user/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>frontend_single_user</code> so its dependencies stay isolated (they may be incompatible with <code>worker_plan</code>):</p> <pre><code>cd frontend_single_user\npython3 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -r requirements.txt\nexport PYTHONPATH=$PWD/../worker_plan:$PYTHONPATH\npython app.py\n</code></pre> <p>The app loads environment variables from a <code>.env</code> file (if present). Create one with:</p> <pre><code># .env\nPLANEXE_WORKER_PLAN_URL=http://localhost:8000\nPLANEXE_OPEN_DIR_SERVER_URL=http://localhost:5100\n</code></pre> <p>Then open http://localhost:7860 (or your <code>PLANEXE_GRADIO_SERVER_PORT</code>). Run <code>deactivate</code> when you are done with the venv.</p> <p>If you prefer to install the shared API package instead of using <code>PYTHONPATH</code>, run <code>pip install -e ../worker_plan</code> (this will bring the worker dependencies into the same venv).</p>"},{"location":"developer/frontend_single_user/#environment-variables","title":"Environment variables","text":"Variable Default Purpose <code>PLANEXE_WORKER_PLAN_URL</code> <code>http://worker_plan:8000</code> Base URL for <code>worker_plan</code> service the UI calls. <code>PLANEXE_WORKER_PLAN_TIMEOUT</code> <code>30</code> HTTP timeout (seconds) for <code>worker_plan</code> requests. <code>PLANEXE_GRADIO_SERVER_NAME</code> <code>0.0.0.0</code> Host/interface Gradio binds to. <code>PLANEXE_GRADIO_SERVER_PORT</code> <code>7860</code> Port Gradio listens on. <code>PLANEXE_PASSWORD</code> (unset) Optional password to protect the UI (<code>user</code> / <code>&lt;value&gt;</code>). Leave unset for local development without auth. <code>PLANEXE_OPEN_DIR_SERVER_URL</code> (unset) URL of the host opener service for \u201cOpen Output Dir\u201d; leave unset to hide the button."},{"location":"developer/frontend_single_user/#password","title":"Password","text":"<p>Leave <code>PLANEXE_PASSWORD</code> unset when running PlanExe on your own computer.</p> <p>However when running in the cloud, here you may want password protection.</p> <p>Set <code>PLANEXE_PASSWORD</code> to turn on Gradio\u2019s basic auth. Example:</p> <pre><code>export PLANEXE_PASSWORD=123\ndocker compose up\n</code></pre> <p>Then open the app and log in with username <code>user</code> and password <code>123</code>.</p>"},{"location":"developer/mcp_cloud/","title":"PlanExe MCP Cloud - Experimental, likely to be changed a lot!","text":"<p>Model Context Protocol (MCP) interface for PlanExe. Implements the MCP specification defined in <code>docs/mcp/planexe_mcp_interface.md</code>.</p>"},{"location":"developer/mcp_cloud/#overview","title":"Overview","text":"<p>mcp_cloud provides a standardized MCP interface for PlanExe's plan generation workflows. It connects to <code>worker_plan_database</code> via the shared Postgres database (<code>database_api</code> models).</p>"},{"location":"developer/mcp_cloud/#features","title":"Features","text":"<ul> <li>Task Management: Create and stop plan generation tasks</li> <li>Progress Tracking: Real-time status and progress updates</li> <li>File Metadata: Get report/zip metadata and download URLs</li> </ul>"},{"location":"developer/mcp_cloud/#run-as-task-mcp-tasks-protocol","title":"Run as task (MCP tasks protocol)","text":"<p>MCP has two ways to run long-running work: tools (what we use) and the tasks protocol (\"Run as task\" in some UIs). PlanExe uses tools only: <code>task_create</code>, <code>task_status</code>, <code>task_stop</code>, <code>task_download</code>. The agent creates a task, polls status, then downloads; that is the intended flow per <code>docs/mcp/planexe_mcp_interface.md</code>. We do not advertise or implement the MCP tasks protocol (tasks/get, tasks/result, etc.). Clients like Cursor do not support it properly\u2014use the tools directly.</p>"},{"location":"developer/mcp_cloud/#client-choice-guide","title":"Client Choice Guide","text":"<ul> <li>Use <code>mcp_cloud</code> directly (HTTP): If you are running in the cloud or you do   not need files saved to the local filesystem.</li> <li>Use <code>mcp_local</code> (proxy): Recommended when you want artifacts downloaded to   your local disk (<code>PLANEXE_PATH</code>). The proxy forwards MCP calls to this server   and handles file downloads locally.</li> <li>Recommended flow: Docker (<code>mcp_cloud</code>) \u2192 <code>mcp_local</code> \u2192 MCP client (LM Studio/Claude).</li> </ul>"},{"location":"developer/mcp_cloud/#docker-usage-recommended","title":"Docker Usage (Recommended)","text":"<p>Build and run mcp_cloud with HTTP endpoints:</p> <pre><code>docker compose up --build mcp_cloud\n</code></pre> <p>mcp_cloud exposes HTTP endpoints on port <code>8001</code> (or <code>${PLANEXE_MCP_HTTP_PORT}</code>). Set <code>PLANEXE_MCP_API_KEY</code> in your <code>.env</code> file or environment to enable API key authentication.</p>"},{"location":"developer/mcp_cloud/#connecting-via-httpurl","title":"Connecting via HTTP/URL","text":"<p>After starting with Docker, configure your MCP client (e.g., LM Studio) to connect via HTTP:</p> <p>Local Docker (development):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"http://localhost:8001/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Railway/Cloud deployment:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"https://your-app.up.railway.app/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Alternative header format (also supported):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"url\": \"https://your-app.up.railway.app/mcp\",\n      \"headers\": {\n        \"API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n</code></pre> <p>Set <code>PLANEXE_MCP_API_KEY</code> to the same value you use in <code>Authorization: Bearer &lt;key&gt;</code> or <code>X-API-Key</code>.</p>"},{"location":"developer/mcp_cloud/#available-http-endpoints","title":"Available HTTP Endpoints","text":"<ul> <li><code>POST /mcp</code> - Main MCP JSON-RPC endpoint (Streamable HTTP; may use SSE for streaming)</li> <li><code>GET /mcp/tools</code> - List tools (JSON). No SSE required. Use this if your client reports \"SSE error\" when connecting to <code>/mcp</code>.</li> <li><code>POST /mcp/tools/call</code> - Call a tool (JSON). No SSE required.</li> <li><code>GET /healthcheck</code> - Health check endpoint</li> <li><code>GET /docs</code> - OpenAPI documentation (Swagger UI)</li> </ul>"},{"location":"developer/mcp_cloud/#sse-error-or-no-server-sse-stream-from-the-client","title":"\"SSE error\" or \"no Server-SSE stream\" from the client","text":"<p>Some MCP clients (e.g. OpenClaw/mcporter) connect by doing a GET to the server URL and expect a Server-Sent Events (SSE) stream (<code>Content-Type: text/event-stream</code>). That is the Streamable HTTP transport. This server mounts FastMCP at <code>/mcp</code>; GET /mcp returns a 307 redirect to <code>/mcp/</code>, and the Streamable HTTP handshake may not match what the client expects, so the client reports \"SSE error\" or \"could not fetch \u2026 no SSE stream\".</p> <p>You do not need SSE for tools. MCP over HTTP can use plain JSON:</p> <ul> <li>List tools: <code>GET http://&lt;host&gt;:8001/mcp/tools</code> \u2192 returns <code>{\"tools\": [...]}</code> (JSON).</li> <li>Call a tool: <code>POST http://&lt;host&gt;:8001/mcp/tools/call</code> with body <code>{\"tool\": \"task_create\", \"arguments\": {\"prompt\": \"\u2026\", \"speed_vs_detail\": \"ping\"}}</code> \u2192 returns JSON.</li> </ul> <p>If your client only supports Streamable HTTP and fails on <code>/mcp</code>, you have two options:</p> <ol> <li>Point the client at the JSON API if it allows a separate \"tools list\" URL: use <code>GET /mcp/tools</code> for listing and <code>POST /mcp/tools/call</code> for calls (no SSE).</li> <li>Use baseUrl with trailing slash (e.g. <code>http://192.168.1.10:8001/mcp/</code>) so the client does not follow a redirect; whether that fixes SSE depends on how the client and FastMCP do the Streamable HTTP handshake.</li> </ol>"},{"location":"developer/mcp_cloud/#environment-variables","title":"Environment Variables","text":""},{"location":"developer/mcp_cloud/#http-server-configuration","title":"HTTP Server Configuration","text":"<ul> <li><code>PLANEXE_MCP_API_KEY</code>: Required for production. API key for authentication. Clients can provide <code>Authorization: Bearer &lt;key&gt;</code> or <code>X-API-Key</code>.</li> <li><code>PLANEXE_MCP_HTTP_HOST</code>: HTTP server host (default: <code>127.0.0.1</code>). Use <code>0.0.0.0</code> to bind all interfaces (containers/cloud).</li> <li><code>PLANEXE_MCP_HTTP_PORT</code>: HTTP server port (default: <code>8001</code>). Railway will override with <code>PORT</code> env var.</li> <li><code>PLANEXE_MCP_PUBLIC_BASE_URL</code>: Public base URL for report/zip download links in <code>task_file_info</code> (e.g. <code>http://192.168.1.40:8001</code>). When unset, the HTTP server uses the request\u2019s host (scheme + authority), so clients connecting at <code>http://192.168.1.40:8001/mcp/</code> get download URLs like <code>http://192.168.1.40:8001/download/...</code> instead of localhost. If clients still see localhost in download URLs (e.g. behind a proxy), uncomment and set this in the repo\u2019s <code>.env.docker-example</code> or <code>.env.developer-example</code> (copy to <code>.env</code> and fill in your public URL).</li> <li><code>PORT</code>: Railway-provided port (takes precedence over <code>PLANEXE_MCP_HTTP_PORT</code>)</li> <li><code>PLANEXE_MCP_CORS_ORIGINS</code>: Comma-separated list of allowed origins (default: <code>http://localhost,http://127.0.0.1</code>).</li> <li><code>PLANEXE_MCP_MAX_BODY_BYTES</code>: Max request size for <code>POST /mcp/tools/call</code> (default: <code>1048576</code>).</li> <li><code>PLANEXE_MCP_RATE_LIMIT</code>: Max requests per window for <code>POST /mcp/tools/call</code> (default: <code>60</code>).</li> <li><code>PLANEXE_MCP_RATE_WINDOW_SECONDS</code>: Rate limit window in seconds (default: <code>60</code>).</li> </ul>"},{"location":"developer/mcp_cloud/#database-configuration","title":"Database Configuration","text":"<p>mcp_cloud uses the same database configuration as other PlanExe services:</p> <ul> <li><code>SQLALCHEMY_DATABASE_URI</code>: Full database connection string (takes precedence)</li> <li><code>PLANEXE_POSTGRES_HOST</code>: Database host (default: <code>database_postgres</code>)</li> <li><code>PLANEXE_POSTGRES_PORT</code>: Database port (default: <code>5432</code>)</li> <li><code>PLANEXE_POSTGRES_DB</code>: Database name (default: <code>planexe</code>)</li> <li><code>PLANEXE_POSTGRES_USER</code>: Database user (default: <code>planexe</code>)</li> <li><code>PLANEXE_POSTGRES_PASSWORD</code>: Database password (default: <code>planexe</code>)</li> <li><code>PLANEXE_WORKER_PLAN_URL</code>: URL of the worker_plan HTTP service (default: <code>http://worker_plan:8000</code>)</li> </ul>"},{"location":"developer/mcp_cloud/#mcp-tools","title":"MCP Tools","text":"<p>See <code>docs/mcp/planexe_mcp_interface.md</code> for full specification. Available tools:</p> <ul> <li><code>prompt_examples</code> - Return example prompts. Use these as examples for task_create.</li> <li><code>task_create</code> - Create a new task (returns task_id as UUID; may require user_api_key for credits)</li> <li><code>task_status</code> - Get task status and progress</li> <li><code>task_stop</code> - Stop an active task</li> <li><code>task_file_info</code> - Get file metadata for report or zip</li> </ul> <p>Note: <code>task_download</code> is a synthetic tool provided by <code>mcp_local</code>, not by this server. If your client exposes <code>task_download</code>, use it to save the report or zip locally; otherwise use <code>task_file_info</code> to get <code>download_url</code> and fetch the file yourself.</p> <p>Tip: Call <code>prompt_examples</code> to get example prompts to use with task_create. The catalog is the same as in the frontends (<code>worker_plan.worker_plan_api.PromptCatalog</code>). When running with <code>PYTHONPATH</code> set to the repo root (e.g. stdio setup), the catalog is loaded automatically; otherwise built-in examples are returned.</p> <p>Download flow: call <code>task_file_info</code> to obtain the <code>download_url</code>, then fetch the report via <code>GET /download/{task_id}/030-report.html</code> (API key required if configured).</p>"},{"location":"developer/mcp_cloud/#debugging-with-the-mcp-inspector","title":"Debugging with the MCP Inspector","text":"<p>Use the MCP Inspector to verify tool registration, authentication, and output schemas.</p> <p>Trailing slash required. The server mounts at <code>/mcp</code> which redirects to <code>/mcp/</code>. Always use <code>/mcp/</code> (with trailing slash) in the inspector URL to avoid a 307 redirect that crashes <code>node-fetch</code> in older inspector versions.</p>"},{"location":"developer/mcp_cloud/#local-no-authentication","title":"Local (no authentication)","text":"<pre><code>npx @modelcontextprotocol/inspector --transport http --server-url http://localhost:8001/mcp/\n</code></pre> <p>Steps: - Click \"Connect\" - Click \"Tools\" - Click \"List Tools\"</p>"},{"location":"developer/mcp_cloud/#production-with-api-key-authentication","title":"Production (with API key authentication)","text":"<p>When <code>PLANEXE_MCP_API_KEY</code> is set on the server, the inspector must send the key with every request. The inspector proxy forwards the <code>Authorization</code> header to the remote server.</p> <pre><code>npx @modelcontextprotocol/inspector --transport http --server-url https://mcp.planexe.org/mcp/\n</code></pre> <p>Steps: 1. In the inspector UI, expand \"Authentication\" in the left sidebar 2. Select Bearer Token 3. Paste your API key (e.g. <code>pex_...</code>) 4. Click \"Connect\" 5. Click \"Tools\" then \"List Tools\" to verify</p> <p>The inspector sends <code>Authorization: Bearer &lt;your-key&gt;</code> which the server accepts via <code>_extract_api_key()</code> (same as <code>X-API-Key</code> or <code>API_KEY</code> headers).</p>"},{"location":"developer/mcp_cloud/#skipping-proxy-authentication-development-only","title":"Skipping proxy authentication (development only)","text":"<p>The inspector proxy itself also requires a session token. To disable that during local development:</p> <pre><code>DANGEROUSLY_OMIT_AUTH=true npx @modelcontextprotocol/inspector --transport http --server-url https://mcp.planexe.org/mcp/\n</code></pre> <p>This only disables the local inspector-proxy token check. The remote server still enforces <code>PLANEXE_MCP_API_KEY</code> if configured.</p>"},{"location":"developer/mcp_cloud/#everything-reference-stdio","title":"Everything reference (stdio)","text":"<p>Sanity-check the inspector itself against the reference server:</p> <pre><code>npx @modelcontextprotocol/inspector --transport stdio npx -y @modelcontextprotocol/server-everything\n</code></pre> <p>Steps: - Click \"Connect\" - Click \"Tools\" - Click \"List Tools\"</p>"},{"location":"developer/mcp_cloud/#architecture","title":"Architecture","text":"<p>mcp_cloud maps MCP concepts to PlanExe's database models:</p> <ul> <li>Task \u2192 <code>TaskItem</code> (each task corresponds to a TaskItem)</li> <li>Run \u2192 Execution of a TaskItem by <code>worker_plan_database</code></li> <li>Report \u2192 HTML report fetched from <code>worker_plan</code> via HTTP API</li> </ul> <p>mcp_cloud reads task state and progress from the database, and fetches artifacts from <code>worker_plan</code> via HTTP instead of accessing the run directory directly. This allows mcp_cloud to work without mounting the run directory, making it compatible with Railway and other cloud platforms that don't support shared volumes across services.</p>"},{"location":"developer/mcp_cloud/#connecting-via-stdio-advanced-contributor-mode","title":"Connecting via stdio (Advanced / Contributor Mode)","text":"<p>For local development, you can run mcp_cloud over stdio instead of HTTP. This is useful for testing but requires local Python + Postgres setup. For most users, the recommended flow is Docker (server) + <code>mcp_local</code> (client).</p>"},{"location":"developer/mcp_cloud/#setup","title":"Setup","text":"<ol> <li>Install dependencies in a virtual environment:</li> </ol> <pre><code>cd mcp_cloud\npython3.13 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <ol> <li>Ensure the database is accessible. If using Docker for the database:</li> </ol> <pre><code># From repo root, ensure database_postgres is running\ndocker compose up -d database_postgres\n</code></pre> <ol> <li>Set environment variables (create a <code>.env</code> file in the repo root or export them):</li> </ol> <pre><code>export PLANEXE_POSTGRES_HOST=localhost\nexport PLANEXE_POSTGRES_PORT=5432  # Or your mapped port (e.g., 5433 if you set PLANEXE_POSTGRES_PORT)\nexport PLANEXE_POSTGRES_DB=planexe\nexport PLANEXE_POSTGRES_USER=planexe\nexport PLANEXE_POSTGRES_PASSWORD=planexe\n</code></pre> <p>Note: The <code>PYTHONPATH</code> environment variable in the LM Studio config (see below) ensures that the <code>database_api</code> module can be imported. Make sure the path points to the PlanExe repository root (where <code>database_api/</code> is located).</p>"},{"location":"developer/mcp_cloud/#lm-studio-configuration","title":"LM Studio Configuration","text":"<p>Add the following to your LM Studio MCP servers configuration file:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"/absolute/path/to/PlanExe/mcp_cloud/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"mcp_cloud.app\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/absolute/path/to/PlanExe\",\n        \"PLANEXE_POSTGRES_HOST\": \"localhost\",\n        \"PLANEXE_POSTGRES_PORT\": \"5432\",\n        \"PLANEXE_POSTGRES_DB\": \"planexe\",\n        \"PLANEXE_POSTGRES_USER\": \"planexe\",\n        \"PLANEXE_POSTGRES_PASSWORD\": \"planexe\"\n      }\n    }\n  }\n}\n</code></pre> <p>Important: Replace <code>/absolute/path/to/PlanExe</code> with the actual absolute path to your PlanExe repository on your system.</p> <p>Example (if PlanExe is at <code>/absolute/path/to/PlanExe</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"/absolute/path/to/PlanExe/mcp_cloud/.venv/bin/python\",\n      \"args\": [\n        \"-m\",\n        \"mcp_cloud.app\"\n      ],\n      \"env\": {\n        \"PYTHONPATH\": \"/absolute/path/to/PlanExe\",\n        \"PLANEXE_POSTGRES_HOST\": \"localhost\",\n        \"PLANEXE_POSTGRES_PORT\": \"5432\",\n        \"PLANEXE_POSTGRES_DB\": \"planexe\",\n        \"PLANEXE_POSTGRES_USER\": \"planexe\",\n        \"PLANEXE_POSTGRES_PASSWORD\": \"planexe\"\n      }\n    }\n  }\n}\n</code></pre> <p>Using Docker (more complex, but keeps dependencies isolated):</p> <p>You can use <code>docker compose exec</code> to run mcp_cloud:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"compose\",\n        \"-f\",\n        \"/absolute/path/to/PlanExe/docker-compose.yml\",\n        \"exec\",\n        \"-T\",\n        \"mcp_cloud\",\n        \"python\",\n        \"-m\",\n        \"mcp_cloud.app\"\n      ]\n    }\n  }\n}\n</code></pre> <p>Note: This requires the <code>mcp_cloud</code> container to be running (<code>docker compose up -d mcp_cloud</code>).</p>"},{"location":"developer/mcp_cloud/#troubleshooting","title":"Troubleshooting","text":"<p>Connection issues: - Ensure the database is running and accessible at the configured host/port - Check that the <code>PYTHONPATH</code> in the LM Studio config points to the PlanExe repository root (containing <code>database_api/</code>, <code>mcp_cloud/</code>, etc.) - Verify the Python interpreter path in the <code>command</code> field is correct and points to the venv Python</p> <p>Import errors: - If you see <code>ModuleNotFoundError: No module named 'database_api'</code>, check that <code>PYTHONPATH</code> is set correctly - If you see <code>ModuleNotFoundError: No module named 'mcp'</code>, ensure you've installed the requirements: <code>pip install -r requirements.txt</code></p> <p>Database connection errors: - Verify Postgres is running: <code>docker compose ps database_postgres</code> - Check the port mapping: if you set <code>PLANEXE_POSTGRES_PORT=5433</code>, use <code>5433</code> in your env vars, not <code>5432</code> - Test connection: <code>psql -h localhost -p 5432 -U planexe -d planexe</code> (or your port)</p> <p>Path issues: - Always use absolute paths in LM Studio config, not relative paths - On Windows, use forward slashes in the config JSON (e.g., <code>C:/Users/...</code>) or escaped backslashes</p>"},{"location":"developer/mcp_cloud/#development","title":"Development","text":"<p>Run locally for testing:</p> <pre><code>cd mcp_cloud\nsource .venv/bin/activate  # If not already activated\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m mcp_cloud.app\n</code></pre>"},{"location":"developer/mcp_cloud/#railway-deployment","title":"Railway Deployment","text":"<p>See <code>railway.md</code> for Railway-specific deployment instructions. The server automatically detects Railway's <code>PORT</code> environment variable and binds to it.</p>"},{"location":"developer/mcp_cloud/#notes","title":"Notes","text":"<ul> <li>mcp_cloud communicates with <code>worker_plan_database</code> indirectly via the database for task management.</li> <li>Artifacts are fetched from <code>worker_plan</code> via HTTP instead of accessing the run directory directly. This avoids needing a shared volume mount, making it compatible with Railway and other cloud platforms.</li> <li>For artifacts:</li> <li><code>report.html</code> is fetched efficiently via the dedicated <code>/runs/{run_id}/report</code> endpoint</li> <li>Other files are fetched by downloading the run zip and extracting the file (less efficient but works without additional endpoints)</li> <li>Artifact writes are not yet supported via HTTP (would require a write endpoint in <code>worker_plan</code>).</li> <li>Artifact writes are rejected while a run is active (strict policy per spec).</li> <li>Task IDs use the TaskItem UUID (e.g., <code>5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1</code>).</li> <li>Security: Always set <code>PLANEXE_MCP_API_KEY</code> in production deployments to prevent unauthorized access.</li> </ul>"},{"location":"developer/mcp_local/","title":"PlanExe MCP locally - Experimental, likely to be changed a lot!","text":"<p>Model Context Protocol (MCP) local proxy for PlanExe.</p> <p>It runs on the user's computer and provides local disk access for downloads. The pipeline still runs in <code>mcp_cloud</code>, the MCP server running in the cloud; this proxy forwards tool calls over HTTP and downloads artifacts from <code>/download/{task_id}/...</code>.</p>"},{"location":"developer/mcp_local/#tools","title":"Tools","text":"<p><code>prompt_examples</code> - Return example prompts. Use these as examples for task_create. You can also call <code>task_create</code> with any prompt\u2014short prompts produce less detailed plans. <code>task_create</code> - Initiate creation of a plan. <code>task_status</code> - Get status and progress about the creation of a plan. <code>task_stop</code> - Abort creation of a plan. <code>task_download</code> - Download the plan, either html report or a zip with everything, and save it to disk.</p> <p>Tip: Call <code>prompt_examples</code> to get example prompts to use with task_create. The full catalog lives at <code>worker_plan/worker_plan_api/prompt/data/simple_plan_prompts.jsonl</code>.</p> <p><code>task_download</code> is a synthetic tool provided by the local proxy. It calls the remote MCP tool <code>task_file_info</code> to obtain a download URL, then downloads the file locally into <code>PLANEXE_PATH</code>.</p>"},{"location":"developer/mcp_local/#run-as-task-mcp-tasks-protocol","title":"Run as task (MCP tasks protocol)","text":"<p>Some MCP clients (e.g. the MCP Inspector) show a \"Run as task\" option for tools. That refers to the MCP tasks protocol: a separate mechanism where the client runs a tool in the background using RPC methods like <code>tasks/run</code>, <code>tasks/get</code>, <code>tasks/result</code>, and <code>tasks/cancel</code>, instead of a single blocking tool call.</p> <p>PlanExe does not use or advertise the MCP tasks protocol. Our interface is tool-based only: the agent calls <code>task_create</code> \u2192 gets a <code>task_id</code> \u2192 polls <code>task_status</code> \u2192 uses <code>task_download</code>. That flow is defined in <code>docs/mcp/planexe_mcp_interface.md</code> and is the intended design.</p> <p>You should not enable \"Run as task\" for PlanExe. The Python MCP SDK and clients like Cursor do not properly support the tasks protocol (method registration and initialization fail). Use the tools directly: create a task, poll status, then download when done.</p>"},{"location":"developer/mcp_local/#how-it-talks-to-mcp_cloud","title":"How it talks to mcp_cloud","text":"<ul> <li>The remote base URL is <code>PLANEXE_URL</code> (for example <code>http://localhost:8001/mcp</code>).</li> <li>Tool calls prefer the remote HTTP wrapper (<code>/mcp/tools/call</code>).</li> <li>If the HTTP wrapper is unavailable, the proxy falls back to MCP JSON-RPC   over <code>POST /mcp</code> (not SSE).</li> <li>Downloads use the remote <code>/download/{task_id}/...</code> endpoints.</li> <li>Authentication uses <code>PLANEXE_MCP_API_KEY</code> as a <code>Bearer</code> token.</li> <li>Retry behavior: Transient failures (server 5xx errors, network timeouts) are   automatically retried up to 3 times with exponential backoff (1s, 2s delays).   Client errors (4xx) are not retried. Retries are logged at WARNING level.</li> </ul>"},{"location":"developer/mcp_local/#debugging-with-mcp-inspector","title":"Debugging with MCP Inspector","text":"<p>Run the MCP inspector with the local script and environment variables:</p> <pre><code>npx @modelcontextprotocol/inspector \\\n  -e \"PLANEXE_URL\"=\"http://localhost:8001/mcp\" \\\n  -e \"PLANEXE_MCP_API_KEY\"=\"insert-your-api-key-here\" \\\n  -e \"PLANEXE_PATH\"=\"/Users/your-name/Desktop\" \\\n  --transport stdio \\\n  uv run --with mcp /absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\n</code></pre> <p>Then click \"Connect\", open \"Tools\", and use \"List Tools\" or invoke individual tools.</p>"},{"location":"developer/mcp_local/#client-configuration-local-script","title":"Client configuration (local script)","text":"<p>Clone the PlanExe repository on your computer. Use the absolute path to <code>planexe_mcp_local.py</code> and set <code>PLANEXE_PATH</code> to a directory where PlanExe is allowed to save files.</p>"},{"location":"developer/mcp_local/#local-docker-development","title":"Local Docker (development)","text":"<pre><code>\"planexe\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp\",\n    \"/absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n  ],\n  \"env\": {\n    \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n    \"PLANEXE_MCP_API_KEY\": \"insert-your-api-key-here\",\n    \"PLANEXE_PATH\": \"/User/your-name/Desktop\"\n  }\n}\n</code></pre>"},{"location":"developer/mcp_local/#remote-server-railway-or-cloud","title":"Remote server (Railway or cloud)","text":"<pre><code>\"planexe\": {\n  \"command\": \"uv\",\n  \"args\": [\n    \"run\",\n    \"--with\",\n    \"mcp\",\n    \"/absolute/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n  ],\n  \"env\": {\n    \"PLANEXE_URL\": \"https://your-railway-app.up.railway.app/mcp\",\n    \"PLANEXE_MCP_API_KEY\": \"insert-your-api-key-here\",\n    \"PLANEXE_PATH\": \"/User/your-name/Desktop\"\n  }\n}\n</code></pre>"},{"location":"developer/open_dir_server/","title":"Host Open Dir Server","text":""},{"location":"developer/open_dir_server/#why-this-exists","title":"Why this exists","text":"<ul> <li>Docker containers cannot launch host applications (e.g., macOS Finder) because they are isolated from the host OS.</li> <li>The Gradio frontend runs in a container and cannot run <code>open</code>, <code>xdg-open</code>, or <code>start</code> on the host.</li> <li>This small FastAPI service runs on the host and receives a path from the frontend, then asks the host OS to open that path.</li> </ul>"},{"location":"developer/open_dir_server/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ on the host (outside Docker).</li> </ul>"},{"location":"developer/open_dir_server/#setup-virtual-environment","title":"Setup (virtual environment)","text":"<pre><code>cd open_dir_server\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npython app.py\n</code></pre>"},{"location":"developer/open_dir_server/#configuration","title":"Configuration","text":"<p>Environment variables (<code>PLANEXE_</code> prefixed): - <code>PLANEXE_OPEN_DIR_SERVER_HOST</code> (default <code>127.0.0.1</code>) - <code>PLANEXE_OPEN_DIR_SERVER_PORT</code> (default <code>5100</code>) - <code>PLANEXE_HOST_RUN_DIR</code>: optional; only allow opening paths under this directory. Defaults to <code>PlanExe/run</code>.</p> <p>Frontend configuration: - Set <code>PLANEXE_OPEN_DIR_SERVER_URL</code> so the container can reach the host service:   - macOS/Windows (Docker Desktop): <code>http://host.docker.internal:5100</code>   - Linux (Docker Engine): <code>http://172.17.0.1:5100</code> (or add <code>host.docker.internal</code> pointing to the bridge IP).   - Local host-only (no Docker): <code>http://localhost:5100</code></p> <p>If you relocate the run directory, set <code>PLANEXE_HOST_RUN_DIR</code> to an absolute path, for example: - macOS: <code>/Users/you/PlanExe/run</code> - Linux: <code>/home/you/PlanExe/run</code> - Windows: <code>C:\\Users\\you\\PlanExe\\run</code></p>"},{"location":"developer/open_dir_server/#start-the-server","title":"Start the server","text":"<p>From <code>open_dir_server</code>: <pre><code>cd open_dir_server\nsource .venv/bin/activate\npython app.py\n</code></pre> The service will listen on <code>PLANEXE_OPEN_DIR_SERVER_HOST:PLANEXE_OPEN_DIR_SERVER_PORT</code>.</p>"},{"location":"developer/open_dir_server/#stop-the-server","title":"Stop the server","text":"<ul> <li>Press <code>Ctrl+C</code> in the terminal where it is running.</li> </ul>"},{"location":"developer/worker_plan/","title":"Worker plan service","text":"<p>This directory hosts the shared <code>worker_plan_internal</code> package that generates plans.</p> <ul> <li><code>worker_plan_internal/</code>: core planning logic.</li> <li><code>worker_plan_api/</code>: shared types (e.g., filenames) used by both the worker and frontend.</li> </ul>"},{"location":"developer/worker_plan/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>worker_plan</code> so its dependencies stay isolated (they may be incompatible with <code>frontend_single_user</code>). Use Python 3.13 \u2014 several native wheels (pydantic-core, orjson, tiktoken, greenlet, jiter) do not yet publish for 3.14 and will fail to build.</p> <pre><code>cd worker_plan\npython3.13 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e .\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m worker_plan.app\n</code></pre> <p>The app reads configuration from the <code>.env</code> file (located in the project root or <code>PLANEXE_CONFIG_PATH</code>). Host and port default to <code>localhost:8000</code> and can be overridden via <code>PLANEXE_WORKER_HOST</code> and <code>PLANEXE_WORKER_PORT</code>.</p> <p>The frontend can then point at <code>http://localhost:8000</code> via <code>PLANEXE_WORKER_PLAN_URL</code>.</p> <p>If you hit <code>ModuleNotFoundError: No module named 'worker_plan'</code>, ensure you: - are in <code>PlanExe/worker_plan</code> (not a subfolder) - ran <code>pip install -e .</code> in this venv without errors - exported <code>PYTHONPATH=$PWD/..:$PYTHONPATH</code> before starting uvicorn (the package lives one level up when your CWD is <code>worker_plan</code>)</p> <p>If you must stay on Python 3.14, expect source builds and potential failures; exporting <code>PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1</code> before <code>pip install -e .</code> may allow wheels to build, but 3.13 is recommended for a smooth setup.</p>"},{"location":"developer/worker_plan/#environment-variables","title":"Environment variables","text":"Variable Default Purpose <code>PLANEXE_WORKER_HOST</code> <code>0.0.0.0</code> Host address the worker binds to (only when running via <code>python -m worker_plan.app</code>). <code>PLANEXE_WORKER_PORT</code> <code>8000</code> Port the worker listens on (only when running via <code>python -m worker_plan.app</code>). <code>PLANEXE_RUN_DIR</code> <code>run</code> Directory under which run output folders are created. <code>PLANEXE_HOST_RUN_DIR</code> (unset) Optional host path base returned in <code>display_run_dir</code> to hint where runs live on the host. <code>PLANEXE_CONFIG_PATH</code> <code>.</code> Working directory for the pipeline; used as the <code>cwd</code> when spawning <code>worker_plan_internal.plan.run_plan_pipeline</code>. <code>PLANEXE_WORKER_RELAY_PROCESS_OUTPUT</code> <code>false</code> When <code>true</code>, pipe pipeline stdout/stderr to the worker logs instead of suppressing them. <code>PLANEXE_PURGE_ENABLED</code> <code>false</code> Enable the background scheduler that purges old run directories. <code>PLANEXE_PURGE_MAX_AGE_HOURS</code> <code>1</code> Maximum age (hours) of runs to delete when purging (scheduler and manual default). <code>PLANEXE_PURGE_INTERVAL_SECONDS</code> <code>3600</code> How often the purge scheduler runs when enabled. <code>PLANEXE_PURGE_RUN_PREFIX</code> <code>PlanExe_</code> Only purge runs whose IDs start with this prefix. <code>PLANEXE_LOG_LEVEL</code> <code>INFO</code> Sets the console log level for the worker API and the pipeline process. Accepted values are the standard logging levels (e.g., <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>). <p><code>PLANEXE_LOG_LEVEL</code> affects both the FastAPI worker and the spawned pipeline logs written to stdout. File logs in <code>run/&lt;id&gt;/log.txt</code> always include DEBUG and above.</p>"},{"location":"developer/worker_plan_database/","title":"Worker plan database","text":"<p>Subclass of the <code>worker_plan</code> service that runs the PlanExe pipeline with a Postgres database.</p> <ul> <li>Polls <code>TaskItem</code> rows, marks them processing, and runs the pipeline.</li> <li>Reports state/progress back to the DB and posts confirmations to MachAI.</li> <li>Uses the same <code>worker_plan_internal</code> code as <code>worker_plan</code>, plus the shared <code>database_api</code> models.</li> <li>Configure MachAI confirmation endpoints with <code>PLANEXE_IFRAME_GENERATOR_CONFIRMATION_PRODUCTION_URL</code> and <code>PLANEXE_IFRAME_GENERATOR_CONFIRMATION_DEVELOPMENT_URL</code> (both are required; the worker fails fast if missing).</li> </ul>"},{"location":"developer/worker_plan_database/#docker-usage","title":"Docker usage","text":"<ul> <li>Build/run single worker: <code>docker compose up --build worker_plan_database</code></li> <li>Run three workers (each with <code>PLANEXE_WORKER_ID=1/2/3</code>): <code>docker compose up -d worker_plan_database_1 worker_plan_database_2 worker_plan_database_3</code></li> <li>Reads <code>SQLALCHEMY_DATABASE_URI</code> when provided, otherwise builds one from:</li> <li><code>PLANEXE_POSTGRES_HOST|PORT|DB|USER|PASSWORD</code></li> <li>falls back to the <code>database_postgres</code> service defaults (<code>planexe/planexe</code> on port 5432)</li> <li>Logs stream to stdout with 12-factor style logging. Configure with <code>PLANEXE_LOG_LEVEL</code> (defaults to <code>INFO</code>).</li> <li>Volumes mounted in compose: <code>./run</code> (pipeline output), <code>.env</code>, <code>llm_config.json</code></li> <li>Entrypoint: <code>python -m worker_plan_database.app</code></li> </ul>"},{"location":"developer/worker_plan_database/#run-locally-with-a-venv","title":"Run locally with a venv","text":"<p>For a faster edit/run loop without Docker. Work from inside <code>worker_plan_database</code> so its dependencies stay isolated:</p> <pre><code>cd worker_plan_database\npython3.13 -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e ../worker_plan\npip install -r requirements.txt\nexport PYTHONPATH=$PWD/..:$PYTHONPATH\npython -m worker_plan_database.app\n</code></pre> <p>Run <code>deactivate</code> when you are done with the venv.</p> <p>The <code>PYTHONPATH</code> addition allows imports of <code>database_api</code> and <code>worker_plan_database</code> modules. The <code>pyrightconfig.json</code> and <code>.vscode/settings.json</code> configure the same paths for editor/IDE support. In Cursor/VS Code, select the interpreter from <code>.venv/bin/python</code> via Cmd+Shift+P \u2192 \"Python: Select Interpreter\".</p>"},{"location":"mcp/antigravity/","title":"Google Antigravity","text":"<p>Antigravity by Google.</p> <p>Antigravity MCP documentation</p>"},{"location":"mcp/antigravity/#interaction","title":"Interaction","text":"<p>My interaction history:</p> <ol> <li>tell me about the planexe mcp tool</li> <li>make 5 suggestions</li> <li>crisis response plan for yellow stone outbreak, please refine that</li> <li>I didn't meant outbreak, I meant vulcanic</li> <li>your prompt is a bit shorter than the example prompts</li> <li>go ahead create the plan</li> <li>stop that plan you are creating.</li> <li>now create the plan again, this time with ALL details. Last time you had FAST selected that would leave out most details.</li> <li>check status</li> <li>status</li> <li>status</li> <li>status</li> <li>download the report</li> <li>summarize the report</li> <li>does it correspond to your expectations?</li> </ol> <p>I had to manually ask about <code>check status</code> to get details how the plan creation was going. It's not something that Antigravity can do.</p> <p>The created plan is here: Yellowstone Evacuation</p>"},{"location":"mcp/antigravity/#prerequisites","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Antigravity if the PlanExe installation is incomplete.</p>"},{"location":"mcp/antigravity/#configuring-antigravity","title":"Configuring Antigravity","text":"<p>To configure Antigravity to use PlanExe, you need to add the MCP server configuration.</p> <ol> <li>Open Antigravity</li> <li>Click the \"...\" icon at the top of the Agent panel</li> <li>Select \"MCP Servers\"</li> <li>This opens the <code>mcp_config.json</code> file.</li> </ol> <p>Add the following <code>planexe</code> dictionary to your <code>mcpServers</code> configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet:</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Once you have saved the <code>mcp_config.json</code>. Then go to the <code>Manage MCP Servers</code> and click the refresh icon.</p> <p>If it doesn't work then ask on the PlanExe Discord for help.</p> <p>This is what it should look like: </p>"},{"location":"mcp/codex/","title":"OpenAI Codex","text":"<p>Guide for connecting codex with PlanExe via MCP.</p>"},{"location":"mcp/codex/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to Codex.</li> <li>PlanExe MCP server reachable by Codex.</li> </ul>"},{"location":"mcp/codex/#quick-setup","title":"Quick setup","text":"<ol> <li>Start Codex.</li> <li>Ask for MCP tools.</li> <li>Call <code>prompt_examples</code> to get examples.</li> <li>Call <code>task_create</code> to start a plan.</li> </ol>"},{"location":"mcp/codex/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/codex/#success-criteria","title":"Success criteria","text":"<ul> <li>You can retrieve prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/codex/#interaction","title":"Interaction","text":"<p>In a terminal, start codex like this:</p> <pre><code>codex\n</code></pre> <p>Inside codex; these are my interactions:</p> <ol> <li>tell me about the mcp tools you have access to</li> <li>for planexe, get the prompt examples</li> <li>I want you to formulate a prompt about constructing a new variant of english where the worst inconsistencies have been fixed such as 11th vs 1st, 21st, 31st,   potentially eliminated such suffixes. And the pronounciation inconsistencies have been cleaned up. I want you to adhere to the planexe example prompts.</li> <li>it's not just the ordinals. try again</li> <li>go ahead create this plan</li> <li>status</li> <li>status</li> <li>status</li> <li>download both</li> <li>summarize the html file</li> </ol> <p>The created plan is here: Clean English</p>"},{"location":"mcp/codex/#prerequisites_1","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Cursor if the PlanExe installation is incomplete.</p>"},{"location":"mcp/codex/#configuring-codex","title":"Configuring Codex","text":"<p>OpenAI's MCP documentation</p> <p>This is the command template. Make sure you tweak it, before running it.</p> <pre><code>codex mcp add planexe --env PLANEXE_URL=\"http://localhost:8001/mcp\" --env PLANEXE_PATH=\"/Users/your-name/Desktop\" -- uv run --with mcp /path/to/PlanExe/mcp_local/planexe_mcp_local.py\n</code></pre> <p>Make these adjustments to the command line.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Verify that it's working.</p> <pre><code>codex mcp list  \nName Command Args Env Cwd Status Auth       \nplanexe  uv       run --with mcp /path/to/PlanExe/mcp_local/planexe_mcp_local.py  PLANEXE_PATH=*****, PLANEXE_URL=*****  -    enabled  Unsupported\n</code></pre>"},{"location":"mcp/cursor/","title":"Cursor","text":"<p>According to Cursor's wikipedia page:</p> <p>Several media outlets have described Cursor as a vibe coding app.</p> <p>And</p> <p>Cursor allows developers produce code from natural language instructions.</p>"},{"location":"mcp/cursor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cursor installed.</li> <li>PlanExe MCP server reachable by Cursor.</li> </ul>"},{"location":"mcp/cursor/#quick-setup","title":"Quick setup","text":"<ol> <li>Configure MCP in Cursor.</li> <li>Ask for prompt examples.</li> <li>Create a plan task and download the report.</li> </ol>"},{"location":"mcp/cursor/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/cursor/#success-criteria","title":"Success criteria","text":"<ul> <li>You can fetch prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/cursor/#video","title":"Video","text":"<p>Video (1m29s) - PlanExe inside Cursor</p> <p>Here I'm chatting with Cursor. Behind the scenes Cursor talks with PlanExe via MCP.</p> <p>In total it takes 18 minutes to create the plan. The boring parts have been cropped out.</p>"},{"location":"mcp/cursor/#interaction","title":"Interaction","text":"<p>My interaction with Cursor for creating a plan is like this:</p> <ol> <li>tell me about the planexe mcp tool you have access to</li> <li>I want you to come up with a good prompt</li> <li>I want something ala winter olympics in Italy 2026</li> <li>Slightly different idea. I want Denmark to switch from DKK to EUR. Use the persona of a person representing Denmark's ministers.</li> <li>go ahead create plan with all details</li> <li>wait for 18 minutes until the plan has been created</li> <li>download the plan</li> </ol> <p>Here is the created plan: DKK to EUR</p>"},{"location":"mcp/cursor/#prerequisites_1","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Cursor if the PlanExe installation is incomplete.</p>"},{"location":"mcp/cursor/#configuring-cursor","title":"Configuring Cursor","text":"<p>Go to <code>Cursor Settings</code> \u2192 <code>Tools &amp; MCP</code></p> <p>Click <code>New MCP Server</code>, which opens <code>.cursor/mcp.json</code></p> <p>Insert the following <code>planexe</code> dictionary inside the <code>mcpServers</code> dictionary. </p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Now Cursor is connected with PlanExe, and it looks like this. If it doesn't then ask on the PlanExe Discord for help.</p> <p></p>"},{"location":"mcp/lm_studio/","title":"LM Studio","text":"<p>LM Studio is available for Linux/macOS/Windows.</p> <p>You need a hefty computer for running models locally.</p>"},{"location":"mcp/lm_studio/#prerequisites","title":"Prerequisites","text":"<ul> <li>LM Studio installed.</li> <li>PlanExe MCP server reachable by LM Studio.</li> </ul>"},{"location":"mcp/lm_studio/#quick-setup","title":"Quick setup","text":"<ol> <li>Configure MCP in LM Studio.</li> <li>Ask for prompt examples.</li> <li>Create a plan task and download the report.</li> </ol>"},{"location":"mcp/lm_studio/#sample-prompt","title":"Sample prompt","text":"<p>Get example prompts for creating a plan.</p>"},{"location":"mcp/lm_studio/#success-criteria","title":"Success criteria","text":"<ul> <li>You can fetch prompt examples.</li> <li>You can create a plan task.</li> <li>You can download the report.</li> </ul>"},{"location":"mcp/lm_studio/#interaction","title":"Interaction","text":"<p>My interaction with LM Studio for creating a plan is like this:</p> <ol> <li>tell me about the planexe mcp tool</li> <li>fetch the example prompts</li> <li>based on the example prompts. I want you to create a plan prompt for a social media website inspired by Reddit, but instead of the target audience being humans, I want the target audience to be AI agents talking with other AI agents. And hanging out in different channels.</li> <li>go ahead create a plan</li> <li>check status</li> <li>what is progress now</li> <li>status</li> <li>how about now</li> <li>download the report</li> <li>also download the zip</li> </ol> <p>LM Studio cannot autonomously check status, so it's up to the user to ask for it to invoke the <code>task_status</code> tool.</p> <p>The created plan is here: AI AgentNet</p>"},{"location":"mcp/lm_studio/#prerequisites_1","title":"Prerequisites","text":"<p>Check that your LM Studio works with a model that support tools such as glm-4.7-flash.</p> <p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring LM Studio if the PlanExe installation is incomplete.</p>"},{"location":"mcp/lm_studio/#configuring-lm-studio","title":"Configuring LM Studio","text":"<p>Follow step 1, 2, 3, 4. This should open LM Studio's <code>mcp.json</code> editor.</p> <p></p> <p>Insert the following <code>planexe</code> dictionary inside the <code>mcpServers</code> dictionary. </p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet.</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Now LM Studio is connected with PlanExe. If it doesn't work then ask on the PlanExe Discord for help.</p>"},{"location":"mcp/mcp_details/","title":"PlanExe MCP Details","text":"<p>MCP is work-in-progress, and I (Simon Strandgaard, the developer) may change it as I see fit. If there is a particular tool you want. Write to me on the PlanExe Discord, and I will see what I can do.</p> <p>This document lists the MCP tools exposed by PlanExe and example prompts for agents.</p>"},{"location":"mcp/mcp_details/#overview","title":"Overview","text":"<ul> <li>The primary MCP server runs in the cloud (see <code>mcp_cloud</code>).</li> <li>The local MCP proxy (<code>mcp_local</code>) forwards calls to the server and adds a local download helper.</li> <li>Tool responses return JSON in both <code>content.text</code> and <code>structuredContent</code>.</li> </ul>"},{"location":"mcp/mcp_details/#tool-catalog-mcp_cloud","title":"Tool Catalog, <code>mcp_cloud</code>","text":""},{"location":"mcp/mcp_details/#prompt_examples","title":"prompt_examples","text":"<p>Returns around five example prompts that show what good prompts look like. Each sample is typically 300\u2013800 words: detailed context, requirements, and success criteria. Usually the AI does the heavy lifting: the user has a vague idea, the agent calls <code>prompt_examples</code>, then expands that idea into a high-quality prompt (300\u2013800 words). The prompt is shown to the user, who can ask for further changes or confirm it\u2019s good to go. When the user confirms, the agent then calls <code>task_create</code>. Shorter or vaguer prompts produce lower-quality plans.</p> <p>Example prompt: <pre><code>Get example prompts for creating a plan.\n</code></pre></p> <p>Example call: <pre><code>{}\n</code></pre></p> <p>Response includes <code>samples</code> (array of prompt strings, each 300\u2013800 words) and <code>message</code>.</p>"},{"location":"mcp/mcp_details/#task_create","title":"task_create","text":"<p>Create a new plan task.</p> <p>Example prompt:</p> <p>Create a plan for: Weekly meetup for humans where participants are randomly paired every 5 minutes...</p> <p>Example call: <pre><code>{\"prompt\": \"Weekly meetup for humans where participants are randomly paired every 5 minutes...\"}\n</code></pre></p> <p>Optional argument: <pre><code>speed_vs_detail: \"ping\" | \"fast\" | \"all\"\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_status","title":"task_status","text":"<p>Fetch status/progress and recent files for a task.</p> <p>Example prompt: <pre><code>Get status for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_stop","title":"task_stop","text":"<p>Request an active task to stop.</p> <p>Example prompt: <pre><code>Stop task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#task_file_info","title":"task_file_info","text":"<p>Return download metadata for report or zip artifacts.</p> <p>Example prompt: <pre><code>Get report info for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p> <p>Available artifacts: <pre><code>\"report\" | \"zip\"\n</code></pre></p>"},{"location":"mcp/mcp_details/#tool-catalog-mcp_local","title":"Tool Catalog, <code>mcp_local</code>","text":"<p>The local proxy exposes the same tools as the server, and adds:</p>"},{"location":"mcp/mcp_details/#task_download","title":"task_download","text":"<p>Download report or zip to a local path.</p> <p>Example prompt: <pre><code>Download the report for task 2d57a448-1b09-45aa-ad37-e69891ff6ec7.\n</code></pre></p> <p>Example call: <pre><code>{\"task_id\": \"2d57a448-1b09-45aa-ad37-e69891ff6ec7\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#typical-flow","title":"Typical Flow","text":""},{"location":"mcp/mcp_details/#1-get-example-prompts","title":"1. Get example prompts","text":"<p>The user often starts with a vague idea. The AI calls <code>prompt_examples</code> first to see what good prompts look like (around five samples, 300\u2013800 words each), then expands the user\u2019s idea into a high-quality prompt and shows it to the user.</p> <p>Prompt: <pre><code>Get example prompts for creating a plan.\n</code></pre></p> <p>Tool call: <pre><code>{}\n</code></pre></p>"},{"location":"mcp/mcp_details/#2-create-a-plan","title":"2. Create a plan","text":"<p>The user reviews the prompt and either asks for further changes or confirms it\u2019s good to go. When the user confirms, the agent calls <code>task_create</code> with that prompt.</p> <p>Tool call: <pre><code>{\"prompt\": \"...\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#3-get-status","title":"3. Get status","text":"<p>Prompt: <pre><code>Get status for my latest task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\"}\n</code></pre></p>"},{"location":"mcp/mcp_details/#4-download-the-report","title":"4. Download the report","text":"<p>Prompt: <pre><code>Download the report for my task.\n</code></pre></p> <p>Tool call: <pre><code>{\"task_id\": \"&lt;task_id_from_task_create&gt;\", \"artifact\": \"report\"}\n</code></pre></p>"},{"location":"mcp/mcp_setup/","title":"MCP setup","text":"<p>This is the shortest path to a working PlanExe MCP integration.</p>"},{"location":"mcp/mcp_setup/#1-understand-the-flow","title":"1. Understand the flow","text":"<ol> <li>Ask for prompt examples.</li> <li>Expand the user idea into a high\u2011quality prompt.</li> <li>Create the plan task.</li> <li>Poll for status.</li> <li>Download the report (HTML or zip).</li> </ol>"},{"location":"mcp/mcp_setup/#2-minimal-tool-usage","title":"2. Minimal tool usage","text":"<ol> <li><code>prompt_examples</code></li> <li><code>task_create</code></li> <li><code>task_status</code></li> <li><code>task_download</code></li> </ol>"},{"location":"mcp/mcp_setup/#3-success-criteria","title":"3. Success criteria","text":"<ul> <li>You can fetch example prompts.</li> <li>You can create a plan task.</li> <li>You can download the report artifact.</li> </ul>"},{"location":"mcp/mcp_setup/#next-steps","title":"Next steps","text":"<ul> <li>Full tool details: MCP details</li> <li>Reference schema: PlanExe MCP interface</li> <li>App setup guides: Cursor, Codex, LM Studio</li> </ul>"},{"location":"mcp/mcp_troubleshooting/","title":"MCP troubleshooting","text":"<p>Common MCP integration issues and fixes.</p>"},{"location":"mcp/mcp_troubleshooting/#cannot-create-a-plan","title":"Cannot create a plan","text":"<ul> <li>Ensure your prompt is detailed (300\u2013800 words).</li> <li>Some topics may be refused by the model (harmful, unethical, or dangerous requests).</li> <li>Try a smaller model or a more reliable paid model.</li> <li>Confirm the MCP server is reachable from your client.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#status-never-changes","title":"Status never changes","text":"<ul> <li>Long\u2011running plans are normal; retry after a few minutes.</li> <li>If it stalls, create a new task and compare behavior.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#download-fails","title":"Download fails","text":"<ul> <li>Confirm the download URL is reachable from your network.</li> <li>If you run locally, make sure any proxy or base URL is correct.</li> <li>Ensure <code>PLANEXE_PATH</code> is a valid directory and that you have write permissions.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#lowquality-output","title":"Low\u2011quality output","text":"<ul> <li>Increase prompt detail and constraints.</li> <li>Use a stronger model.</li> <li>Avoid \u201cvague\u201d or conflicting requirements.</li> </ul>"},{"location":"mcp/mcp_troubleshooting/#quota-or-billing-errors","title":"Quota or billing errors","text":"<p>If you see errors like:</p> <p>You exceeded your current quota, please check your plan and billing details</p> <p>Then:</p> <ul> <li>Verify your provider has active billing.</li> <li>Check usage limits or rate limits.</li> <li>Try a different model or provider.</li> </ul>"},{"location":"mcp/mcp_welcome/","title":"Welcome to PlanExe MCP","text":"<p>PlanExe MCP lets AI agents (and the tools you build) create strategic plans from a plain-English prompt. You send a goal; PlanExe produces a draft plan. The MCP user then chooses whether to download the HTML report or a zip of intermediary files (JSON, MD, CSV) used to build that report.</p> <p>No MCP experience is required to get started.</p>"},{"location":"mcp/mcp_welcome/#who-this-is-for","title":"Who this is for","text":"<ul> <li>You\u2019re an AI agent \u2014 You have access to PlanExe\u2019s tools and want to create a plan for the user.</li> <li>You\u2019re building an agent or integration \u2014 You want to connect your app or assistant to PlanExe and need a gentle overview before diving into technical details.</li> </ul>"},{"location":"mcp/mcp_welcome/#what-you-can-do","title":"What you can do","text":"<ul> <li>Get example prompts \u2014 See what good prompts look like (detailed, typically 300\u2013800 words). It is the caller\u2019s responsibility to take inspiration from these examples and ensure the prompt sent to PlanExe is of similar or better quality. The agent can refine a vague idea into a high-quality prompt and show it to the user for approval before creating the plan.</li> <li>Create a plan \u2014 Send a prompt; PlanExe starts creating the plan (takes about 15\u201320 minutes). If the input prompt is of low quality, the output plan will be crap too.</li> <li>Check progress \u2014 Ask for status and see how far the plan has gotten.</li> <li>Download the report \u2014 When the plan is ready, the user specifies whether to download the HTML report or the zip of intermediary files (JSON, MD, CSV).</li> </ul>"},{"location":"mcp/mcp_welcome/#what-you-get","title":"What you get","text":"<p>The MCP user chooses which artifact to download:</p> <ul> <li>HTML report (around 40 pages) \u2014 executive summary, Gantt chart, risks, next steps, and more. Opens in a browser.</li> <li>Zip \u2014 intermediary files (JSON, MD, CSV) used to build the HTML report, for deeper inspection.</li> </ul>"},{"location":"mcp/mcp_welcome/#next-steps","title":"Next steps","text":"<ul> <li>Setup \u2014 MCP setup: recommended path to a working integration.</li> <li>See the tools and a typical flow \u2014 MCP details: tool list, example prompts, and step-by-step flow without heavy protocol detail.</li> <li>Set up in Cursor \u2014 Cursor: video, prerequisites, and how to connect PlanExe to Cursor.</li> <li>Set up in Windsurf \u2014 Windsurf: setup steps and example interaction.</li> <li>Set up in LM Studio \u2014 LM Studio: setup steps and example interaction.</li> <li>Set up in Codex \u2014 Codex: setup steps and example interaction.</li> <li>Set up in Antigravity \u2014 Antigravity: setup steps and example interaction.</li> <li>Full technical specification \u2014 PlanExe MCP interface: for implementors; request/response schemas, state machine, error codes, and compatibility rules.</li> <li>Troubleshooting \u2014 MCP troubleshooting: common integration issues and fixes.</li> </ul>"},{"location":"mcp/mcp_welcome/#get-help","title":"Get help","text":"<p>If something doesn\u2019t work or you\u2019re unsure how to integrate, ask on the PlanExe Discord. Include what you tried, your setup, and any error output.</p>"},{"location":"mcp/planexe_mcp_interface/","title":"PlanExe MCP Interface Specification (v1.0)","text":""},{"location":"mcp/planexe_mcp_interface/#1-purpose","title":"1. Purpose","text":""},{"location":"mcp/planexe_mcp_interface/#11-what-is-planexe","title":"1.1 What is PlanExe","text":"<p>PlanExe is a service that generates rough-draft project plans from a natural-language prompt. You describe a large goal (e.g. open a clinic, launch a product, build a moon base)\u2014the kind of project that in reality takes months or years. PlanExe produces a structured draft: steps, documents, and deliverables. The plan is not executable in its current form; it is a draft to refine and act on. Creating a plan is a long-running task (100+ LLM inference calls): create a task with a prompt, poll status, then download the HTML report and zip when done.</p>"},{"location":"mcp/planexe_mcp_interface/#12-what-kind-of-plan-does-it-create","title":"1.2 What kind of plan does it create","text":"<p>The plan is a project plan: a DAG of steps (Luigi tasks) that produce artifacts including a Gantt chart, risk analysis, and other project management deliverables. The main output is a large HTML file (approx 700KB) containing many sections. There is also a zip file containing all intermediary files (md, json, csv). Plan quality depends on prompt quality; use the prompt_examples tool to see the baseline before calling task_create.</p>"},{"location":"mcp/planexe_mcp_interface/#121-agent-facing-summary-for-server-instructions-tool-descriptions","title":"1.2.1 Agent-facing summary (for server instructions / tool descriptions)","text":"<p>Implementors should expose the following to agents so they understand what PlanExe does:</p> <ul> <li>What: PlanExe turns a plain-English goal into a structured strategic-plan draft (executive summary, Gantt, risk register, governance, etc.) in ~15\u201320 min. The plan is a draft to refine, not an executable or final document.</li> <li>Required interaction order: Step 1 \u2014 Call prompt_examples to fetch example prompts. Step 2 \u2014 Formulate a good prompt (use examples as a baseline; similar structure; get user approval). Step 3 \u2014 Only then call task_create with the approved prompt. Then poll task_status; use task_download or task_file_info when complete. To stop, call task_stop with the task_id from task_create.</li> <li>Output: Large HTML report (~700KB) and optional zip of intermediate files (md, json, csv).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#13-scope-of-this-document","title":"1.3 Scope of this document","text":"<p>This document specifies a Model Context Protocol (MCP) interface for PlanExe that enables AI agents and client UIs to:</p> <ol> <li>Create and run long-running plan generation workflows.</li> <li>Receive real-time progress updates (task status, log output).</li> <li>List, read, and edit artifacts produced in an output directory.</li> <li>Stop and resume execution with Luigi-aware incremental recomputation.</li> </ol> <p>The interface is designed to support:</p> <ul> <li>interactive \"build systems\" behavior (like make / bazel),</li> <li>resumable DAG execution (Luigi),</li> <li>deterministic artifact management.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#2-goals","title":"2. Goals","text":""},{"location":"mcp/planexe_mcp_interface/#21-functional-goals","title":"2.1 Functional goals","text":"<ul> <li>Task-based orchestration: each run is associated with a task ID.</li> <li>Long-running execution: starts asynchronously; clients poll or subscribe to events.</li> <li>Artifact-first workflow: outputs are exposed as file-like artifacts.</li> <li>Stop / Resume with minimal recompute:</li> <li>on resume, only invalidated downstream tasks regenerate.</li> <li>Progress reporting:</li> <li>progress_percentage</li> <li>Editable artifacts:</li> <li>user edits a generated file</li> <li>pipeline continues from that point, producing dependent outputs</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#22-non-functional-goals","title":"2.2 Non-functional goals","text":"<ul> <li>Idempotency: repeated tool calls should not corrupt state.</li> <li>Observability: logs, state transitions, and artifacts must be inspectable.</li> <li>Concurrency safety: prevent conflicting writes and illegal resume patterns.</li> <li>Extensibility: future versions can add task graph browsing, caching backends, exports.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#3-non-goals","title":"3. Non-goals","text":"<ul> <li>Defining PlanExe's internal plan schema, content format, or prompt strategy.</li> <li>Providing remote code execution inside artifacts.</li> <li>Implementing a full Luigi UI clone in MCP v1 (optional later).</li> <li>Guaranteeing ETA estimates (allowed but must be optional / best-effort).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#31-mcp-tools-vs-mcp-tasks-run-as-task","title":"3.1 MCP tools vs MCP tasks (\"Run as task\")","text":"<p>The MCP specification defines two different mechanisms:</p> <ul> <li>MCP tools (e.g. task_create, task_status, task_stop): the server exposes named tools; the client calls them and receives a response. PlanExe's interface is tool-based: the agent calls task_create \u2192 receives task_id \u2192 polls task_status \u2192 uses task_download. This document specifies those tools.</li> <li>MCP tasks protocol (\"Run as task\" in some UIs): a separate mechanism where the client can run a tool \"as a task\" using RPC methods such as tasks/run, tasks/get, tasks/result, tasks/cancel, tasks/list, so the tool runs in the background and the client polls for results.</li> </ul> <p>PlanExe does not use or advertise the MCP tasks protocol. Implementors and clients should use the tools only. Do not enable \"Run as task\" for PlanExe; many clients (e.g. Cursor) and the Python MCP SDK do not support the tasks protocol properly. The intended flow is: Step 1 \u2014 call prompt_examples; Step 2 \u2014 formulate a good prompt (user approval); Step 3 \u2014 call task_create; then poll task_status and call task_download when complete.</p>"},{"location":"mcp/planexe_mcp_interface/#4-system-model","title":"4. System Model","text":""},{"location":"mcp/planexe_mcp_interface/#41-core-entities","title":"4.1 Core entities","text":""},{"location":"mcp/planexe_mcp_interface/#task","title":"Task","text":"<p>A long-lived container for a PlanExe project run.</p> <p>Key properties</p> <ul> <li>task_id: UUID returned by task_create for that task. Each task_create returns a new UUID. Use that exact UUID for all MCP calls; do not substitute ids from other services.</li> <li>output_dir: artifact root namespace for task</li> <li>config: immutable run configuration (models, runtime limits, Luigi params)</li> <li>created_at, updated_at</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#run","title":"Run","text":"<p>A single execution attempt inside a task (e.g., after a resume).</p> <p>Key properties</p> <ul> <li>state: running | stopped | completed | failed</li> <li>progress_percentage: computed progress percentage (float)</li> <li>started_at, ended_at</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#artifact","title":"Artifact","text":"<p>A file-like output managed by PlanExe.</p> <p>Key properties</p> <ul> <li>path: path relative to task output root</li> <li>size, updated_at</li> <li>content_type: text/markdown, text/html, application/json, etc.</li> <li>sha256: content hash for optimistic locking and invalidation</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#event","title":"Event","text":"<p>A typed message emitted during execution for UI/agent consumption.</p> <p>Key properties</p> <ul> <li>cursor: ordering token</li> <li>ts: timestamp</li> <li>type: event type</li> <li>data: event payload</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#5-state-machine","title":"5. State Machine","text":""},{"location":"mcp/planexe_mcp_interface/#51-task-states","title":"5.1 Task states","text":"<p>Tasks may exist independent of active runs.</p> <ul> <li>created: task initialized, no run started</li> <li>active: at least one run exists, may be running or stopped</li> <li>archived: optional; immutable, no new runs allowed</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#52-run-states","title":"5.2 Run states","text":"<ul> <li>running</li> <li>stopping (optional transitional state)</li> <li>stopped (user stopped, resumable)</li> <li>completed</li> <li>failed (resumable depending on failure type)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#53-allowed-transitions","title":"5.3 Allowed transitions","text":"<ul> <li>running \u2192 stopped via task_stop</li> <li>running \u2192 completed via normal success</li> <li>running \u2192 failed via error</li> </ul> <p>Invalid</p> <ul> <li>completed \u2192 running (new run must be triggered by creating a new task)</li> <li>running \u2192 running (no concurrent runs in v1)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#6-mcp-tools-v1-required","title":"6. MCP Tools (v1 Required)","text":"<p>All tool names below are normative.</p>"},{"location":"mcp/planexe_mcp_interface/#61-prompt_examples","title":"6.1 prompt_examples","text":"<p>Step 1 \u2014 Call this first. Returns example prompts that define the baseline for what a good prompt looks like. Do not call task_create yet. Correct flow: Step 1 \u2014 call this tool to fetch examples. Step 2 \u2014 Formulate a good prompt (use examples as a baseline; similar structure; get user approval). Step 3 \u2014 Only then call task_create with the approved prompt. If you call task_create before formulating and approving a prompt, the resulting plan will be lower quality than it could be.</p> <p>Request: no parameters (empty object).</p> <p>Response:</p> <pre><code>{\n  \"samples\": [\"prompt text 1\", \"prompt text 2\", \"...\"],\n  \"message\": \"...\"\n}\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#62-task_create","title":"6.2 task_create","text":"<p>Step 3 \u2014 Call only after prompt_examples (Step 1) and after you have formulated a good prompt and got user approval (Step 2). Start creating a new plan with the approved prompt. speed_vs_detail modes: 'all' runs the full pipeline with all details (slower, higher token usage/cost). 'fast' runs the full pipeline with minimal work per step (faster, fewer details), useful to verify the pipeline is working. 'ping' runs the pipeline entrypoint and makes a single LLM call to verify the worker_plan_database is processing tasks and can reach the LLM.</p> <p>Request</p> <p>Schema</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"prompt\": { \"type\": \"string\" },\n    \"speed_vs_detail\": {\n      \"type\": \"string\",\n      \"enum\": [\"ping\", \"fast\", \"all\"],\n      \"default\": \"ping\"\n    }\n  },\n  \"required\": [\"prompt\"]\n}\n</code></pre> <p>Example</p> <pre><code>{\n  \"prompt\": \"string\",\n  \"speed_vs_detail\": \"ping\",\n  \"user_api_key\": \"pex_...\"\n}\n</code></pre> <p>Prompt quality</p> <p>The <code>prompt</code> parameter should be a detailed description of what the plan should cover. Good prompts are typically 300\u2013800 words and include:</p> <ul> <li>Clear context: background, constraints, and goals</li> <li>Specific requirements: budget, timeline, location, or technical constraints</li> <li>Success criteria: what \"done\" looks like</li> <li>Banned words or approaches (if any)</li> </ul> <p>Short one-liners (e.g., \"Construct a bridge\") tend to produce poor output because they lack context for the planning pipeline. Important details are location, budget, time frame.</p> <p>Optional</p> <ul> <li>user_api_key: user API key for credits and attribution (if your deployment requires it).</li> </ul> <p>Clients can call the MCP tool prompt_examples to retrieve example prompts. Use these as examples for task_create; they can also call task_create with any prompt\u2014short prompts produce less detailed plans.</p> <p>For the full catalog file:</p> <ul> <li><code>worker_plan/worker_plan_api/prompt/data/simple_plan_prompts.jsonl</code> \u2014 JSONL with <code>id</code>, <code>prompt</code>, optional <code>tags</code>, and optional <code>mcp_example</code> (true = curated for MCP).</li> </ul> <p>Response</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",\n  \"created_at\": \"2026-01-14T12:34:56Z\"\n}\n</code></pre> <p>Important</p> <ul> <li>task_id is a UUID returned by task_create. Use this exact UUID for task_status/task_stop/task_download.</li> </ul> <p>Behavior</p> <ul> <li>Must be idempotent only if client supplies an optional client_request_id (optional extension).</li> <li>Task config is immutable after creation in v1.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#63-task_status","title":"6.3 task_status","text":"<p>Returns run status and progress. Used for progress bars and UI states. Polling interval: call at reasonable intervals only (e.g. every 5 minutes); plan generation takes 15\u201320+ minutes and frequent polling is unnecessary.</p> <p>Request</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\"\n}\n</code></pre> <p>Input</p> <ul> <li>task_id: UUID returned by task_create. Use it to reference the plan being created.</li> </ul> <p>Response</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\",\n  \"state\": \"running\",\n  \"progress_percentage\": 62.0,\n  \"timing\": {\n    \"started_at\": \"2026-01-14T12:35:10Z\",\n    \"elapsed_sec\": 512\n  },\n  \"files\": [\n    {\n      \"path\": \"plan.md\",\n      \"updated_at\": \"2026-01-14T12:43:11Z\"\n    }\n  ]\n}\n</code></pre> <p>Notes</p> <ul> <li>progress_percentage must be a float within [0,100].</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#64-task_stop","title":"6.4 task_stop","text":"<p>Requests the plan generation to stop. Pass the task_id (the UUID returned by task_create). This is a normal MCP tool call: call task_stop with that task_id.</p> <p>Request</p> <pre><code>{\n  \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\"\n}\n</code></pre> <p>Input</p> <ul> <li>task_id: UUID returned by task_create. Use this same UUID when calling task_stop to request the run to stop.</li> </ul> <p>Response</p> <pre><code>{\n  \"state\": \"stopped\"\n}\n</code></pre> <p>Required semantics</p> <ul> <li>Must stop workers cleanly where possible.</li> <li>Must persist enough Luigi state to resume incrementally.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#65-download-flow-task_download-vs-task_file_info","title":"6.5 Download flow (task_download vs task_file_info)","text":"<p>If your client exposes task_download (e.g. mcp_local): use it to save the report or zip locally; it calls task_file_info under the hood, then fetches and writes to the local save path (e.g. PLANEXE_PATH).</p> <p>If you only have task_file_info (e.g. direct connection to mcp_cloud): call it with task_id and artifact (\"report\" or \"zip\"); use the returned download_url to fetch the file (e.g. GET with API key if configured).</p> <p>task_file_info input</p> <ul> <li>task_id: UUID returned by task_create. Use it to download the created plan.</li> <li>artifact: \"report\" or \"zip\" (default \"report\").</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#7-targets","title":"7. Targets","text":""},{"location":"mcp/planexe_mcp_interface/#71-standard-targets","title":"7.1 Standard targets","text":"<p>The following targets MUST be supported:</p> <ul> <li>build_plan</li> <li>validate_plan</li> <li>build_plan_and_validate</li> </ul> <p>Targets map to Luigi \"final tasks\".</p>"},{"location":"mcp/planexe_mcp_interface/#8-concurrency-locking","title":"8. Concurrency &amp; Locking","text":""},{"location":"mcp/planexe_mcp_interface/#81-single-active-run-per-task","title":"8.1 Single active run per task","text":"<p>In v1, tasks MUST enforce:</p> <ul> <li>at most one run in running state.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#9-error-model","title":"9. Error Model","text":"<p>Errors MUST return:</p> <ul> <li>code: stable machine-readable</li> <li>message: human-readable</li> <li>details: optional</li> </ul> <p>Example:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"RUN_ALREADY_ACTIVE\",\n    \"message\": \"A run is currently active for this task.\",\n    \"details\": { \"run_id\": \"run_0001\" }\n  }\n}\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#91-required-error-codes","title":"9.1 Required error codes","text":"<ul> <li>TASK_NOT_FOUND</li> <li>RUN_NOT_FOUND</li> <li>RUN_ALREADY_ACTIVE</li> <li>RUN_NOT_ACTIVE</li> <li>INVALID_TARGET</li> <li>INVALID_ARTIFACT_URI</li> <li>CONFLICT</li> <li>PERMISSION_DENIED</li> <li>RUNNING_READONLY</li> <li>INTERNAL_ERROR</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#10-security-isolation","title":"10. Security &amp; Isolation","text":""},{"location":"mcp/planexe_mcp_interface/#101-sandbox-constraints","title":"10.1 Sandbox constraints","text":"<ul> <li>All artifacts must live under task-scoped storage.</li> <li>Artifact URIs must not permit path traversal.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#102-access-control","title":"10.2 Access control","text":"<p>At minimum:</p> <ul> <li>task must be scoped to a user identity (metadata.user_id)</li> <li>callers without permission must receive PERMISSION_DENIED</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#103-sensitive-data-handling","title":"10.3 Sensitive data handling","text":"<ul> <li>logs may include model prompts/responses \u2192 treat logs as sensitive artifacts</li> <li>allow a config option to redact prompt content in event streaming</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#11-performance-requirements","title":"11. Performance Requirements","text":""},{"location":"mcp/planexe_mcp_interface/#111-responsiveness","title":"11.1 Responsiveness","text":"<ul> <li>task_status must return within &lt; 250ms under normal load.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#112-large-artifacts","title":"11.2 Large artifacts","text":"<ul> <li>server SHOULD impose max read size per call (e.g., 2\u201310MB)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#12-observability-requirements","title":"12. Observability Requirements","text":"<p>The server MUST persist:</p> <ul> <li>run lifecycle events</li> <li>stop reasons</li> <li>failure tracebacks as artifacts (e.g., run_error.json)</li> <li>luigi execution logs (run.log)</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#13-reference-ui-integration-contract","title":"13. Reference UI Integration Contract","text":"<p>To match your UI behavior:</p> <p>Progress bars</p> <p>Use:</p> <ul> <li>task_status.progress_percentage</li> <li>or progress_updated events</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#14-compatibility-versioning","title":"14. Compatibility &amp; Versioning","text":""},{"location":"mcp/planexe_mcp_interface/#141-versioning-strategy","title":"14.1 Versioning strategy","text":"<ul> <li>MCP server exposes: planexe.version = \"1.0\"</li> <li>breaking changes require major bump</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#142-forward-compatibility","title":"14.2 Forward compatibility","text":"<p>Clients must ignore unknown fields and unknown event types.</p>"},{"location":"mcp/planexe_mcp_interface/#15-testing-strategy","title":"15. Testing Strategy","text":""},{"location":"mcp/planexe_mcp_interface/#151-contract-tests-required","title":"15.1 Contract tests (required)","text":"<ul> <li>Start/stop/resume loops</li> <li>Invalid transition errors</li> <li>Event cursor monotonicity</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#152-determinism-tests-recommended","title":"15.2 Determinism tests (recommended)","text":"<ul> <li>Given same inputs + same edits, ensure same downstream artifacts unless models are stochastic</li> <li>If models are stochastic, test pipeline correctness, not identical bytes</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#153-load-tests-recommended","title":"15.3 Load tests (recommended)","text":"<ul> <li>multiple tasks concurrently, one run each</li> <li>event streaming stability under heavy log output</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#16-future-extensions-mcp-resources","title":"16. Future Extensions (MCP Resources)","text":"<p>PlanExe is artifact-first, and MCP already has a native concept for that: resources. Today artifacts are exposed via download_url or via proxy download + saved_path. Future versions SHOULD expose artifacts as MCP resources so clients can fetch them via standard resource reads (and treat PlanExe as a first-class MCP server rather than a thin API wrapper).</p> <p>Proposed resource identifiers</p> <ul> <li>planexe://task//report <li>planexe://task//zip <p>Recommended resource metadata</p> <ul> <li>mime type (content_type)</li> <li>size (bytes)</li> <li>sha256 (content hash)</li> <li>generated_at (UTC timestamp)</li> </ul> <p>Notes</p> <ul> <li>Resources can be backed by existing HTTP endpoints internally; the MCP resource read returns the bytes + metadata.</li> <li>This enables richer MCP client UX (preview, caching, validation) without custom tool calls.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#17-future-tools-high-leverage-low-complexity","title":"17. Future Tools (High-Leverage, Low-Complexity)","text":"<p>The following tools remove common UX friction without expanding the core model.</p>"},{"location":"mcp/planexe_mcp_interface/#171-task_list-or-task_recent","title":"17.1 task_list (or task_recent)","text":"<p>Return a short list of recent tasks so agents can recover if they lost a task_id.</p> <p>Notes</p> <ul> <li>Default limit: 5\u201310 tasks.</li> <li>Include task_id, created_at, state, and prompt summary.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#172-task_wait","title":"17.2 task_wait","text":"<p>Blocking helper that polls internally until the task completes or times out. Returns the final task_status payload plus suggested next steps.</p> <p>Notes</p> <ul> <li>Inputs: task_id, timeout_sec (optional), poll_interval_sec (optional).</li> <li>Outputs: same as task_status + next_steps (string or list).</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#173-task_get_latest","title":"17.3 task_get_latest","text":"<p>Simplest recovery: return the most recently created task for the caller.</p> <p>Notes</p> <ul> <li>Useful for single-user / single-session flows.</li> <li>Should be scoped to the caller/user_id when available.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#174-task_logs_tail-optional","title":"17.4 task_logs_tail (optional)","text":"<p>Return the tail of recent log lines for troubleshooting failures.</p> <p>Notes</p> <ul> <li>Inputs: task_id, max_lines (optional), since_cursor (optional).</li> <li>Useful when task_status shows failed but no context.</li> </ul>"},{"location":"mcp/planexe_mcp_interface/#appendix-a-example-end-to-end-flow","title":"Appendix A \u2014 Example End-to-End Flow","text":"<p>Create task</p> <pre><code>{ \"prompt\": \"...\" }\n</code></pre> <p>Start run</p> <pre><code>{ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }\n</code></pre> <p>Stop</p> <pre><code>{ \"task_id\": \"5e2b2a7c-8b49-4d2f-9b8f-6a3c1f05b9a1\" }\n</code></pre>"},{"location":"mcp/planexe_mcp_interface/#appendix-b-optional-v11-extensions","title":"Appendix B \u2014 Optional v1.1 Extensions","text":"<p>If you want richer Luigi integration later:</p> <ul> <li>planexe.task.graph (nodes + edges + states)</li> <li>planexe.task.invalidate (rerun subtree)</li> <li>planexe.export.bundle (zip all artifacts)</li> <li>planexe.validate.only (audit without regeneration)</li> <li>planexe.task.archive (freeze task)</li> </ul>"},{"location":"mcp/windsurf/","title":"Windsurf","text":"<p>Windsurf.</p> <p>Windsurf MCP documentation</p> <p>Windsurf MCP tutorial</p>"},{"location":"mcp/windsurf/#interaction","title":"Interaction","text":"<p>My interaction history:</p> <ol> <li>get planexe example prompts</li> <li>I want you to suggest 5 prompts, based on the example prompts</li> <li>suggest something that fixes real world problems</li> <li>5 more</li> <li>I'm in europe. make 5 suggestions that fixes serious issues in europe</li> <li>I like your Heatwave mortality reduction idea. I want you to make a full prompt ala the planexe example prompts, and show me the prompt</li> <li>remove the heading \"Full PlanExe-style prompt: Heatwave mortality reduction (Europe)\". what do you think about the prompt?</li> <li>go ahead create this plan</li> <li>status</li> <li>status     Here windsurf went ahead and downloaded the created HTML report</li> <li>compare the created plan with the prompt you formulated</li> <li>also download the zip</li> </ol> <p>I had to manually ask about <code>check status</code> to get details how the plan creation was going. It's not something that Windsurf can do.</p> <p>The created plan is here: Heatwave Resilience</p>"},{"location":"mcp/windsurf/#prerequisites","title":"Prerequisites","text":"<p>A working installation of PlanExe.</p> <ul> <li>The recommended way is to install PlanExe by following the Getting Started instructions.   Make sure that <code>docker compose up</code> is running, in order to connect to PlanExe.</li> <li>Alternatively: Run PlanExe on another server and port.</li> <li>Alternatively: If you are a developer run PlanExe inside a python virtual environment.</li> </ul> <p>Double check that PlanExe can take a prompt and create a plan. Since it doesn't make sense to start configuring Windsurf if the PlanExe installation is incomplete.</p>"},{"location":"mcp/windsurf/#configuring-windsurf","title":"Configuring Windsurf","text":"<p>To configure Windsurf to use PlanExe, you need to add the MCP server configuration.</p> <p></p> <ol> <li>Open Windsurf</li> <li>Click the \"...\" icon at the top of the Agent panel, this opens a menu.</li> <li>Click the \"Open MCP Config File\" icon at the bottom of the menu.</li> <li>This opens the <code>mcp_config.json</code> file.</li> </ol> <p>Add the following <code>planexe</code> dictionary to your <code>mcpServers</code> configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"planexe\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--with\",\n        \"mcp\",\n        \"/path/to/PlanExe/mcp_local/planexe_mcp_local.py\"\n      ],\n      \"env\": {\n        \"PLANEXE_URL\": \"http://localhost:8001/mcp\",\n        \"PLANEXE_PATH\": \"/Users/your-name/Desktop\"\n      }\n    }\n  }\n}\n</code></pre> <p>Make these adjustments to the <code>planexe</code> snippet:</p> <ul> <li>Make adjustments to <code>/path/to/PlanExe</code> so it points to where PlanExe is located on your computer.</li> <li>Make adjustments to <code>/Users/your-name/Desktop</code> so it points to the directory where PlanExe is allowed to write to, so the plan can be downloaded.</li> <li>Optional: Make adjustments to <code>http://localhost:8001/mcp</code> if you have PlanExe running on another port.</li> </ul> <p>Once you have saved the <code>mcp_config.json</code>. Then go to the <code>Manage MCP Servers</code> and click the refresh icon.</p> <p>If it doesn't work then ask on the PlanExe Discord for help.</p>"},{"location":"proposals/01-agent-smart-routing/","title":"Agent Smart Routing - Meta-Agent Dispatcher","text":""},{"location":"proposals/01-agent-smart-routing/#overview","title":"Overview","text":"<p>PlanExe's planning pipeline currently uses a single agent profile for all stages. As plans grow in complexity and domain diversity, different stages benefit from specialized agents optimized for specific tasks (research, writing, technical validation, creativity).</p> <p>This proposal introduces a meta-agent dispatcher that routes each pipeline stage to the most appropriate agent based on stage type, domain, and requirements.</p>"},{"location":"proposals/01-agent-smart-routing/#problem","title":"Problem","text":"<ul> <li> <p>Generic agents produce mediocre results across all domains</p> </li> <li> <p>No way to leverage specialized models (reasoning models for analysis, fast models for formatting, etc.)</p> </li> <li> <p>Pipeline stages have different cost/quality trade-offs that aren't exploited</p> </li> </ul>"},{"location":"proposals/01-agent-smart-routing/#proposed-solution","title":"Proposed Solution","text":""},{"location":"proposals/01-agent-smart-routing/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PlanExe Core   \u2502\n\u2502   (Orchestrator)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Meta-Agent      \u2502  \u2190 Dispatcher logic\n\u2502 Router          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u251c\u2500\u2500\u2192 Research Agent (Gemini 2.0 Flash)\n         \u251c\u2500\u2500\u2192 Writing Agent (Claude Sonnet)\n         \u251c\u2500\u2500\u2192 Technical Agent (GPT-4 + reasoning)\n         \u2514\u2500\u2500\u2192 Format Agent (Haiku/Fast model)\n</code></pre>"},{"location":"proposals/01-agent-smart-routing/#routing-rules","title":"Routing Rules","text":"<p>Store routing configuration in <code>llm_config.json</code>:</p> <pre><code>{\n  \"agent_routing\": {\n    \"research\": {\n      \"model\": \"google/gemini-2.0-flash-thinking-exp\",\n      \"reason\": \"Fast, cheap, good at web search synthesis\"\n    },\n    \"outline\": {\n      \"model\": \"anthropic/claude-sonnet-4\",\n      \"reason\": \"Strong at structure and planning\"\n    },\n    \"technical\": {\n      \"model\": \"openai/gpt-4-turbo\",\n      \"thinking\": \"enabled\",\n      \"reason\": \"Deep reasoning for complex technical content\"\n    },\n    \"format\": {\n      \"model\": \"anthropic/claude-haiku-4\",\n      \"reason\": \"Fast, cheap, reliable for formatting\"\n    }\n  }\n}\n</code></pre>"},{"location":"proposals/01-agent-smart-routing/#implementation","title":"Implementation","text":"<ol> <li> <p>Add <code>AgentRouter</code> class in <code>backend/mcp_cloud/src/routing/</code></p> </li> <li> <p>Modify pipeline stages to call <code>router.get_agent(stage_type, domain)</code></p> </li> <li> <p>Add telemetry to track agent selection and performance per stage</p> </li> <li> <p>Build admin UI to override routing rules per-customer</p> </li> </ol>"},{"location":"proposals/01-agent-smart-routing/#benefits","title":"Benefits","text":"<ul> <li> <p>15-30% cost reduction by using fast models for simple stages</p> </li> <li> <p>Quality improvement from specialized agents</p> </li> <li> <p>Flexibility for customers to bring their own agent configs</p> </li> <li> <p>A/B testing different agent combinations per stage</p> </li> </ul>"},{"location":"proposals/01-agent-smart-routing/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Increased complexity Start with 3-4 agent profiles, expand gradually Debugging harder Add detailed logging of agent selection Config drift Validate routing config on startup, fail fast"},{"location":"proposals/01-agent-smart-routing/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Prototype with 3 agents (research, writing, format)</p> </li> <li> <p>Run side-by-side comparison on 20 existing plans</p> </li> <li> <p>Measure cost savings and quality delta</p> </li> <li> <p>Ship behind feature flag, enable for beta customers</p> </li> </ol>"},{"location":"proposals/01-agent-smart-routing/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Cost per plan decreases by 20%+</p> </li> <li> <p>User satisfaction rating increases (via post-plan survey)</p> </li> <li> <p>No increase in pipeline failure rate</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/","title":"Plans as LLM Templates - Parameterized Prompt Export","text":""},{"location":"proposals/02-plans-as-LLM-templates/#overview","title":"Overview","text":"<p>PlanExe generates comprehensive business plans, but they're currently opaque artifacts. External agents and automation tools can't easily consume plan logic or adapt plans to new contexts.</p> <p>This proposal treats completed plans as reusable LLM templates with parameterized sections, enabling:</p> <ul> <li> <p>Export as Jinja2-style templates</p> </li> <li> <p>API endpoint for template rendering with custom variables</p> </li> <li> <p>Plan remixing and few-shot learning for downstream agents</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#problem","title":"Problem","text":"<ul> <li> <p>Plans are one-shot artifacts with no reuse mechanism</p> </li> <li> <p>Agents can't easily say \"give me a plan like X but for industry Y\"</p> </li> <li> <p>No structured way to extract the prompt logic that created a good plan</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#proposed-solution","title":"Proposed Solution","text":""},{"location":"proposals/02-plans-as-LLM-templates/#plan-template-format","title":"Plan Template Format","text":"<p>Export plans as structured templates with:</p> <pre><code>---\ntemplate_id: restaurant-expansion-v1\nbase_plan_id: {{ plan_uuid }}\nvariables:\n  - industry: string (required)\n  - location: string (required)\n  - budget: number (optional, default: 50000)\n  - timeline_months: number (optional, default: 12)\n---\n\n# {{ industry | title }} Expansion Plan - {{ location }}\n\n## Executive Summary\n\nThis plan outlines a {{ timeline_months }}-month expansion strategy for a {{ industry }} business in {{ location }} with a budget of ${{ budget | number_format }}.\n\n{% if budget &lt; 100000 %}\n**Budget Constraint Noted**: Lean startup approach recommended given capital limitations.\n{% endif %}\n\n## Market Analysis\n\n{% block market_analysis %}\n[Market research for {{ industry }} in {{ location }}]\n{% endblock %}\n\n...\n</code></pre>"},{"location":"proposals/02-plans-as-LLM-templates/#api-endpoint","title":"API Endpoint","text":"<pre><code>POST /api/plan/template/render\nAuthorization: Bearer &lt;api_key&gt;\nContent-Type: application/json\n\n{\n  \"template_id\": \"restaurant-expansion-v1\",\n  \"variables\": {\n    \"industry\": \"coffee shop\",\n    \"location\": \"Portland, OR\",\n    \"budget\": 75000,\n    \"timeline_months\": 8\n  }\n}\n</code></pre> <p>Response: <pre><code>{\n  \"rendered_plan\": \"# Coffee Shop Expansion Plan - Portland, OR\\n\\n...\",\n  \"estimated_tokens\": 12500,\n  \"template_version\": \"1.0.0\"\n}\n</code></pre></p>"},{"location":"proposals/02-plans-as-LLM-templates/#storage-schema","title":"Storage Schema","text":"<p>Add <code>plan_templates</code> table:</p> <pre><code>CREATE TABLE plan_templates (\n  id UUID PRIMARY KEY,\n  source_plan_id UUID REFERENCES plans(id),\n  template_name TEXT UNIQUE,\n  template_body TEXT,  -- Jinja2 template\n  variables JSONB,     -- Variable schema\n  created_at TIMESTAMPTZ DEFAULT now(),\n  downloads INTEGER DEFAULT 0\n);\n</code></pre>"},{"location":"proposals/02-plans-as-LLM-templates/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Agent Few-Shot Learning: \"Generate a plan like template X but for domain Y\"</p> </li> <li> <p>Customer Self-Service: Browse template library, fill in variables, instant draft</p> </li> <li> <p>Plan Remixing: Combine sections from multiple templates</p> </li> <li> <p>API Integration: External tools can request plans programmatically</p> </li> </ol>"},{"location":"proposals/02-plans-as-LLM-templates/#benefits","title":"Benefits","text":"<ul> <li> <p>Plan reuse - Good plans become templates for future work</p> </li> <li> <p>Faster generation - Template rendering is instant (no LLM call for structure)</p> </li> <li> <p>Consistency - Templates enforce proven structures</p> </li> <li> <p>Monetization - Premium template library for subscribers</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#implementation-plan","title":"Implementation Plan","text":""},{"location":"proposals/02-plans-as-LLM-templates/#phase-1-template-export-week-1-2","title":"Phase 1: Template Export (Week 1-2)","text":"<ul> <li> <p>Add \"Export as Template\" button in plan UI</p> </li> <li> <p>Generate Jinja2 from plan HTML/markdown</p> </li> <li> <p>Store in <code>plan_templates</code> table</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#phase-2-rendering-engine-week-3","title":"Phase 2: Rendering Engine (Week 3)","text":"<ul> <li> <p>Build Jinja2 renderer with variable validation</p> </li> <li> <p>Add <code>/api/plan/template/render</code> endpoint</p> </li> <li> <p>Rate limit: 10 renders/hour for free tier</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#phase-3-template-library-week-4-5","title":"Phase 3: Template Library (Week 4-5)","text":"<ul> <li> <p>Public template browse UI</p> </li> <li> <p>Search and filter by industry/domain</p> </li> <li> <p>User ratings and favorites</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#phase-4-advanced-features-future","title":"Phase 4: Advanced Features (Future)","text":"<ul> <li> <p>Template versioning (v1, v2, etc.)</p> </li> <li> <p>Diff view between template versions</p> </li> <li> <p>Collaborative template editing</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Template quality varies Curate \"verified\" templates from high-rated plans Variable validation complexity Start with simple types (string, number, boolean) Jinja2 injection attacks Sandbox rendering, whitelist allowed filters Templates go stale Track usage, deprecate low-download templates"},{"location":"proposals/02-plans-as-LLM-templates/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>50+ templates published in first month</p> </li> <li> <p>20% of new plans start from a template</p> </li> <li> <p>Template renders account for 15%+ of API usage</p> </li> <li> <p>User feedback: \"faster than starting from scratch\"</p> </li> </ul>"},{"location":"proposals/02-plans-as-LLM-templates/#references","title":"References","text":"<ul> <li> <p>Jinja2 documentation: https://jinja.palletsprojects.com/</p> </li> <li> <p>Similar pattern: Terraform modules, Helm charts, AWS CloudFormation templates</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/","title":"Distributed Plan Execution - Worker Pool Parallelism","text":""},{"location":"proposals/03-distributed-plan-execution/#overview","title":"Overview","text":"<p>PlanExe's plan generation pipeline currently runs sequentially on a single worker. For complex, multi-stage plans (research \u2192 outline \u2192 expand \u2192 review), this creates bottlenecks and wastes compute when stages could run in parallel.</p> <p>This proposal introduces a distributed execution model with worker pool parallelism and DAG-based scheduling for compute-heavy plan stages.</p>"},{"location":"proposals/03-distributed-plan-execution/#problem","title":"Problem","text":"<ul> <li> <p>Single-threaded execution = slow generation for complex plans</p> </li> <li> <p>Wasted compute: Outline stage could start while research continues</p> </li> <li> <p>No horizontal scaling: Can't throw more workers at the problem</p> </li> <li> <p>Railway infrastructure supports multi-worker deployments but pipeline doesn't use it</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#proposed-solution","title":"Proposed Solution","text":""},{"location":"proposals/03-distributed-plan-execution/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Plan Request        \u2502\n\u2502  (HTTP API)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  DAG Scheduler       \u2502  \u2190 Determines stage dependencies\n\u2502  (Coordinator)       \u2502     and dispatches to workers\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     v           v         v         v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Worker 1 \u2502 \u2502Worker 2 \u2502 \u2502Worker 3 \u2502 \u2502Worker N \u2502\n\u2502(Research)\u2502 \u2502(Outline)\u2502 \u2502(Expand) \u2502 \u2502(Review) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502           \u2502         \u2502         \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   v\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502  Redis Queue  \u2502  \u2190 Job state + results\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/03-distributed-plan-execution/#stage-dependency-dag","title":"Stage Dependency DAG","text":"<pre><code># Example DAG for standard business plan\nplan_dag = {\n    \"research\": {\n        \"depends_on\": [],\n        \"parallelizable\": True,\n        \"subtasks\": [\"market_research\", \"competitor_analysis\", \"regulatory_research\"]\n    },\n    \"outline\": {\n        \"depends_on\": [\"research\"],\n        \"parallelizable\": False\n    },\n    \"expand_sections\": {\n        \"depends_on\": [\"outline\"],\n        \"parallelizable\": True,\n        \"subtasks\": [\"exec_summary\", \"market_analysis\", \"operations\", \"financial\"]\n    },\n    \"review\": {\n        \"depends_on\": [\"expand_sections\"],\n        \"parallelizable\": False\n    },\n    \"format\": {\n        \"depends_on\": [\"review\"],\n        \"parallelizable\": False\n    }\n}\n</code></pre>"},{"location":"proposals/03-distributed-plan-execution/#worker-pool-management","title":"Worker Pool Management","text":"<p>Railway Configuration: <pre><code># railway.toml\n[workers]\n  plan_worker:\n    build:\n      dockerfile: Dockerfile.worker\n    replicas: 5  # Scale based on load\n    env:\n      REDIS_URL: ${REDIS_URL}\n      WORKER_POOL: plan_execution\n</code></pre></p> <p>Task Queue (Celery-style): <pre><code>from celery import Celery\n\napp = Celery('planexe', broker='redis://localhost:6379/0')\n\n@app.task(name='stage.research')\ndef execute_research_stage(plan_id, prompt_context):\n    # Run research subtasks in parallel\n    results = group([\n        research_market.s(plan_id, prompt_context),\n        research_competitors.s(plan_id, prompt_context),\n        research_regulatory.s(plan_id, prompt_context)\n    ])()\n    return results.get()\n\n@app.task(name='stage.outline')\ndef execute_outline_stage(plan_id, research_results):\n    # Depends on research completion\n    return generate_outline(plan_id, research_results)\n</code></pre></p>"},{"location":"proposals/03-distributed-plan-execution/#implementation-plan","title":"Implementation Plan","text":""},{"location":"proposals/03-distributed-plan-execution/#phase-1-dag-scheduler-week-1-2","title":"Phase 1: DAG Scheduler (Week 1-2)","text":"<ul> <li> <p>Define stage dependency graph schema (YAML config)</p> </li> <li> <p>Build coordinator service that parses DAG and dispatches tasks</p> </li> <li> <p>Add Redis for job state management</p> </li> <li> <p>Single worker proof-of-concept</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#phase-2-worker-pool-week-3","title":"Phase 2: Worker Pool (Week 3)","text":"<ul> <li> <p>Deploy 3-5 workers on Railway</p> </li> <li> <p>Implement task routing and load balancing</p> </li> <li> <p>Add retry logic and failure handling</p> </li> <li> <p>Monitor queue depth and worker utilization</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#phase-3-parallel-stages-week-4","title":"Phase 3: Parallel Stages (Week 4)","text":"<ul> <li> <p>Enable parallel execution for research subtasks</p> </li> <li> <p>Enable parallel execution for section expansion</p> </li> <li> <p>Add progress reporting (% complete across all workers)</p> </li> <li> <p>Optimize stage chunking for latency</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#phase-4-auto-scaling-week-5","title":"Phase 4: Auto-Scaling (Week 5+)","text":"<ul> <li> <p>Dynamic worker scaling based on queue depth</p> </li> <li> <p>Cost optimization (scale down during off-hours)</p> </li> <li> <p>Priority queues (premium users get dedicated workers)</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#benefits","title":"Benefits","text":"<ul> <li> <p>3-5x faster plan generation for complex plans</p> </li> <li> <p>Horizontal scaling - add more workers as load increases</p> </li> <li> <p>Better resource utilization - multiple stages run concurrently</p> </li> <li> <p>Resilience - worker failure doesn't kill entire plan generation</p> </li> <li> <p>Cost efficiency - pay for compute only when queue is deep</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#technical-stack","title":"Technical Stack","text":"<ul> <li> <p>Task Queue: Celery + Redis (battle-tested, Python-native)</p> </li> <li> <p>DAG Engine: Custom lightweight scheduler (simpler than Airflow for our use case)</p> </li> <li> <p>Worker Runtime: Docker containers on Railway</p> </li> <li> <p>State Storage: Redis (job metadata) + PostgreSQL (completed plans)</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Added complexity Start with simple DAG, expand gradually Redis becomes bottleneck Use Redis cluster, cache subtask results Worker coordination overhead Keep DAG shallow (max 5 stages), minimize inter-worker communication Cost increase Monitor worker utilization, scale down aggressively Debugging harder Centralized logging (Sentry), trace IDs across workers"},{"location":"proposals/03-distributed-plan-execution/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Average plan generation time decreases by 50%+</p> </li> <li> <p>Worker CPU utilization stays 60-80% (not idle, not maxed)</p> </li> <li> <p>Task retry rate &lt; 2% (most jobs succeed first try)</p> </li> <li> <p>P95 latency under 10 minutes for standard business plan</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#future-enhancements","title":"Future Enhancements","text":"<ul> <li> <p>GPU workers for vision/multimodal stages</p> </li> <li> <p>Speculative execution (start likely next stage before deps finish)</p> </li> <li> <p>Agent-specific worker pools (specialized workers for finance plans vs. tech plans)</p> </li> </ul>"},{"location":"proposals/03-distributed-plan-execution/#references","title":"References","text":"<ul> <li> <p>Celery documentation: https://docs.celeryq.dev/</p> </li> <li> <p>Railway multi-service deploys: https://docs.railway.app/</p> </li> <li> <p>DAG scheduling patterns: Apache Airflow, Prefect, Temporal</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/","title":"Plan Explain API - Natural Language Summaries","text":""},{"location":"proposals/04-plan-explain-as-API-service/#overview","title":"Overview","text":"<p>PlanExe generates detailed, comprehensive business plans that can be 50-100 pages long. Users often need quick summaries for:</p> <ul> <li> <p>Email updates to stakeholders</p> </li> <li> <p>Dashboard previews</p> </li> <li> <p>Customer support responses</p> </li> <li> <p>Social media posts about plan progress</p> </li> </ul> <p>This proposal introduces a <code>/api/plan/{id}/explain</code> endpoint that returns natural-language summaries of any plan using a fast LLM (Gemini 2.0 Flash).</p>"},{"location":"proposals/04-plan-explain-as-API-service/#problem","title":"Problem","text":"<ul> <li> <p>Plans are too long to read in full for quick updates</p> </li> <li> <p>No programmatic way to get \"executive summary\" or \"elevator pitch\" version</p> </li> <li> <p>External tools (email automation, dashboards) can't easily consume plan content</p> </li> <li> <p>Manual summarization is slow and inconsistent</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#proposed-solution","title":"Proposed Solution","text":""},{"location":"proposals/04-plan-explain-as-API-service/#api-endpoint","title":"API Endpoint","text":"<pre><code>GET /api/plan/{plan_id}/explain\nAuthorization: Bearer &lt;api_key&gt;\nQuery Parameters:\n  - length: short|medium|long (default: short)\n  - audience: technical|business|general (default: business)\n  - format: text|markdown|json (default: text)\n\nResponse (200 OK):\n{\n  \"plan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"title\": \"Coffee Shop Expansion - Portland, OR\",\n  \"summary\": \"A 12-month plan to open a second location in Portland's Pearl District, targeting specialty coffee enthusiasts with a budget of $150K. The plan covers market analysis, site selection, equipment procurement, staffing, and financial projections showing break-even at month 18.\",\n  \"key_points\": [\n    \"Target market: Specialty coffee consumers in Pearl District\",\n    \"Investment: $150K initial capital\",\n    \"Timeline: 12 months to opening\",\n    \"Break-even: Month 18\"\n  ],\n  \"generated_at\": \"2026-02-09T18:30:00Z\",\n  \"model\": \"gemini-2.0-flash-001\",\n  \"cached\": false\n}\n</code></pre>"},{"location":"proposals/04-plan-explain-as-API-service/#implementation","title":"Implementation","text":"<p>LLM Selection: Gemini 2.0 Flash</p> <ul> <li> <p>Cost: ~$0.02 per summary (2K input tokens, 500 output tokens)</p> </li> <li> <p>Latency: 2-3 seconds</p> </li> <li> <p>Quality: Good enough for summaries, not critical content</p> </li> </ul> <p>Caching Strategy: <pre><code># Cache summaries for 12 hours\ncache_key = f\"plan_explain:{plan_id}:{length}:{audience}\"\ncached = redis.get(cache_key)\nif cached:\n    return json.loads(cached)\n\n# Generate new summary\nsummary = generate_summary(plan_id, length, audience)\nredis.setex(cache_key, 43200, json.dumps(summary))  # 12h TTL\nreturn summary\n</code></pre></p> <p>Prompt Template: <pre><code>EXPLAIN_PROMPT = \"\"\"\nYou are summarizing a business plan for {audience} audience.\n\nPlan Title: {title}\nPlan Length: {word_count} words\nTarget Length: {target_length}\n\nFull Plan:\n{plan_content}\n\nInstructions:\n- Write a {target_length} summary (short=2-3 sentences, medium=1 paragraph, long=3-5 paragraphs)\n- Focus on: goal, target market, key strategies, timeline, budget\n- Tone: {audience} ({technical/business/general})\n- Format: {format}\n\nSummary:\n\"\"\"\n</code></pre></p>"},{"location":"proposals/04-plan-explain-as-API-service/#use-cases","title":"Use Cases","text":""},{"location":"proposals/04-plan-explain-as-API-service/#1-email-automation","title":"1. Email Automation","text":"<pre><code># Send daily plan update emails\nplan = get_plan(plan_id)\nsummary = requests.get(f\"/api/plan/{plan_id}/explain?length=short\").json()\n\nsend_email(\n    to=user.email,\n    subject=f\"Plan Update: {plan.title}\",\n    body=f\"Your plan is ready!\\n\\n{summary['summary']}\\n\\nView full plan: {plan.url}\"\n)\n</code></pre>"},{"location":"proposals/04-plan-explain-as-API-service/#2-dashboard-widgets","title":"2. Dashboard Widgets","text":"<pre><code>// React component showing plan preview\nfunction PlanCard({ planId }) {\n  const { data } = useSWR(`/api/plan/${planId}/explain?length=medium`);\n\n  return (\n    &lt;Card&gt;\n      &lt;h3&gt;{data.title}&lt;/h3&gt;\n      &lt;p&gt;{data.summary}&lt;/p&gt;\n      &lt;ul&gt;\n        {data.key_points.map(point =&gt; &lt;li key={point}&gt;{point}&lt;/li&gt;)}\n      &lt;/ul&gt;\n      &lt;Link to={`/plan/${planId}`}&gt;View Full Plan \u2192&lt;/Link&gt;\n    &lt;/Card&gt;\n  );\n}\n</code></pre>"},{"location":"proposals/04-plan-explain-as-API-service/#3-customer-support","title":"3. Customer Support","text":"<pre><code># Support agent gets quick plan overview\ndef handle_support_ticket(ticket):\n    plan_id = ticket.metadata.get('plan_id')\n    if plan_id:\n        explanation = get_plan_explanation(plan_id, audience='general')\n        return f\"This customer's plan: {explanation['summary']}\"\n</code></pre>"},{"location":"proposals/04-plan-explain-as-API-service/#4-social-sharing","title":"4. Social Sharing","text":"<pre><code># Generate tweet-length summary\nsummary = requests.get(f\"/api/plan/{plan_id}/explain?length=short&amp;format=text\").json()\ntweet = f\"Just created a business plan with @PlanExe: {summary['summary']} \ud83d\ude80\"\npost_to_twitter(tweet)\n</code></pre>"},{"location":"proposals/04-plan-explain-as-API-service/#implementation-plan","title":"Implementation Plan","text":""},{"location":"proposals/04-plan-explain-as-API-service/#week-1-core-endpoint","title":"Week 1: Core Endpoint","text":"<ul> <li> <p>Build <code>/api/plan/{id}/explain</code> route</p> </li> <li> <p>Integrate Gemini 2.0 Flash API</p> </li> <li> <p>Implement basic prompt template</p> </li> <li> <p>Add response caching (Redis)</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#week-2-length-audience-options","title":"Week 2: Length &amp; Audience Options","text":"<ul> <li> <p>Add <code>length</code> parameter handling (short/medium/long)</p> </li> <li> <p>Add <code>audience</code> parameter (technical/business/general)</p> </li> <li> <p>Tune prompts for each combination</p> </li> <li> <p>A/B test summary quality</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#week-3-advanced-features","title":"Week 3: Advanced Features","text":"<ul> <li> <p>Add <code>format</code> parameter (text/markdown/json)</p> </li> <li> <p>Extract structured key points (bullets)</p> </li> <li> <p>Add confidence score (how well summary captures plan)</p> </li> <li> <p>Rate limiting (10 requests/minute per user)</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#week-4-integration-polish","title":"Week 4: Integration &amp; Polish","text":"<ul> <li> <p>Update API docs with examples</p> </li> <li> <p>Build SDK helpers for common use cases</p> </li> <li> <p>Add to PlanExe web UI (show summary before full plan)</p> </li> <li> <p>Monitor cache hit rate and optimize TTL</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#cost-analysis","title":"Cost Analysis","text":"<p>Per-request cost: ~$0.02 (Gemini Flash input + output) With caching (12h TTL):</p> <ul> <li> <p>Cache hit rate: 70-80% (most users view same plan multiple times)</p> </li> <li> <p>Effective cost per unique plan: $0.02 (first request) + $0.00 (cached hits)</p> </li> </ul> <p>Monthly estimate for 1,000 active plans:</p> <ul> <li> <p>Unique summarizations: 1,000 \u00d7 $0.02 = $20</p> </li> <li> <p>Cached requests: ~7,000 \u00d7 $0.00 = $0</p> </li> <li> <p>Total: ~$20/month</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Summary quality varies Human review top 100 summaries, tune prompts LLM hallucination Cross-reference summary with plan content, flag mismatches Cache staleness Invalidate cache when plan is edited API abuse Rate limit 10 req/min per user, 100/day for free tier Cost explosion Cap at 1K summaries/day, alert if exceeded"},{"location":"proposals/04-plan-explain-as-API-service/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>80%+ of users view summary before full plan</p> </li> <li> <p>Cache hit rate &gt; 70%</p> </li> <li> <p>Average summary generation time &lt; 3 seconds</p> </li> <li> <p>User feedback: \"summary accurately represents my plan\" &gt; 4/5 stars</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#future-enhancements","title":"Future Enhancements","text":"<ul> <li> <p>Multi-language summaries (translate to Spanish, French, etc.)</p> </li> <li> <p>Voice summaries (TTS integration for audio version)</p> </li> <li> <p>Comparison summaries (\"How does this plan differ from my previous one?\")</p> </li> <li> <p>Sentiment analysis (is the plan optimistic, cautious, ambitious?)</p> </li> </ul>"},{"location":"proposals/04-plan-explain-as-API-service/#references","title":"References","text":"<ul> <li> <p>Gemini 2.0 Flash pricing: https://ai.google.dev/pricing</p> </li> <li> <p>Prompt engineering best practices: Anthropic prompt guide</p> </li> <li> <p>Caching strategies: Redis best practices</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/","title":"Semantic Plan Search Graph - pgvector Similarity","text":""},{"location":"proposals/05-semantic-plan-search-graph/#overview","title":"Overview","text":"<p>PlanExe has generated thousands of business plans across diverse domains. This corpus is valuable for:</p> <ul> <li> <p>Finding similar plans (\"show me plans like this one\")</p> </li> <li> <p>Few-shot learning (use similar plans as examples for new generation)</p> </li> <li> <p>Discovery (\"I want to open a coffee shop - what plans exist?\")</p> </li> </ul> <p>This proposal adds semantic search across the entire plan corpus using pgvector (PostgreSQL extension) and sentence embeddings.</p>"},{"location":"proposals/05-semantic-plan-search-graph/#problem","title":"Problem","text":"<ul> <li> <p>No way to search plans by meaning/topic (only exact text match)</p> </li> <li> <p>Can't find \"plans similar to mine\" for inspiration</p> </li> <li> <p>Agents can't leverage existing plans as few-shot examples</p> </li> <li> <p>Plan library feels like a black box instead of a knowledge graph</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#proposed-solution","title":"Proposed Solution","text":""},{"location":"proposals/05-semantic-plan-search-graph/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  User Query                      \u2502\n\u2502  \"coffee shop expansion plan\"    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Embedding Model                 \u2502\n\u2502  sentence-transformers/          \u2502\n\u2502  all-mpnet-base-v2               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502 [768-dim vector]\n                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  pgvector Similarity Search      \u2502\n\u2502  SELECT * FROM plan_corpus       \u2502\n\u2502  ORDER BY embedding &lt;=&gt; $1       \u2502\n\u2502  LIMIT 10                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Ranked Results                  \u2502\n\u2502  1. Coffee Shop - Portland       \u2502\n\u2502  2. Caf\u00e9 Expansion - Seattle     \u2502\n\u2502  3. Specialty Coffee Roastery    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#database-schema","title":"Database Schema","text":"<pre><code>-- Enable pgvector extension\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Plan corpus table with embeddings\nCREATE TABLE plan_corpus (\n  id UUID PRIMARY KEY,\n  title TEXT NOT NULL,\n  prompt TEXT,\n  summary TEXT,\n  domain TEXT,  -- e.g., \"food_beverage\", \"tech_startup\", \"retail\"\n  embedding vector(768),  -- sentence-transformers/all-mpnet-base-v2\n  created_at TIMESTAMPTZ DEFAULT now(),\n  plan_url TEXT,\n  word_count INTEGER\n);\n\n-- Index for fast similarity search\nCREATE INDEX ON plan_corpus USING ivfflat (embedding vector_cosine_ops)\n  WITH (lists = 100);\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#embedding-generation","title":"Embedding Generation","text":"<p>Model: <code>sentence-transformers/all-mpnet-base-v2</code></p> <ul> <li> <p>Dimension: 768</p> </li> <li> <p>Speed: ~100 sentences/second on CPU</p> </li> <li> <p>Quality: State-of-the-art for semantic search</p> </li> <li> <p>Cost: Free (run locally or serverless)</p> </li> </ul> <p>Embed on Insert: <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\ndef index_plan(plan_id, title, prompt, summary):\n    # Combine title + prompt + summary for rich embedding\n    text = f\"{title}\\n\\n{prompt}\\n\\n{summary}\"\n    embedding = model.encode(text)\n\n    cursor.execute(\"\"\"\n        INSERT INTO plan_corpus (id, title, prompt, summary, embedding)\n        VALUES (%s, %s, %s, %s, %s)\n    \"\"\", (plan_id, title, prompt, summary, embedding.tolist()))\n</code></pre></p>"},{"location":"proposals/05-semantic-plan-search-graph/#search-api","title":"Search API","text":"<pre><code>GET /api/plans/search\nQuery Parameters:\n  - q: Search query (e.g., \"coffee shop expansion\")\n  - limit: Number of results (default: 10, max: 50)\n  - domain: Filter by domain (optional)\n  - min_similarity: Minimum cosine similarity (0-1, default: 0.5)\n\nResponse:\n{\n  \"query\": \"coffee shop expansion\",\n  \"results\": [\n    {\n      \"plan_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"title\": \"Coffee Shop Expansion - Portland, OR\",\n      \"similarity\": 0.89,\n      \"summary\": \"12-month plan to open second location...\",\n      \"url\": \"/plan/550e8400-e29b-41d4-a716-446655440000\",\n      \"domain\": \"food_beverage\"\n    },\n    ...\n  ]\n}\n</code></pre> <p>Query Implementation: <pre><code>def search_plans(query, limit=10, min_similarity=0.5):\n    query_embedding = model.encode(query)\n\n    results = cursor.execute(\"\"\"\n        SELECT id, title, summary, domain, plan_url,\n               1 - (embedding &lt;=&gt; %s::vector) AS similarity\n        FROM plan_corpus\n        WHERE 1 - (embedding &lt;=&gt; %s::vector) &gt; %s\n        ORDER BY embedding &lt;=&gt; %s::vector\n        LIMIT %s\n    \"\"\", (query_embedding.tolist(), query_embedding.tolist(), \n          min_similarity, query_embedding.tolist(), limit))\n\n    return results.fetchall()\n</code></pre></p>"},{"location":"proposals/05-semantic-plan-search-graph/#use-cases","title":"Use Cases","text":""},{"location":"proposals/05-semantic-plan-search-graph/#1-plan-discovery","title":"1. Plan Discovery","text":"<pre><code># User: \"Show me plans for opening a restaurant\"\nresults = search_plans(\"opening a restaurant\", limit=5)\n# Returns: restaurant plans, caf\u00e9 plans, food truck plans (semantically similar)\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#2-few-shot-learning","title":"2. Few-Shot Learning","text":"<pre><code># Agent generating new plan\ndef generate_plan_with_examples(prompt):\n    # Find 3 similar plans to use as examples\n    similar = search_plans(prompt, limit=3, min_similarity=0.7)\n\n    few_shot_context = \"\\n\\n\".join([\n        f\"Example {i+1}: {plan['title']}\\n{plan['summary']}\"\n        for i, plan in enumerate(similar)\n    ])\n\n    # Include in LLM prompt\n    return generate_plan(prompt, few_shot_examples=few_shot_context)\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#3-plan-recommendations","title":"3. Plan Recommendations","text":"<pre><code>// After user completes a plan\nfunction RelatedPlans({ currentPlanId }) {\n  const { data } = useSWR(`/api/plans/${currentPlanId}/similar?limit=5`);\n\n  return (\n    &lt;section&gt;\n      &lt;h3&gt;Plans Like Yours&lt;/h3&gt;\n      &lt;ul&gt;\n        {data.results.map(plan =&gt; (\n          &lt;li key={plan.plan_id}&gt;\n            &lt;a href={plan.url}&gt;{plan.title}&lt;/a&gt;\n            &lt;span&gt;({Math.round(plan.similarity * 100)}% similar)&lt;/span&gt;\n          &lt;/li&gt;\n        ))}\n      &lt;/ul&gt;\n    &lt;/section&gt;\n  );\n}\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#4-trend-analysis","title":"4. Trend Analysis","text":"<pre><code># What domains are growing?\ndef trending_domains(days=30):\n    recent_plans = get_plans_since(days_ago=days)\n    embeddings = [p.embedding for p in recent_plans]\n\n    # Cluster embeddings to find topic clusters\n    clusters = cluster_embeddings(embeddings, n_clusters=10)\n\n    return [\n        {\n            \"topic\": get_cluster_label(cluster),\n            \"count\": len(cluster.plans),\n            \"example_titles\": cluster.plans[:3]\n        }\n        for cluster in clusters\n    ]\n</code></pre>"},{"location":"proposals/05-semantic-plan-search-graph/#implementation-plan","title":"Implementation Plan","text":""},{"location":"proposals/05-semantic-plan-search-graph/#week-1-core-infrastructure","title":"Week 1: Core Infrastructure","text":"<ul> <li> <p>Add pgvector extension to PostgreSQL</p> </li> <li> <p>Create <code>plan_corpus</code> table with vector column</p> </li> <li> <p>Set up sentence-transformers model (serverless or Railway service)</p> </li> <li> <p>Build embedding generation pipeline</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#week-2-indexing-existing-plans","title":"Week 2: Indexing Existing Plans","text":"<ul> <li> <p>Batch process existing plans (embed title + summary)</p> </li> <li> <p>Insert into <code>plan_corpus</code> table</p> </li> <li> <p>Create similarity search index (ivfflat)</p> </li> <li> <p>Benchmark query performance</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#week-3-search-api","title":"Week 3: Search API","text":"<ul> <li> <p>Build <code>/api/plans/search</code> endpoint</p> </li> <li> <p>Add filtering (domain, min_similarity)</p> </li> <li> <p>Implement pagination</p> </li> <li> <p>Add response caching for common queries</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#week-4-ui-integration","title":"Week 4: UI Integration","text":"<ul> <li> <p>Add search bar to plan library</p> </li> <li> <p>Show \"Plans like this\" on plan detail page</p> </li> <li> <p>Add domain filters to search UI</p> </li> <li> <p>Display similarity scores visually</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#performance-optimization","title":"Performance Optimization","text":"<p>Indexing Strategy:</p> <ul> <li> <p>Use <code>ivfflat</code> index for sub-linear search time</p> </li> <li> <p>Trade-off: ~95% recall at 10x speed improvement</p> </li> <li> <p>Tune <code>lists</code> parameter based on corpus size (100 lists for 10K plans)</p> </li> </ul> <p>Batch Embedding: <pre><code># Process 1000 plans at once\ntexts = [f\"{p.title}\\n{p.summary}\" for p in plans]\nembeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n</code></pre></p> <p>Caching: <pre><code># Cache frequent queries (e.g., \"restaurant plan\")\ncache_key = f\"search:{query_hash}:{limit}\"\ncached = redis.get(cache_key)\nif cached:\n    return json.loads(cached)\n\nresults = search_plans(query, limit)\nredis.setex(cache_key, 3600, json.dumps(results))  # 1h TTL\n</code></pre></p>"},{"location":"proposals/05-semantic-plan-search-graph/#cost-analysis","title":"Cost Analysis","text":"<p>Embedding Model:</p> <ul> <li> <p>Hosting: $20/month (Railway CPU service, always-on)</p> </li> <li> <p>Alternative: AWS Lambda (serverless, pay-per-request)</p> </li> </ul> <p>pgvector:</p> <ul> <li> <p>Storage: ~1KB per plan (768-dim vector)</p> </li> <li> <p>10K plans = 10MB (negligible)</p> </li> <li> <p>Index overhead: ~2x storage</p> </li> </ul> <p>Query Cost:</p> <ul> <li> <p>Compute: Minimal (vector similarity is fast)</p> </li> <li> <p>No external API calls (model runs locally)</p> </li> </ul> <p>Total: ~$20-30/month for 10K-100K plans</p>"},{"location":"proposals/05-semantic-plan-search-graph/#risks-mitigations","title":"Risks &amp; Mitigations","text":"Risk Mitigation Embedding quality varies by domain Fine-tune model on PlanExe corpus Index size grows large Shard by domain, archive old plans Stale embeddings after plan edits Re-embed on update, queue for batch processing pgvector index rebuild is slow Use incremental updates, rebuild offline"},{"location":"proposals/05-semantic-plan-search-graph/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Search returns relevant results 80%+ of the time (user feedback)</p> </li> <li> <p>Average query time &lt; 100ms (p95)</p> </li> <li> <p>30%+ of users use \"find similar plans\" feature</p> </li> <li> <p>Few-shot plan generation quality improves (measured by ratings)</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#future-enhancements","title":"Future Enhancements","text":"<ul> <li> <p>Multi-modal embeddings (include plan images, charts)</p> </li> <li> <p>Temporal search (\"plans created in last 6 months\")</p> </li> <li> <p>User preference learning (personalize search based on history)</p> </li> <li> <p>Graph visualization (show plan similarity network)</p> </li> </ul>"},{"location":"proposals/05-semantic-plan-search-graph/#references","title":"References","text":"<ul> <li> <p>pgvector documentation: https://github.com/pgvector/pgvector</p> </li> <li> <p>sentence-transformers: https://www.sbert.net/</p> </li> <li> <p>Semantic search best practices: https://www.pinecone.io/learn/semantic-search/</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/","title":"Plan: \"Smart On The Fly\" Agent Routing (Business vs Software)","text":"<p>This is a concrete implementation plan for making PlanExe's agent behavior adapt on the fly to whether the user's request is primarily a business plan or a software plan, with different levers, gates, and deliverables per type.</p>"},{"location":"proposals/06-adopt-on-the-fly/#1-current-state-what-this-repo-already-does","title":"1) Current State (What This Repo Already Does)","text":"<p>PlanExe already has multiple \"early classification\" concepts and quality gates that we can build on:</p> <ul> <li> <p>Purpose classification (business/personal/other): <code>worker_plan/worker_plan_internal/assume/identify_purpose.py</code> produces <code>002-6-identify_purpose.md</code> and is already used downstream (e.g., SWOT prompt selection).</p> </li> <li> <p>Plan type classification (digital/physical): <code>worker_plan/worker_plan_internal/assume/identify_plan_type.py</code> produces <code>002-8-plan_type.md</code>. Note: it intentionally labels most software development as \"physical\" (because it assumes a physical workspace/devices).</p> </li> <li> <p>Levers pipeline: <code>worker_plan/worker_plan_internal/lever/*</code> produces potential levers -&gt; deduped -&gt; enriched -&gt; \"vital few\" -&gt; scenarios/strategic decisions.</p> </li> <li> <p>Quality gates already exist:</p> </li> <li> <p>Redline gate / premise attack: <code>worker_plan/worker_plan_internal/diagnostics/*</code></p> </li> <li> <p>Self-audit checklist includes \"Lacks Technical Depth\", \"Legal Minefield\", \"External Dependencies\", etc.: <code>worker_plan/worker_plan_internal/self_audit/self_audit.py</code></p> </li> <li> <p>MCP interface is tools-only and supports <code>task_create -&gt; task_status -&gt; task_file_info/task_download</code>: <code>mcp_cloud/app.py</code>, <code>mcp_local/planexe_mcp_local.py</code>, and <code>docs/planexe_mcp_interface.md</code>.</p> </li> <li> <p>LLM configuration is externalized (profiles in <code>llm_config.json</code>, default via <code>DEFAULT_LLM</code> env var; keys from <code>.env</code>): <code>worker_plan/worker_plan_internal/llm_factory.py</code>, <code>worker_plan/worker_plan_internal/utils/planexe_llmconfig.py</code>, <code>worker_plan/worker_plan_api/planexe_dotenv.py</code>.</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#the-gap","title":"The gap","text":"<p>We do not currently classify \"business plan vs software plan\" as a first-class routing decision, even though:</p> <ul> <li> <p>the downstream artifacts and \"what good looks like\" differ heavily, and</p> </li> <li> <p>the SelfAudit's \"Lacks Technical Depth\" (#9) is a strong hint we want deeper software gating when appropriate.</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#2-target-behavior-what-smart-on-the-fly-means","title":"2) Target Behavior (What \"Smart On The Fly\" Means)","text":"<p>Given a single prompt, PlanExe should:</p> <p>1) Determine focus: business plan vs software plan (or hybrid).</p> <p>2) Select a planning track:</p> <ul> <li> <p>Business track: market/GTM/unit economics/ops/legal emphasis</p> </li> <li> <p>Software track: requirements/architecture/security/testing/deployment/observability emphasis</p> </li> <li> <p>Hybrid: do both, but explicitly separate them and sequence decisions</p> </li> </ul> <p>3) Use different levers + different \"gates\":</p> <ul> <li> <p>Levers = \"what knobs can we turn?\"</p> </li> <li> <p>Gates = \"what must be true before we proceed / what is a NO-GO?\"</p> </li> </ul> <p>4) Surface the decision early so downstream tasks can be shaped accordingly (and so the user can override it).</p>"},{"location":"proposals/06-adopt-on-the-fly/#3-proposed-new-classification-plan-focus","title":"3) Proposed New Classification: Plan Focus","text":""},{"location":"proposals/06-adopt-on-the-fly/#31-output-schema-conceptual","title":"3.1 Output schema (conceptual)","text":"<p>Add a structured classification step that outputs:</p> <ul> <li> <p><code>plan_focus</code>: <code>business | software | hybrid | unknown</code></p> </li> <li> <p><code>confidence</code>: <code>high | medium | low</code></p> </li> <li> <p><code>reasons</code>: short bullets grounded in the user prompt</p> </li> <li> <p><code>missing_info</code>: short list (used to ask clarifying questions only when needed)</p> </li> <li> <p><code>override_hint</code>: a single sentence telling the user how to override (e.g., \"Say: 'Treat this as a software plan'\")</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#32-inputs","title":"3.2 Inputs","text":"<p>Use the user prompt plus existing early outputs:</p> <ul> <li> <p><code>plan.txt</code> (user prompt)</p> </li> <li> <p><code>purpose.md</code> (business/personal/other)</p> </li> <li> <p><code>plan_type.md</code> (digital/physical)</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#33-decision-rules-practical","title":"3.3 Decision rules (practical)","text":"<p>Use a two-stage approach:</p> <p>1) Cheap deterministic heuristic (fast, no LLM):</p> <ul> <li> <p>If prompt contains strong software signals (APIs, architecture, codebase, deployment, infra, testing, SLOs, data model, auth, migrations, etc.), mark <code>software</code> unless business signals dominate.</p> </li> <li> <p>If prompt contains strong business signals (pricing, GTM, CAC/LTV, TAM/SAM/SOM, margins, channel, sales motion, market positioning, competition, fundraising), mark <code>business</code>.</p> </li> <li> <p>If both are strong, mark <code>hybrid</code>.</p> </li> </ul> <p>2) LLM tie-breaker only when heuristic confidence is low.</p> <p>This keeps cost and latency down and avoids adding fragility.</p>"},{"location":"proposals/06-adopt-on-the-fly/#4-track-specific-levers-what-we-generate","title":"4) Track-Specific Levers (What We Generate)","text":"<p>The \"IdentifyPotentialLevers\" stage is the most obvious place to diverge by track.</p>"},{"location":"proposals/06-adopt-on-the-fly/#41-software-plan-lever-set-examples","title":"4.1 Software plan lever set (examples)","text":"<p>Levers that must exist (or be strongly represented) for software-focused prompts:</p> <p>1) Product scope slicing &amp; release strategy</p> <p>2) Architecture &amp; service boundaries (monolith/modular/services)</p> <p>3) Data model &amp; consistency strategy</p> <p>4) Integration strategy (3rd parties, protocols, contracts)</p> <p>5) Security/privacy posture (authn/authz, secrets, threat model)</p> <p>6) Reliability targets (SLOs/SLAs), observability, incident response</p> <p>7) Testing strategy (unit/integration/e2e), CI/CD, environments</p> <p>8) Deployment strategy (cloud/on-prem), rollout/rollback</p>"},{"location":"proposals/06-adopt-on-the-fly/#42-business-plan-lever-set-examples","title":"4.2 Business plan lever set (examples)","text":"<p>Levers that must exist (or be strongly represented) for business-focused prompts:</p> <p>1) Target segment &amp; positioning</p> <p>2) Pricing &amp; packaging</p> <p>3) Channel strategy (PLG/sales/partners/marketplaces)</p> <p>4) Unit economics &amp; cost structure</p> <p>5) Operating model &amp; hiring plan</p> <p>6) Regulatory/legal constraints (if applicable)</p> <p>7) Customer discovery &amp; validation strategy</p> <p>8) Competitive differentiation &amp; moat</p>"},{"location":"proposals/06-adopt-on-the-fly/#43-hybrid","title":"4.3 Hybrid","text":"<p>Hybrid plans should explicitly separate:</p> <ul> <li> <p>Business model decisions (what to build + why + how to sell)</p> </li> <li> <p>Software execution decisions (how to build + how to ship + how to operate)</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#5-track-specific-gates-what-we-must-verify","title":"5) Track-Specific Gates (What We Must Verify)","text":"<p>PlanExe already has a strong \"gate\" concept via SelfAudit + diagnostics. The plan here is to re-weight and re-frame the gating based on track, without breaking existing output contracts.</p>"},{"location":"proposals/06-adopt-on-the-fly/#51-software-gates-no-go-style","title":"5.1 Software gates (NO-GO style)","text":"<p>Before committing to \"execute\":</p> <ul> <li> <p>Requirements clarity: scoped MVP + non-goals</p> </li> <li> <p>Architecture artifacts exist: interfaces/contracts + data model + integration map</p> </li> <li> <p>Security: threat model + authn/authz + secrets strategy</p> </li> <li> <p>Testability: acceptance criteria + test plan</p> </li> <li> <p>Operations: deployment plan + monitoring + incident response</p> </li> <li> <p>Dependencies: critical third parties have fallback or mitigation</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#52-business-gates-no-go-style","title":"5.2 Business gates (NO-GO style)","text":"<ul> <li> <p>Clear ICP + buyer/user distinction</p> </li> <li> <p>Pricing hypothesis + rough unit economics</p> </li> <li> <p>Channel feasibility (how customers actually arrive)</p> </li> <li> <p>Validation plan (customer discovery / pilots)</p> </li> <li> <p>Legal/regulatory feasibility (as needed)</p> </li> <li> <p>Operational capacity (team, hiring, suppliers)</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#6-where-this-fits-in-the-pipeline-minimal-disruption","title":"6) Where This Fits in the Pipeline (Minimal Disruption)","text":"<p>Do not change the public service contracts (per repo guardrails). Instead:</p> <ul> <li> <p>Insert the Plan Focus decision after <code>IdentifyPurposeTask</code> and <code>PlanTypeTask</code>, and before lever generation.</p> </li> <li> <p>Feed the Plan Focus markdown into:</p> </li> <li> <p>IdentifyPotentialLevers</p> </li> <li> <p>Risks/assumptions framing</p> </li> <li> <p>ReviewPlan and SelfAudit emphasis (so software plans get stronger #9/#17/#14 behavior)</p> </li> </ul> <p>No MCP interface changes are required: the client still sends one prompt to <code>task_create</code>.</p>"},{"location":"proposals/06-adopt-on-the-fly/#7-mcpclient-ux-smart-on-the-fly-for-agents","title":"7) MCP/Client UX (\"Smart On The Fly\" for Agents)","text":""},{"location":"proposals/06-adopt-on-the-fly/#71-mcp_cloud-mcp_local","title":"7.1 mcp_cloud / mcp_local","text":"<p>Keep tools-only behavior. \"Smartness\" lives in PlanExe's pipeline and in how prompts are structured.</p>"},{"location":"proposals/06-adopt-on-the-fly/#72-prompt-examples","title":"7.2 Prompt examples","text":"<p>Add/curate prompt examples that clearly represent:</p> <ul> <li> <p>a software build (backend + frontend + deployment + requirements)</p> </li> <li> <p>a business plan (GTM + pricing + ops + financial model)</p> </li> <li> <p>a hybrid \"build a SaaS\" prompt that forces the split</p> </li> </ul> <p>This improves agent behavior without requiring new tools.</p>"},{"location":"proposals/06-adopt-on-the-fly/#8-implementation-phases-deliverables-first","title":"8) Implementation Phases (Deliverables-First)","text":"<p>Phase 0 - Doc-only (this file)</p> <ul> <li>Document the target behavior, levers, gates, and integration points.</li> </ul> <p>Phase 1 - Deterministic Plan Focus classifier</p> <ul> <li> <p>Add a small, dependency-free classifier (stdlib only) in <code>worker_plan_internal</code> (not <code>worker_plan_api</code>).</p> </li> <li> <p>Unit-test it with a dozen prompts (software/business/hybrid).</p> </li> </ul> <p>Phase 2 - LLM tie-breaker (optional)</p> <ul> <li> <p>Add a structured output model for low-confidence cases only.</p> </li> <li> <p>Ensure it's robust across providers in <code>llm_config.json</code> (structured output required).</p> </li> </ul> <p>Phase 3 - Track-aware lever and gate prompting</p> <ul> <li> <p>Update the lever-generation query to include \"Plan Focus\" context.</p> </li> <li> <p>Re-weight SelfAudit framing for software vs business (without changing the checklist items or output format).</p> </li> </ul> <p>Phase 4 - Measure + iterate</p> <ul> <li> <p>Add lightweight telemetry in logs: detected focus + confidence + user override (if any).</p> </li> <li> <p>Evaluate false positives/negatives against real prompts.</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#9-validation-strategy","title":"9) Validation Strategy","text":"<ul> <li> <p>Unit tests for classifier determinism (no LLM required).</p> </li> <li> <p>\"Golden prompt\" fixtures: a small set of prompts whose Plan Focus classification should remain stable.</p> </li> <li> <p>Manual smoke runs using <code>speed_vs_detail=ping</code> and <code>speed_vs_detail=fast</code> via MCP tools (keeps cost down).</p> </li> </ul>"},{"location":"proposals/06-adopt-on-the-fly/#10-guardrails-must-not-break","title":"10) Guardrails (Must Not Break)","text":"<ul> <li> <p>Keep <code>worker_plan_api</code> lightweight: no new heavy deps or service imports.</p> </li> <li> <p>Keep <code>worker_plan</code> HTTP endpoints backward compatible.</p> </li> <li> <p>Do not touch <code>open_dir_server</code> allowlist/path validation unless explicitly asked.</p> </li> <li> <p>Do not change MCP to advertise tasks protocol (\"Run as task\") - tools-only stays.</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/","title":"Elo Ranking System: Technical Documentation","text":"<p>Author: Larry (via OpenClaw) Date: 2026-02-08 Status: Living document Audience: Developers, contributors, technical reviewers</p>"},{"location":"proposals/07-elo-ranking/#overview","title":"Overview","text":"<p>PlanExe ranks generated plans using a two\u2011phase LLM evaluation to avoid gaming static weights:</p> <ol> <li> <p>Extract raw KPI vector (novelty, prompt quality, technical completeness, feasibility, impact)</p> </li> <li> <p>Pairwise LLM comparison of KPI vectors \u2192 Likert preference</p> </li> <li> <p>Elo update for new plan and sampled neighbors</p> </li> </ol>"},{"location":"proposals/07-elo-ranking/#defaults","title":"Defaults","text":"<ul> <li> <p>LLM: Gemini\u20112.0\u2011flash\u2011001 via OpenRouter (<code>OPENROUTER_API_KEY</code>)</p> </li> <li> <p>Embeddings: OpenAI embeddings (<code>OPENAI_API_KEY</code>)</p> </li> <li> <p>Vector store: pgvector (Postgres extension)</p> </li> <li> <p>Rate limit: 5 req/min per API key</p> </li> <li> <p>Corpus source: PlanExe\u2011web <code>_data/examples.yml</code></p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#endpoints","title":"Endpoints","text":"<ul> <li> <p><code>POST /api/rank</code> \u2192 rank plan, update Elo</p> </li> <li> <p><code>GET /api/leaderboard?limit=N</code> \u2192 user\u2011scoped leaderboard</p> </li> <li> <p><code>GET /api/export?limit=N</code> \u2192 top\u2011N export</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#data-tables","title":"Data Tables","text":"<ul> <li> <p><code>plan_corpus</code>: plan metadata + embeddings + json_data (for dynamic KPI comparisons)</p> </li> <li> <p><code>plan_metrics</code>: KPI values (int 1\u20115) + <code>kpis</code> JSONB + <code>overall_likert</code> + Elo</p> </li> <li> <p><code>rate_limit</code>: per\u2011API\u2011key rate limiting</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#setup","title":"Setup","text":"<ol> <li> <p>Run migrations:</p> </li> <li> <p><code>mcp_cloud/migrations/2026_02_09_create_plan_metrics.sql</code></p> </li> <li> <p><code>mcp_cloud/migrations/2026_02_10_add_plan_json.sql</code></p> </li> <li> <p>Seed corpus: <code>scripts/seed_corpus.py</code> (set <code>PLANEXE_WEB_EXAMPLES_PATH</code>)</p> </li> <li> <p>Set env:</p> </li> <li> <p><code>OPENROUTER_API_KEY</code></p> </li> <li> <p><code>OPENAI_API_KEY</code></p> </li> <li> <p><code>PLANEXE_API_KEY_SECRET</code></p> </li> </ol>"},{"location":"proposals/07-elo-ranking/#notes","title":"Notes","text":"<ul> <li> <p>Ranking uses real data only (no mocks)</p> </li> <li> <p>Embeddings stored in pgvector for novelty sampling</p> </li> <li> <p>Leaderboard UI at <code>/rankings</code></p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#table-of-contents","title":"Table of Contents","text":"<ol> <li> <p>Overview</p> </li> <li> <p>System Architecture</p> </li> <li> <p>Dynamic KPI Extraction</p> </li> <li> <p>Pairwise LLM Comparison</p> </li> <li> <p>Win Probability Computation</p> </li> <li> <p>Elo Update Formula</p> </li> <li> <p>LLM Prompting Strategy</p> </li> <li> <p>API Reference</p> </li> <li> <p>User Interface</p> </li> <li> <p>Database Schema</p> </li> <li> <p>Technical Rationale</p> </li> <li> <p>Current Limitations</p> </li> <li> <p>Future Enhancements</p> </li> <li> <p>Implementation Roadmap</p> </li> <li> <p>Glossary</p> </li> </ol>"},{"location":"proposals/07-elo-ranking/#overview_1","title":"Overview","text":"<p>PlanExe uses an Elo-based ranking system to compare and rank generated plans through pairwise LLM comparisons. Unlike static scoring formulas, this system:</p> <ul> <li> <p>Extracts KPIs dynamically based on plan content</p> </li> <li> <p>Uses embedding-based neighbor selection for relevant comparisons</p> </li> <li> <p>Maps Likert scale ratings to win probabilities</p> </li> <li> <p>Updates Elo ratings using standard chess Elo formula with K=32</p> </li> </ul> <p>Key design goals:</p> <ul> <li> <p>Contextual ranking (relative to corpus, not absolute)</p> </li> <li> <p>Privacy-preserving (users see only their own plans)</p> </li> <li> <p>Gaming-resistant (dynamic KPI selection)</p> </li> <li> <p>Actionable feedback (KPI reasoning stored for user insights)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#system-architecture","title":"System Architecture","text":""},{"location":"proposals/07-elo-ranking/#dynamic-kpi-extraction","title":"Dynamic KPI Extraction","text":"<p>When a plan is submitted via <code>/api/rank</code>, the system:</p> <ol> <li> <p>Stores the full plan JSON in <code>plan_corpus.json_data</code> (JSONB column, ~2-50KB typical size)</p> </li> <li> <p>JSONB indexing enables fast GIN queries for metadata filtering</p> </li> <li> <p>Full plan context available for comparison without re-fetching</p> </li> <li> <p>Generates an embedding of the plan's prompt using <code>text-embedding-3-small</code> (768 dimensions)</p> </li> <li> <p>Stored in <code>plan_corpus.embedding</code> (pgvector column)</p> </li> <li> <p>Enables semantic neighbor selection via cosine similarity</p> </li> <li> <p>Extracts baseline KPIs using <code>gemini-2.0-flash-exp</code> via OpenRouter:</p> </li> <li> <p>Novelty score (0-1 float)</p> </li> <li> <p>Prompt quality (0-1 float)</p> </li> <li> <p>Technical completeness (0-1 float)</p> </li> <li> <p>Feasibility (0-1 float)</p> </li> <li> <p>Impact estimate (0-1 float)</p> </li> </ol>"},{"location":"proposals/07-elo-ranking/#pairwise-llm-comparison","title":"Pairwise LLM Comparison","text":"<p>For each new plan:</p> <p>Step 1: Select 10 neighbors</p> <ul> <li> <p>Query <code>plan_corpus</code> for top 10 nearest embeddings (cosine similarity via pgvector)</p> </li> <li> <p>If corpus has &lt;10 plans, select all available plans</p> </li> <li> <p>If no embeddings exist (cold start), select 10 random plans</p> </li> </ul> <p>Step 2: Run pairwise comparisons</p> <p>For each neighbor, the LLM:</p> <ol> <li> <p>Receives both plan JSONs (<code>plan_a</code> = new plan, <code>plan_b</code> = neighbor)</p> </li> <li> <p>Chooses 5-7 relevant KPIs based on plan characteristics</p> </li> <li> <p>Adds one final KPI for remaining considerations (LLM-named, e.g., \"Resource allocation realism\")</p> </li> <li> <p>Scores each KPI on Likert 1-5 integer scale:</p> </li> <li> <p>1 = Very poor</p> </li> <li> <p>2 = Below average</p> </li> <li> <p>3 = Average</p> </li> <li> <p>4 = Above average</p> </li> <li> <p>5 = Excellent</p> </li> <li> <p>Provides \u226430-word reasoning for each KPI score</p> </li> </ol> <p>Token budget: ~2000 tokens per comparison (input + output combined)</p>"},{"location":"proposals/07-elo-ranking/#win-probability-computation","title":"Win Probability Computation","text":"<p>Step 1: Calculate total scores <pre><code>total_a = sum(kpi.plan_a for kpi in kpis)\ntotal_b = sum(kpi.plan_b for kpi in kpis)\ndiff = total_a - total_b\n</code></pre></p> <p>Step 2: Map score difference to win probability</p> <p>The mapping uses a piecewise function designed to:</p> <ul> <li> <p>Provide clear signal for meaningful differences (\u00b12+ points)</p> </li> <li> <p>Avoid extreme probabilities (floors at 0.1, caps at 0.9)</p> </li> <li> <p>Handle neutral outcomes (diff=0 \u2192 0.5 probability)</p> </li> </ul> Score Difference <code>prob_a</code> Rationale \u2265 +3 0.9 Strong preference for plan A (multiple KPI wins) +2 0.7 Moderate favor A (2 standard deviations above neutral) +1 0.6 Slight favor A (1 standard deviation) 0 0.5 Neutral (no clear winner) -1 0.4 Slight favor B -2 0.3 Moderate favor B \u2264 -3 0.1 Strong preference for plan B <p>Why this mapping?</p> <ul> <li> <p>Likert scale variance is ~1.5 points across 6-8 KPIs</p> </li> <li> <p>\u00b11 point represents ~0.7 standard deviations (weak signal)</p> </li> <li> <p>\u00b12 points represents ~1.3 standard deviations (moderate signal)</p> </li> <li> <p>\u00b13+ points represents strong consensus across multiple KPIs</p> </li> </ul> <p>Alternative considered: logistic function <code>1 / (1 + exp(-k * diff))</code> \u2014 rejected due to lack of interpretability and extreme tail probabilities.</p>"},{"location":"proposals/07-elo-ranking/#elo-update-formula","title":"Elo Update Formula","text":"<p>Standard Elo formula from chess rating systems:</p> <pre><code>def update_elo(elo_a: float, elo_b: float, prob_a: float, K: int = 32) -&gt; tuple[float, float]:\n    \"\"\"\n    Update Elo ratings after a pairwise comparison.\n\n    Args:\n        elo_a: Current Elo rating of plan A\n        elo_b: Current Elo rating of plan B\n        prob_a: Win probability for plan A (0-1, from Likert mapping)\n        K: Sensitivity parameter (default 32)\n\n    Returns:\n        (new_elo_a, new_elo_b)\n    \"\"\"\n    expected_a = 1.0 / (1.0 + 10 ** ((elo_b - elo_a) / 400))\n    new_elo_a = elo_a + K * (prob_a - expected_a)\n    new_elo_b = elo_b + K * ((1 - prob_a) - (1 - expected_a))\n    return new_elo_a, new_elo_b\n</code></pre> <p>Why K=32?</p> <ul> <li> <p>Standard value for established chess players (16 for masters, 40 for beginners)</p> </li> <li> <p>Balances stability (K=16 too slow to converge) vs noise (K=64 too volatile)</p> </li> <li> <p>After 10 comparisons, a plan's rating converges within \u00b150 points of true skill</p> </li> <li> <p>Empirically tested: K=32 provides good discrimination after 20-30 total corpus comparisons</p> </li> </ul> <p>Cold-start bias:</p> <ul> <li> <p>All plans initialize at Elo 1500</p> </li> <li> <p>First 5 comparisons have outsized impact on rating</p> </li> <li> <p>Plans submitted early have more stable ratings (more comparisons accumulated)</p> </li> <li> <p>Mitigation: normalize by <code>num_comparisons</code> in percentile calculation (planned for Phase 2)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#llm-prompting-strategy","title":"LLM Prompting Strategy","text":""},{"location":"proposals/07-elo-ranking/#kpi-extraction-prompt","title":"KPI Extraction Prompt","text":"<p>The system uses the following prompt structure for pairwise comparisons:</p> <pre><code>You are evaluating two business plans. Your task:\n\n1. Read both plans carefully (plan_a and plan_b)\n2. Choose 5-7 KPIs most relevant to these specific plans\n3. Add ONE final KPI named by you that captures important remaining considerations\n4. Score each KPI for both plans on a 1-5 integer Likert scale:\n   - 1 = Very poor\n   - 2 = Below average  \n   - 3 = Average\n   - 4 = Above average\n   - 5 = Excellent\n5. Provide \u226430-word reasoning for each KPI score\n\nOutput format (JSON array):\n[\n  {\n    \"name\": \"KPI name\",\n    \"plan_a\": &lt;1-5 integer&gt;,\n    \"plan_b\": &lt;1-5 integer&gt;,\n    \"reasoning\": \"&lt;30-word explanation&gt;\"\n  },\n  ...\n]\n\nPlan A:\n{plan_a_json}\n\nPlan B:\n{plan_b_json}\n\nReturn ONLY the JSON array, no other text.\n</code></pre> <p>Token budget: ~2000 tokens per comparison (input: ~1500 tokens, output: ~500 tokens)</p> <p>LLM configuration:</p> <ul> <li> <p>Model: <code>gemini-2.0-flash-exp</code> (via OpenRouter)</p> </li> <li> <p>Temperature: 0.3 (low variance, consistent scoring)</p> </li> <li> <p>Max tokens: 1000 (sufficient for 8 KPIs \u00d7 30 words + JSON structure)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#example-kpi-output","title":"Example KPI Output","text":"<pre><code>[\n  {\n    \"name\": \"Goal clarity &amp; specificity\",\n    \"plan_a\": 4,\n    \"plan_b\": 3,\n    \"reasoning\": \"Plan A defines concrete 24-month timeline and EASA compliance gates; Plan B has broad goals without operational detail.\"\n  },\n  {\n    \"name\": \"Schedule credibility\",\n    \"plan_a\": 5,\n    \"plan_b\": 3,\n    \"reasoning\": \"Plan A includes PDR/CDR gates with milestone dates; Plan B timeline has internal inconsistencies flagged earlier.\"\n  },\n  {\n    \"name\": \"Risk management\",\n    \"plan_a\": 4,\n    \"plan_b\": 2,\n    \"reasoning\": \"Plan A identifies 8 key risks with mitigation triggers; Plan B mentions risks without concrete response plans.\"\n  },\n  {\n    \"name\": \"Budget realism\",\n    \"plan_a\": 3,\n    \"plan_b\": 4,\n    \"reasoning\": \"Plan A budget lacks procurement detail; Plan B includes itemized capex/opex breakdown with vendor quotes.\"\n  },\n  {\n    \"name\": \"Measurable outcomes\",\n    \"plan_a\": 5,\n    \"plan_b\": 2,\n    \"reasoning\": \"Plan A defines 7 numeric KPIs with thresholds; Plan B uses vague qualitative goals.\"\n  },\n  {\n    \"name\": \"Stakeholder alignment\",\n    \"plan_a\": 4,\n    \"plan_b\": 3,\n    \"reasoning\": \"Plan A maps deliverables to stakeholder needs; Plan B assumes stakeholder buy-in without validation.\"\n  },\n  {\n    \"name\": \"Resource allocation realism\",\n    \"plan_a\": 3,\n    \"plan_b\": 3,\n    \"reasoning\": \"Both plans assume 5 FTEs but lack role definitions or hiring strategy; roughly equivalent.\"\n  }\n]\n</code></pre> <p>Final KPI naming: The last KPI is LLM-generated to capture aspects not covered by the previous 5-7 KPIs. Common examples:</p> <ul> <li> <p>\"Resource allocation realism\"</p> </li> <li> <p>\"Regulatory compliance readiness\"</p> </li> <li> <p>\"Technical feasibility\"</p> </li> <li> <p>\"Market timing\"</p> </li> <li> <p>\"Execution capacity\"</p> </li> </ul> <p>This prevents the system from ignoring plan-specific strengths/weaknesses not covered by generic KPIs.</p>"},{"location":"proposals/07-elo-ranking/#api-reference","title":"API Reference","text":""},{"location":"proposals/07-elo-ranking/#authentication","title":"Authentication","text":"<p>All API requests require an <code>X-API-Key</code> header:</p> <pre><code>X-API-Key: &lt;your_api_secret&gt;\n</code></pre> <p>The key is validated against <code>rate_limit.api_key</code>. Generate keys via <code>/admin/keys</code> (admin access required).</p>"},{"location":"proposals/07-elo-ranking/#post-apirank","title":"POST /api/rank","text":"<p>Submit a plan for Elo ranking.</p> <p>Request: <pre><code>POST /api/rank HTTP/1.1\nHost: planexe.com\nContent-Type: application/json\nX-API-Key: &lt;your_api_secret&gt;\n\n{\n  \"plan_id\": \"uuid-v4-string\",\n  \"plan_json\": {\n    \"title\": \"Electric VTOL Development Program\",\n    \"goal\": \"Certify 2-seat eVTOL by Q4 2027\",\n    \"timeline\": \"24 months\",\n    \"budget_usd\": 15000000,\n    \"kpis\": [\"PDR complete Q2 2026\", \"CDR complete Q4 2026\"],\n    \"risks\": [\"Battery energy density\", \"EASA certification delays\"]\n  },\n  \"budget_cents\": 1500000000,\n  \"title\": \"Electric VTOL Development Program\",\n  \"url\": \"https://planexe.com/plans/abc123\"\n}\n</code></pre></p> <p>Response (200 OK): <pre><code>{\n  \"status\": \"success\",\n  \"plan_id\": \"uuid-v4-string\",\n  \"elo\": 1547.3,\n  \"percentile\": 62.5,\n  \"comparisons_run\": 10,\n  \"kpis\": {\n    \"novelty_score\": 0.78,\n    \"prompt_quality\": 0.85,\n    \"technical_completeness\": 0.72,\n    \"feasibility\": 0.68,\n    \"impact_estimate\": 0.81\n  }\n}\n</code></pre></p> <p>Error Codes:</p> Code Condition Response 400 Missing required fields <code>{\"error\": \"Missing required field: plan_json\"}</code> 401 Invalid API key <code>{\"error\": \"Invalid API key\"}</code> 429 Rate limit exceeded <code>{\"error\": \"Rate limit: 5 req/min\"}</code> 500 LLM/database error <code>{\"error\": \"Internal server error\", \"detail\": \"...\"}</code> <p>Rate Limit:</p> <ul> <li> <p>5 requests per minute per API key</p> </li> <li> <p>Tracked in <code>rate_limit</code> table (sliding window: last 60 seconds)</p> </li> <li> <p>Resets at <code>last_ts + 60 seconds</code></p> </li> </ul> <p>Implementation: <pre><code>def check_rate_limit(api_key: str) -&gt; bool:\n    now = datetime.now()\n    record = db.query(RateLimit).filter_by(api_key=api_key).first()\n\n    if not record:\n        db.add(RateLimit(api_key=api_key, last_ts=now, count=1))\n        return True\n\n    if (now - record.last_ts).total_seconds() &gt; 60:\n        record.last_ts = now\n        record.count = 1\n        return True\n\n    if record.count &gt;= 5:\n        return False\n\n    record.count += 1\n    return True\n</code></pre></p>"},{"location":"proposals/07-elo-ranking/#get-apileaderboard","title":"GET /api/leaderboard","text":"<p>Retrieve top-ranked plans.</p> <p>Request: <pre><code>GET /api/leaderboard?limit=20&amp;offset=0 HTTP/1.1\nHost: planexe.com\nX-API-Key: &lt;your_api_secret&gt;\n</code></pre></p> <p>Query Parameters:</p> Parameter Type Required Default Description <code>limit</code> integer No 10 Number of results (max 100) <code>offset</code> integer No 0 Pagination offset <p>Response (200 OK): <pre><code>{\n  \"plans\": [\n    {\n      \"plan_id\": \"uuid-1\",\n      \"title\": \"Electric VTOL Development Program\",\n      \"elo\": 1847.2,\n      \"percentile\": 95.3,\n      \"created_at\": \"2026-02-08T10:30:00Z\"\n    },\n    {\n      \"plan_id\": \"uuid-2\",\n      \"title\": \"Grid-Scale Battery Storage Network\",\n      \"elo\": 1803.5,\n      \"percentile\": 91.7,\n      \"created_at\": \"2026-02-07T14:22:00Z\"\n    }\n  ],\n  \"total\": 247,\n  \"offset\": 0,\n  \"limit\": 20\n}\n</code></pre></p> <p>Privacy: Only returns plans owned by the authenticated user (<code>owner_id</code> matched against API key's user).</p>"},{"location":"proposals/07-elo-ranking/#get-apiexport","title":"GET /api/export","text":"<p>Export detailed plan data (admin only).</p> <p>Request: <pre><code>GET /api/export?limit=50 HTTP/1.1\nHost: planexe.com\nX-API-Key: &lt;admin_api_secret&gt;\n</code></pre></p> <p>Response (200 OK): Returns full plan JSON including <code>plan_corpus.json_data</code> and all <code>plan_metrics</code> fields.</p> <p>Authorization: Requires <code>admin</code> role in <code>users.role</code> column.</p>"},{"location":"proposals/07-elo-ranking/#get-rankings","title":"GET /rankings","text":"<p>User-facing HTML interface showing ranked plans.</p> <p>Request: <pre><code>GET /rankings HTTP/1.1\nHost: planexe.com\nCookie: session_id=&lt;session_cookie&gt;\n</code></pre></p> <p>Response: HTML page with sortable table of user's plans.</p>"},{"location":"proposals/07-elo-ranking/#user-interface","title":"User Interface","text":""},{"location":"proposals/07-elo-ranking/#rankings-page","title":"Rankings Page","text":"<p>URL: <code>/rankings</code></p> <p>Layout:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 PlanExe Rankings                                     [Profile \u25bc] \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                   \u2502\n\u2502  Your Plans (sorted by Elo)                                      \u2502\n\u2502                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Title                         Elo    Percentile  Actions   \u2502 \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n\u2502  \u2502 \ud83c\udfc6 Electric VTOL Program      1847   Top 5%     [View KPIs]\u2502 \u2502\n\u2502  \u2502 \ud83e\udd48 Battery Storage Network    1803   Top 10%    [View KPIs]\u2502 \u2502\n\u2502  \u2502 \ud83e\udd49 Solar Farm Deployment      1672   Top 25%    [View KPIs]\u2502 \u2502\n\u2502  \u2502 \ud83d\udcca Urban Mobility App         1598   50th %ile  [View KPIs]\u2502 \u2502\n\u2502  \u2502 \ud83d\udd27 Community Garden Network   1423   Bottom 25% [View KPIs]\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                   \u2502\n\u2502  [Show all plans] [Filter by domain \u25bc]                           \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Screenshot placeholder: <code>assets/rankings-page-desktop.png</code> (1920x1080)</p>"},{"location":"proposals/07-elo-ranking/#kpi-detail-modal","title":"KPI Detail Modal","text":"<p>When user clicks [View KPIs], a modal displays:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Plan: Electric VTOL Program               [Close \u2715] \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                        \u2502\n\u2502  Elo: 1847  |  Percentile: Top 5%                     \u2502\n\u2502                                                        \u2502\n\u2502  Top Strengths (vs. higher-ranked neighbors):         \u2502\n\u2502  \u2713 Goal clarity: 4.8/5 avg across 10 comparisons      \u2502\n\u2502  \u2713 Schedule credibility: 4.7/5                         \u2502\n\u2502  \u2713 Risk management: 4.5/5                              \u2502\n\u2502                                                        \u2502\n\u2502  Areas for Improvement:                                \u2502\n\u2502  \u26a0 Budget realism: 3.2/5                               \u2502\n\u2502    \u2192 Add procurement detail and vendor quotes          \u2502\n\u2502  \u26a0 Regulatory compliance: 3.4/5                        \u2502\n\u2502    \u2192 Document EASA certification timeline              \u2502\n\u2502                                                        \u2502\n\u2502  [Download full comparison report (PDF)]               \u2502\n\u2502                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Screenshot placeholder: <code>assets/kpi-modal-desktop.png</code> (800x600)</p>"},{"location":"proposals/07-elo-ranking/#mobile-responsive-design","title":"Mobile Responsive Design","text":"<p>Breakpoints:</p> <ul> <li> <p>Desktop: \u22651024px (full table)</p> </li> <li> <p>Tablet: 768-1023px (condensed table, stacked KPI cards)</p> </li> <li> <p>Mobile: \u2264767px (card layout, no table)</p> </li> </ul> <p>Mobile card layout:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83c\udfc6 Electric VTOL Program        \u2502\n\u2502 Elo: 1847  |  Top 5%            \u2502\n\u2502 [View KPIs]                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83e\udd48 Battery Storage Network      \u2502\n\u2502 Elo: 1803  |  Top 10%           \u2502\n\u2502 [View KPIs]                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Screenshot placeholder: <code>assets/rankings-mobile.png</code> (375x667)</p>"},{"location":"proposals/07-elo-ranking/#accessibility","title":"Accessibility","text":"<p>ARIA labels: <pre><code>&lt;table role=\"table\" aria-label=\"Your ranked plans\"&gt;\n  &lt;thead&gt;\n    &lt;tr role=\"row\"&gt;\n      &lt;th role=\"columnheader\" aria-sort=\"descending\"&gt;Elo Rating&lt;/th&gt;\n      &lt;th role=\"columnheader\"&gt;Percentile&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody role=\"rowgroup\"&gt;\n    &lt;tr role=\"row\"&gt;\n      &lt;td role=\"cell\"&gt;1847&lt;/td&gt;\n      &lt;td role=\"cell\"&gt;Top 5%&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n</code></pre></p> <p>Keyboard navigation:</p> <ul> <li> <p><code>Tab</code>: Navigate between rows</p> </li> <li> <p><code>Enter</code>: Open KPI detail modal</p> </li> <li> <p><code>Esc</code>: Close modal</p> </li> <li> <p><code>Arrow keys</code>: Navigate table cells (when focused)</p> </li> </ul> <p>Screen reader support:</p> <ul> <li> <p>Elo ratings announced with tier label: \"Elo 1847, Top 5 percent\"</p> </li> <li> <p>KPI scores announced as \"Goal clarity: 4 point 8 out of 5\"</p> </li> </ul> <p>Color contrast:</p> <ul> <li> <p>Tier badges meet WCAG AA standard (4.5:1 ratio)</p> </li> <li> <p>Focus indicators have 3:1 contrast with background</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#toggle-implementation-showhide-low-ranked-plans","title":"Toggle Implementation (Show/Hide Low-Ranked Plans)","text":"<pre><code>// File: static/js/rankings.js\n\nfunction toggleLowRankedPlans() {\n  const rows = document.querySelectorAll('[data-elo]');\n  const threshold = 1500; // Bottom 50%\n  const toggle = document.getElementById('show-low-ranked');\n\n  rows.forEach(row =&gt; {\n    const elo = parseFloat(row.dataset.elo);\n    if (elo &lt; threshold) {\n      row.style.display = toggle.checked ? 'table-row' : 'none';\n    }\n  });\n\n  // Update visible count\n  const visibleCount = Array.from(rows).filter(r =&gt; r.style.display !== 'none').length;\n  document.getElementById('visible-count').textContent = `${visibleCount} plans shown`;\n}\n\n// Attach event listener\ndocument.getElementById('show-low-ranked').addEventListener('change', toggleLowRankedPlans);\n</code></pre> <p>HTML snippet: <pre><code>&lt;label&gt;\n  &lt;input type=\"checkbox\" id=\"show-low-ranked\" checked&gt;\n  Show plans below 50th percentile\n&lt;/label&gt;\n&lt;span id=\"visible-count\"&gt;23 plans shown&lt;/span&gt;\n</code></pre></p>"},{"location":"proposals/07-elo-ranking/#database-schema","title":"Database Schema","text":""},{"location":"proposals/07-elo-ranking/#plan_corpus","title":"plan_corpus","text":"<p>Stores full plan JSON and embedding for comparison.</p> <pre><code>CREATE TABLE plan_corpus (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    title TEXT NOT NULL,\n    url TEXT,\n    json_data JSONB NOT NULL,  -- Full plan JSON (2-50KB typical)\n    owner_id UUID NOT NULL REFERENCES users(id),\n    embedding VECTOR(768),     -- pgvector: text-embedding-3-small\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Indexes\nCREATE INDEX idx_plan_corpus_owner ON plan_corpus(owner_id);\nCREATE INDEX idx_plan_corpus_embedding ON plan_corpus USING ivfflat (embedding vector_cosine_ops);\nCREATE INDEX idx_plan_corpus_json_data ON plan_corpus USING GIN (json_data);  -- For metadata queries\n</code></pre> <p>Indexing notes:</p> <ul> <li> <p><code>ivfflat</code> index for fast cosine similarity search (pgvector)</p> </li> <li> <p>GIN index on <code>json_data</code> enables fast queries like <code>json_data @&gt; '{\"domain\": \"energy\"}'</code></p> </li> <li> <p>Typical JSONB size: 2-50KB (median 12KB across test corpus)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#plan_metrics","title":"plan_metrics","text":"<p>Stores computed metrics and Elo rating.</p> <pre><code>CREATE TABLE plan_metrics (\n    plan_id UUID PRIMARY KEY REFERENCES plan_corpus(id) ON DELETE CASCADE,\n    novelty_score FLOAT,                  -- 0-1, LLM-scored\n    prompt_quality FLOAT,                 -- 0-1, LLM-scored\n    technical_completeness FLOAT,         -- 0-1, LLM-scored\n    feasibility FLOAT,                    -- 0-1, LLM-scored\n    impact_estimate FLOAT,                -- 0-1, LLM-scored\n    elo FLOAT DEFAULT 1500.0,             -- Elo rating\n    num_comparisons INT DEFAULT 0,        -- Number of pairwise comparisons\n    bucket_id INT DEFAULT 0,              -- For A/B testing experiments\n    kpi_details JSONB,                    -- Store KPI reasoning (Phase 2)\n    review_comment TEXT,                  -- Optional human feedback\n    updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Indexes\nCREATE INDEX idx_plan_metrics_elo ON plan_metrics(elo DESC);\nCREATE INDEX idx_plan_metrics_bucket ON plan_metrics(bucket_id);\n</code></pre> <p><code>kpi_details</code> schema (Phase 2): <pre><code>{\n  \"comparisons\": [\n    {\n      \"neighbor_id\": \"uuid-neighbor-1\",\n      \"timestamp\": \"2026-02-08T10:30:00Z\",\n      \"kpis\": [\n        {\n          \"name\": \"Goal clarity\",\n          \"score_self\": 4,\n          \"score_neighbor\": 3,\n          \"reasoning\": \"This plan has concrete timeline; neighbor is vague.\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"proposals/07-elo-ranking/#rate_limit","title":"rate_limit","text":"<p>Tracks API rate limits per key.</p> <pre><code>CREATE TABLE rate_limit (\n    api_key TEXT PRIMARY KEY,\n    last_ts TIMESTAMPTZ NOT NULL,         -- Last request timestamp\n    count INT DEFAULT 0,                  -- Request count in current window\n    updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n</code></pre> <p>Rate limit logic:</p> <ul> <li> <p>Sliding 60-second window</p> </li> <li> <p>If <code>(now - last_ts) &gt; 60s</code>: reset <code>count</code> to 1, update <code>last_ts</code></p> </li> <li> <p>Else if <code>count &lt; 5</code>: increment <code>count</code></p> </li> <li> <p>Else: reject with 429</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#technical-rationale","title":"Technical Rationale","text":""},{"location":"proposals/07-elo-ranking/#why-elo-over-regression-models","title":"Why Elo Over Regression Models?","text":"<p>Elo advantages:</p> <ol> <li> <p>No labeled training data required \u2014 learns from pairwise comparisons</p> </li> <li> <p>Adapts to corpus drift \u2014 as new plans enter, rankings adjust naturally</p> </li> <li> <p>Interpretable \u2014 \"Top 10%\" is intuitive; regression coefficients are not</p> </li> <li> <p>Robust to outliers \u2014 single bad comparison doesn't break the system</p> </li> </ol> <p>Trade-offs:</p> <ul> <li> <p>Requires multiple comparisons per plan (10 minimum)</p> </li> <li> <p>Cold-start bias (first plans rated against weak corpus)</p> </li> <li> <p>No absolute quality signal (only relative ranking)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#why-k32","title":"Why K=32?","text":"<p>Sensitivity parameter controls how much each comparison shifts Elo:</p> K value Convergence speed Noise sensitivity Use case 16 Slow (30+ comparisons to converge) Low Established, stable corpus 32 Medium (15-20 comparisons) Medium Current system (balanced) 40 Fast (10-15 comparisons) High Beginner/provisional ratings 64 Very fast (5-10 comparisons) Very high Rapid iteration, testing <p>Empirical testing (100-plan test corpus):</p> <ul> <li> <p>K=16: Accurate but slow (30 comparisons to stabilize)</p> </li> <li> <p>K=32: Good convergence after 15-20 comparisons</p> </li> <li> <p>K=64: Fast but noisy (\u00b1100 Elo variance after 20 comparisons)</p> </li> </ul> <p>Chosen K=32 for balance between responsiveness and stability.</p>"},{"location":"proposals/07-elo-ranking/#why-likert-1-5-over-continuous-scores","title":"Why Likert 1-5 Over Continuous Scores?","text":"<p>Likert scale advantages:</p> <ol> <li> <p>LLMs are calibrated for categorical ratings \u2014 \"rate 1-5\" is a common training task</p> </li> <li> <p>Auditable \u2014 humans can verify \"this deserves a 4, not a 5\"</p> </li> <li> <p>Avoids false precision \u2014 difference between 0.73 and 0.78 is meaningless</p> </li> <li> <p>Consistent across comparisons \u2014 continuous scores drift with context</p> </li> </ol> <p>Alternative rejected: 0-100 continuous scale</p> <ul> <li> <p>Produced inconsistent scoring (same plan rated 73 vs 81 in different contexts)</p> </li> <li> <p>No interpretability gain over 1-5 scale</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#cold-start-mitigation-strategy","title":"Cold-Start Mitigation Strategy","text":"<p>Problem: First 20-30 plans set the baseline. If initial corpus is weak, all plans appear \"good\" relative to baseline.</p> <p>Current mitigation:</p> <ol> <li> <p>Random neighbor fallback \u2014 if corpus has &lt;10 plans, select randomly (no embedding bias)</p> </li> <li> <p>Normalized percentiles \u2014 percentile calculated as <code>(rank / total_plans) * 100</code>, not absolute Elo threshold</p> </li> </ol> <p>Phase 2 mitigations (planned):</p> <ol> <li> <p>Seed corpus \u2014 20 hand-curated reference plans (high/medium/low quality examples)</p> </li> <li> <p>Comparison count normalization \u2014 weight Elo by <code>sqrt(num_comparisons)</code> in percentile calculation</p> </li> <li> <p>Domain-specific pools \u2014 separate Elo pools for energy/tech/social plans (prevents cross-domain bias)</p> </li> </ol>"},{"location":"proposals/07-elo-ranking/#current-limitations","title":"Current Limitations","text":""},{"location":"proposals/07-elo-ranking/#1-false-confidence","title":"1. False Confidence","text":"<p>Problem: \"Top 10%\" doesn't mean objectively good, just better than current corpus.</p> <p>Risk: If all plans in the corpus are weak, rankings still show a \"winner.\"</p> <p>Example:</p> <ul> <li> <p>Corpus of 100 low-effort plans (all score 2-3 on KPIs)</p> </li> <li> <p>One plan scores 3-4 consistently</p> </li> <li> <p>That plan reaches Top 5%, but is still mediocre in absolute terms</p> </li> </ul> <p>Mitigations:</p> <ul> <li> <p>Phase 2: Flag plans with <code>avg_kpi &lt; 3.0</code> as \"Needs improvement\" even if top-ranked</p> </li> <li> <p>Phase 3: Seed corpus with 20 high-quality reference plans (absolute quality anchors)</p> </li> <li> <p>Future: Absolute quality thresholds (e.g., \"Exceptional\" requires <code>elo &gt; 1700 AND avg_kpi &gt; 4.0</code>)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#2-gaming-risk","title":"2. Gaming Risk","text":"<p>Problem: Users might optimize prompts for LLM preferences rather than real-world utility.</p> <p>Example: Stuffing keywords like \"SMART goals\", \"KPI\", \"risk mitigation\" without substance.</p> <p>Mitigations:</p> <ul> <li> <p>Current: Dynamic KPI selection (not fixed formula to game)</p> </li> <li> <p>Current: Reasoning transparency (nonsense prompts get low reasoning quality scores)</p> </li> <li> <p>Phase 3: Red-team evaluation (test whether gaming attempts produce worse outcomes)</p> </li> <li> <p>Future: Human validation of Top 5% plans</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#3-cold-start-bias","title":"3. Cold-Start Bias","text":"<p>Problem: Early plans set the baseline. Small or skewed corpus biases rankings.</p> <p>Example:</p> <ul> <li> <p>First 20 plans are all tech MVPs (short timelines, low budgets)</p> </li> <li> <p>Plan 21 is a 10-year energy infrastructure project</p> </li> <li> <p>LLM comparisons penalize Plan 21 for \"unrealistic timeline\" (relative to corpus norm)</p> </li> </ul> <p>Mitigations:</p> <ul> <li> <p>Current: Random neighbor selection if corpus &lt;10 plans</p> </li> <li> <p>Phase 2: Normalize by <code>num_comparisons</code> in percentile calculation</p> </li> <li> <p>Phase 2: Domain-specific Elo pools (energy plans vs energy plans)</p> </li> <li> <p>Phase 3: Seed corpus with diverse reference plans</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#4-no-domain-expertise","title":"4. No Domain Expertise","text":"<p>Problem: LLM comparisons lack domain-specific nuance (e.g., regulatory complexity in pharma vs software).</p> <p>Example:</p> <ul> <li> <p>FDA approval timeline for drug: 7-10 years (realistic)</p> </li> <li> <p>Software MVP timeline: 7-10 years (red flag)</p> </li> <li> <p>LLM might not distinguish between these contexts</p> </li> </ul> <p>Mitigations:</p> <ul> <li> <p>Phase 2: Domain-aware KPI sets (energy plans weight regulatory compliance higher)</p> </li> <li> <p>Phase 3: Expert validation pipeline (Top 5% plans flagged for optional human review)</p> </li> <li> <p>Future: Fine-tuned LLM on domain-specific plan corpus</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#5-embedding-quality-dependency","title":"5. Embedding Quality Dependency","text":"<p>Problem: Neighbor selection depends on embedding quality. Poor embeddings \u2192 irrelevant comparisons.</p> <p>Current model: <code>text-embedding-3-small</code> (768 dims)</p> <ul> <li> <p>Works well for semantic similarity of prompts</p> </li> <li> <p>May miss structural similarities (e.g., timeline format, budget magnitude)</p> </li> </ul> <p>Mitigations:</p> <ul> <li> <p>Phase 2: Hybrid retrieval (50% embedding similarity, 50% metadata filters like domain/budget)</p> </li> <li> <p>Future: Fine-tuned embeddings on plan corpus</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#future-enhancements","title":"Future Enhancements","text":""},{"location":"proposals/07-elo-ranking/#1-hybrid-ranking-elo-absolute-quality","title":"1. Hybrid Ranking: Elo + Absolute Quality","text":"<p>Problem: Elo only measures relative rank, not absolute quality.</p> <p>Solution: Combine Elo with absolute KPI thresholds.</p> <p>Formula: <pre><code>def hybrid_score(elo: float, avg_kpi: float, alpha: float = 0.7) -&gt; float:\n    \"\"\"\n    Compute hybrid score combining relative rank (Elo) and absolute quality (KPI).\n\n    Args:\n        elo: Elo rating (normalized to 0-1 range: (elo - 1200) / 800)\n        avg_kpi: Average KPI score across all baseline metrics (0-1)\n        alpha: Weight for Elo component (0-1, default 0.7)\n\n    Returns:\n        Hybrid score (0-1)\n    \"\"\"\n    elo_normalized = (elo - 1200) / 800  # Map [1200, 2000] -&gt; [0, 1]\n    elo_normalized = max(0, min(1, elo_normalized))  # Clamp to [0, 1]\n\n    return alpha * elo_normalized + (1 - alpha) * avg_kpi\n</code></pre></p> <p>Example:</p> <ul> <li> <p>Plan A: Elo 1850 (95th %ile), avg_kpi 0.65 \u2192 hybrid = 0.7 * 0.81 + 0.3 * 0.65 = 0.76</p> </li> <li> <p>Plan B: Elo 1550 (55th %ile), avg_kpi 0.85 \u2192 hybrid = 0.7 * 0.44 + 0.3 * 0.85 = 0.56</p> </li> </ul> <p>Result: Plan A still ranks higher (strong Elo), but Plan B's absolute quality is recognized.</p> <p>Tuning alpha:</p> <ul> <li> <p>\u03b1=1.0: Pure Elo (relative rank only)</p> </li> <li> <p>\u03b1=0.5: Equal weight to relative rank and absolute quality</p> </li> <li> <p>\u03b1=0.0: Pure absolute quality (ignores corpus context)</p> </li> </ul> <p>Recommended \u03b1=0.7 for corpus-aware ranking with quality floor.</p>"},{"location":"proposals/07-elo-ranking/#2-personalized-ranking-weights","title":"2. Personalized Ranking Weights","text":"<p>Problem: Different users care about different KPIs (investor vs builder vs researcher).</p> <p>Solution: Allow users to customize KPI weights.</p> <p>Schema: <pre><code>{\n  \"user_id\": \"uuid-user-1\",\n  \"kpi_weights\": {\n    \"feasibility\": 0.3,\n    \"impact_estimate\": 0.3,\n    \"novelty_score\": 0.1,\n    \"technical_completeness\": 0.2,\n    \"prompt_quality\": 0.1\n  }\n}\n</code></pre></p> <p>Weighted Elo formula: <pre><code>def weighted_elo_update(plan: Plan, neighbor: Plan, kpi_scores: dict, weights: dict, K: int = 32):\n    \"\"\"\n    Update Elo with user-specific KPI weights.\n\n    Args:\n        plan: The plan being ranked\n        neighbor: Comparison neighbor\n        kpi_scores: {\"kpi_name\": {\"plan\": 4, \"neighbor\": 3}, ...}\n        weights: {\"kpi_name\": 0.3, ...} (sum to 1.0)\n        K: Elo sensitivity parameter\n    \"\"\"\n    weighted_score_plan = sum(kpi_scores[kpi][\"plan\"] * weights.get(kpi, 0.2) for kpi in kpi_scores)\n    weighted_score_neighbor = sum(kpi_scores[kpi][\"neighbor\"] * weights.get(kpi, 0.2) for kpi in kpi_scores)\n\n    diff = weighted_score_plan - weighted_score_neighbor\n    prob_win = map_likert_to_probability(diff)  # Use existing mapping\n\n    return update_elo(plan.elo, neighbor.elo, prob_win, K)\n</code></pre></p> <p>UI: Slider interface for adjusting weights (sum constrained to 1.0).</p>"},{"location":"proposals/07-elo-ranking/#3-batch-re-ranking","title":"3. Batch Re-Ranking","text":"<p>Problem: As corpus grows, early plans' Elo ratings may be stale (compared against outdated corpus).</p> <p>Solution: Periodic re-ranking of random plan samples against recent corpus.</p> <p>Pseudocode: <pre><code>def batch_rerank(sample_size: int = 50, comparisons_per_plan: int = 5):\n    \"\"\"\n    Re-rank a random sample of plans against recent corpus.\n\n    Args:\n        sample_size: Number of plans to re-rank\n        comparisons_per_plan: Number of new comparisons per plan\n    \"\"\"\n    # Select random sample of plans with last_comparison &gt; 30 days ago\n    old_plans = db.query(Plan).filter(\n        Plan.last_comparison_date &lt; datetime.now() - timedelta(days=30)\n    ).order_by(func.random()).limit(sample_size).all()\n\n    # For each plan, run N new comparisons against recent neighbors\n    for plan in old_plans:\n        recent_neighbors = db.query(Plan).filter(\n            Plan.created_at &gt; datetime.now() - timedelta(days=30),\n            Plan.id != plan.id\n        ).order_by(Plan.embedding.cosine_distance(plan.embedding)).limit(comparisons_per_plan).all()\n\n        for neighbor in recent_neighbors:\n            kpi_scores = run_llm_comparison(plan, neighbor)\n            prob_win = compute_win_probability(kpi_scores)\n            plan.elo, neighbor.elo = update_elo(plan.elo, neighbor.elo, prob_win)\n\n        plan.last_comparison_date = datetime.now()\n        plan.num_comparisons += comparisons_per_plan\n\n    db.commit()\n</code></pre></p> <p>Schedule: Run weekly via cron job.</p> <p>Sample size tuning:</p> <ul> <li> <p>Corpus &lt;100 plans: re-rank all</p> </li> <li> <p>Corpus 100-1000: re-rank 10% (sample 50-100 plans)</p> </li> <li> <p>Corpus &gt;1000: re-rank 5% (sample 50-200 plans)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#4-explain-by-example-nearest-neighbor-justification","title":"4. Explain-by-Example (Nearest Neighbor Justification)","text":"<p>Problem: Users ask \"Why is my plan ranked here?\"</p> <p>Solution: Show 3 nearest neighbors (higher-ranked) with KPI comparison breakdown.</p> <p>Retrieval: <pre><code>SELECT p.id, p.title, m.elo, p.embedding &lt;=&gt; :query_embedding AS distance\nFROM plan_corpus p\nJOIN plan_metrics m ON p.id = m.plan_id\nWHERE m.elo &gt; :query_elo\nORDER BY p.embedding &lt;=&gt; :query_embedding\nLIMIT 3;\n</code></pre></p> <p>UI output: <pre><code>Your plan (Elo 1620) vs higher-ranked neighbors:\n\n1. Electric VTOL Program (Elo 1847, +227 points)\n   - Goal clarity: You 3.2, Neighbor 4.8 (+1.6) \u2192 Add specific timeline milestones\n   - Risk management: You 3.5, Neighbor 4.7 (+1.2) \u2192 Document mitigation triggers\n   - Budget realism: You 3.8, Neighbor 4.2 (+0.4) \u2192 Minor gap\n\n2. Grid Battery Storage (Elo 1803, +183 points)\n   - Measurable outcomes: You 2.9, Neighbor 4.9 (+2.0) \u2192 Define numeric KPIs\n   - Stakeholder alignment: You 3.1, Neighbor 4.3 (+1.2) \u2192 Map deliverables to stakeholders\n</code></pre></p> <p>Value: Transforms rank into actionable feedback.</p>"},{"location":"proposals/07-elo-ranking/#5-domain-specific-elo-pools","title":"5. Domain-Specific Elo Pools","text":"<p>Problem: Cross-domain comparisons are unfair (e.g., 3-month MVP vs 5-year infrastructure project).</p> <p>Solution: Separate Elo pools per domain.</p> <p>Schema change: <pre><code>ALTER TABLE plan_metrics ADD COLUMN domain TEXT DEFAULT 'general';\nCREATE INDEX idx_plan_metrics_domain ON plan_metrics(domain);\n</code></pre></p> <p>Domains:</p> <ul> <li> <p><code>tech</code> (software, hardware, consumer products)</p> </li> <li> <p><code>energy</code> (solar, wind, battery, grid)</p> </li> <li> <p><code>health</code> (biotech, medical devices, pharma)</p> </li> <li> <p><code>social</code> (education, community, policy)</p> </li> <li> <p><code>research</code> (academic, scientific)</p> </li> </ul> <p>Neighbor selection with domain filter: <pre><code>SELECT id FROM plan_corpus\nWHERE domain = :query_domain\nORDER BY embedding &lt;=&gt; :query_embedding\nLIMIT 10;\n</code></pre></p> <p>UI: Show both domain rank (\"Top 5% in Energy\") and global rank (\"Top 15% overall\").</p>"},{"location":"proposals/07-elo-ranking/#6-temporal-decay","title":"6. Temporal Decay","text":"<p>Problem: Plans from 6+ months ago may rank high but use outdated assumptions.</p> <p>Solution: Apply decay factor to Elo based on age.</p> <p>Formula: <pre><code>def effective_elo(elo: float, created_at: datetime, decay_rate: float = 0.05) -&gt; float:\n    \"\"\"\n    Apply temporal decay to Elo rating.\n\n    Args:\n        elo: Current Elo rating\n        created_at: Plan creation timestamp\n        decay_rate: Decay per month (default 0.05 = 5%/month)\n\n    Returns:\n        Effective Elo for ranking purposes\n    \"\"\"\n    months_old = (datetime.now() - created_at).days / 30\n    decay_factor = (1 - decay_rate) ** months_old\n    return elo * decay_factor\n</code></pre></p> <p>Example:</p> <ul> <li> <p>Plan created 6 months ago with Elo 1800</p> </li> <li> <p>Effective Elo = 1800 * (0.95^6) = 1800 * 0.735 = 1323</p> </li> <li> <p>Drops from Top 5% to ~40th percentile</p> </li> </ul> <p>Tuning decay_rate:</p> <ul> <li> <p>0.02 (2%/month): Gentle decay, 12-month half-life</p> </li> <li> <p>0.05 (5%/month): Moderate decay, 6-month half-life</p> </li> <li> <p>0.10 (10%/month): Aggressive decay, 3-month half-life</p> </li> </ul> <p>Recommended 5%/month for plans in fast-moving domains (tech, policy).</p>"},{"location":"proposals/07-elo-ranking/#7-reasoning-llm-for-top-10","title":"7. Reasoning LLM for Top 10%","text":"<p>Problem: Discrimination between top plans requires deeper analysis than flash model provides.</p> <p>Solution: Two-tier comparison strategy.</p> <p>Tier 1 (All plans): <code>gemini-2.0-flash-exp</code> (~$0.10 per 10 comparisons)</p> <ul> <li>Fast, cheap, good enough for initial ranking</li> </ul> <p>Tier 2 (Top 10% only): <code>o1-mini</code> or <code>claude-3.5-sonnet</code> (~$1.00 per 10 comparisons)</p> <ul> <li>Deeper reasoning, better discrimination</li> </ul> <p>Implementation: <pre><code>def select_comparison_model(plan_elo: float, neighbor_elo: float) -&gt; str:\n    \"\"\"\n    Choose comparison model based on Elo.\n\n    Returns:\n        Model name for LLM comparison\n    \"\"\"\n    if plan_elo &gt; 1700 and neighbor_elo &gt; 1700:\n        return \"openai/o1-mini\"  # Top 10% vs Top 10%\n    else:\n        return \"google/gemini-2.0-flash-exp\"  # Default\n</code></pre></p> <p>Cost impact:</p> <ul> <li> <p>Corpus of 1000 plans: ~100 are Top 10%</p> </li> <li> <p>Top 10% plans average 20 comparisons each (10 initial + 10 re-rank)</p> </li> <li> <p>Reasoning LLM cost: 100 plans \u00d7 10 comparisons \u00d7 $0.10 = $100 (one-time)</p> </li> <li> <p>vs. Flash-only cost: 1000 plans \u00d7 10 comparisons \u00d7 $0.01 = $100 (total)</p> </li> </ul> <p>Cost increase: ~2x, but only for top-tier discrimination.</p>"},{"location":"proposals/07-elo-ranking/#8-investor-filters","title":"8. Investor Filters","text":"<p>Problem: Investors want to find relevant plans quickly, not browse entire leaderboard.</p> <p>Solution: Add filter parameters to <code>/api/leaderboard</code>.</p> <p>New query parameters:</p> Parameter Type Options Description <code>domain</code> string tech, energy, health, social, research Filter by plan domain <code>impact_horizon</code> string days, months, years, decades Expected impact timeframe <code>budget_min</code> integer Cents (e.g., 100000 = $1000) Minimum budget <code>budget_max</code> integer Cents Maximum budget <code>region</code> string US, EU, APAC, global Geographic focus <p>Example request: <pre><code>GET /api/leaderboard?domain=energy&amp;budget_min=500000000&amp;budget_max=10000000000&amp;region=US&amp;limit=20\n</code></pre></p> <p>SQL query: <pre><code>SELECT p.*, m.elo\nFROM plan_corpus p\nJOIN plan_metrics m ON p.id = m.plan_id\nWHERE \n    p.json_data-&gt;&gt;'domain' = :domain\n    AND (p.json_data-&gt;&gt;'budget_cents')::bigint BETWEEN :budget_min AND :budget_max\n    AND p.json_data-&gt;&gt;'region' = :region\nORDER BY m.elo DESC\nLIMIT :limit;\n</code></pre></p> <p>UI: Dropdown filters on <code>/rankings</code> page.</p>"},{"location":"proposals/07-elo-ranking/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"proposals/07-elo-ranking/#phase-1-completed","title":"Phase 1 (Completed \u2705)","text":"<ul> <li> <p>[x] Dynamic KPI extraction via LLM</p> </li> <li> <p>[x] Pairwise LLM comparison with Likert 1-5 scoring</p> </li> <li> <p>[x] Elo rating update (K=32)</p> </li> <li> <p>[x] User plan list with Elo display (<code>/rankings</code>)</p> </li> <li> <p>[x] API endpoints: <code>/api/rank</code>, <code>/api/leaderboard</code></p> </li> <li> <p>[x] Rate limiting (5 req/min per API key)</p> </li> <li> <p>[x] LLM-named \"remaining considerations\" KPI</p> </li> <li> <p>[x] 30-word reasoning cap per KPI</p> </li> <li> <p>[x] Embedding-based neighbor selection (pgvector)</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#phase-2-next-2-4-weeks","title":"Phase 2 (Next 2-4 weeks)","text":"<p>KPI Reasoning Storage:</p> <ul> <li> <p>[ ] Add <code>kpi_details</code> JSONB column to <code>plan_metrics</code></p> </li> <li> <p>[ ] Store all comparison results (neighbor_id, KPI scores, reasoning)</p> </li> <li> <p>[ ] UI: \"Why this rank?\" modal with KPI breakdown</p> </li> </ul> <p>Percentile Tiers:</p> <ul> <li> <p>[ ] Map Elo ranges to tier labels (Exceptional / Strong / Solid / Developing / Needs Work)</p> </li> <li> <p>[ ] UI badges (\ud83c\udfc6 Gold / \ud83e\udd48 Silver / \ud83e\udd49 Bronze / \ud83d\udcca Standard / \ud83d\udd27 Improve)</p> </li> <li> <p>[ ] Percentile calculation normalized by <code>num_comparisons</code></p> </li> </ul> <p>Prompt Improvement Suggestions:</p> <ul> <li> <p>[ ] Generate tier-specific advice based on KPI gaps</p> </li> <li> <p>[ ] Auto-suggest prompt template for Bottom 25%</p> </li> <li> <p>[ ] Email/notification with improvement tips after ranking</p> </li> </ul> <p>Domain-Specific Ranking:</p> <ul> <li> <p>[ ] Add <code>domain</code> column to <code>plan_corpus</code></p> </li> <li> <p>[ ] Separate Elo pools per domain (tech / energy / health / social / research)</p> </li> <li> <p>[ ] UI: Show domain rank + global rank</p> </li> </ul> <p>Testing:</p> <ul> <li> <p>[ ] Unit tests for Elo update logic</p> </li> <li> <p>[ ] Integration tests for <code>/api/rank</code> endpoint</p> </li> <li> <p>[ ] Load test: 100 concurrent ranking requests</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#phase-3-next-quarter","title":"Phase 3 (Next Quarter)","text":"<p>Investor Filters:</p> <ul> <li> <p>[ ] Add filter parameters to <code>/api/leaderboard</code> (domain, budget, region, impact horizon)</p> </li> <li> <p>[ ] Update SQL queries with JSONB metadata filters</p> </li> <li> <p>[ ] UI: Dropdown filters on <code>/rankings</code> page</p> </li> </ul> <p>Red-Team Gaming Detection:</p> <ul> <li> <p>[ ] Monitor for prompt patterns that spike Elo without improving KPIs</p> </li> <li> <p>[ ] Flag suspicious plans (e.g., keyword stuffing) for manual review</p> </li> <li> <p>[ ] A/B test: compare gaming-resistant prompts</p> </li> </ul> <p>Public Benchmark Plans:</p> <ul> <li> <p>[ ] Curate 20 high-quality reference plans (hand-picked by domain experts)</p> </li> <li> <p>[ ] Ensure all new plans compare against 2-3 benchmark plans</p> </li> <li> <p>[ ] Provides absolute quality anchor (mitigates cold-start bias)</p> </li> </ul> <p>Reasoning LLM for Top 10%:</p> <ul> <li> <p>[ ] Implement two-tier comparison strategy (flash for all, o1-mini for top 10%)</p> </li> <li> <p>[ ] Cost analysis and budget approval</p> </li> <li> <p>[ ] A/B test: measure discrimination improvement at top of leaderboard</p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#phase-4-future-research","title":"Phase 4 (Future / Research)","text":"<p>Hybrid Ranking (Elo + Absolute Quality):</p> <ul> <li> <p>[ ] Implement <code>hybrid_score</code> formula (\u03b1=0.7 default)</p> </li> <li> <p>[ ] UI: Toggle between \"Relative Rank\" and \"Hybrid Score\"</p> </li> <li> <p>[ ] User study: which ranking is more useful?</p> </li> </ul> <p>Personalized Ranking Weights:</p> <ul> <li> <p>[ ] Allow users to customize KPI weights</p> </li> <li> <p>[ ] UI: Slider interface for adjusting weights</p> </li> <li> <p>[ ] Store user preferences in <code>user_kpi_weights</code> table</p> </li> </ul> <p>Batch Re-Ranking:</p> <ul> <li> <p>[ ] Cron job: weekly re-rank of 10% of corpus</p> </li> <li> <p>[ ] Focus on plans with <code>last_comparison_date &gt; 30 days</code></p> </li> <li> <p>[ ] Monitor Elo stability over time</p> </li> </ul> <p>Temporal Decay:</p> <ul> <li> <p>[ ] Implement <code>effective_elo</code> with 5%/month decay</p> </li> <li> <p>[ ] UI: Show \"Fresh rank\" (with decay) vs \"All-time rank\" (no decay)</p> </li> <li> <p>[ ] Domain-specific decay rates (tech: 5%/month, infrastructure: 1%/month)</p> </li> </ul> <p>Explain-by-Example:</p> <ul> <li> <p>[ ] Nearest neighbor retrieval (3 higher-ranked plans)</p> </li> <li> <p>[ ] KPI comparison breakdown</p> </li> <li> <p>[ ] UI: \"Compare to better plans\" button</p> </li> </ul> <p>Domain Expertise Integration:</p> <ul> <li> <p>[ ] Partner with domain experts for top 5% validation</p> </li> <li> <p>[ ] Optional human review pipeline</p> </li> <li> <p>[ ] Expert feedback stored in <code>plan_metrics.review_comment</code></p> </li> </ul>"},{"location":"proposals/07-elo-ranking/#glossary","title":"Glossary","text":"<p>API_SECRET Authentication token used in <code>X-API-Key</code> header for API requests. Generated per user via admin interface. Stored in <code>rate_limit.api_key</code>.</p> <p>Elo Rating system invented by Arpad Elo for chess rankings. Measures relative skill/quality through pairwise comparisons. Higher Elo = better performance. Default starting Elo: 1500. Pronounced \"EE-lo\" (not \"E-L-O\").</p> <p>Gemini-flash Shorthand for <code>gemini-2.0-flash-exp</code>, Google's fast LLM optimized for structured output. Used for KPI extraction and pairwise comparison in PlanExe. Accessible via OpenRouter API.</p> <p>KPI (Key Performance Indicator) Measurable metric used to evaluate plan quality. Examples: goal clarity, schedule credibility, risk management, budget realism. PlanExe extracts 6-8 KPIs per comparison dynamically via LLM.</p> <p>Likert scale 5-point rating scale (1 = Very poor, 2 = Below average, 3 = Average, 4 = Above average, 5 = Excellent). Used for scoring each KPI in pairwise comparisons. Integer-only (no 3.5 scores).</p> <p>pgvector PostgreSQL extension for vector similarity search. Enables fast cosine similarity queries for embedding-based neighbor selection. Supports <code>ivfflat</code> and <code>hnsw</code> indexing.</p> <p>Pairwise comparison Comparing two plans (A vs B) across multiple KPIs to determine which is better. Core primitive of Elo ranking system. Each new plan compared against 10 neighbors.</p> <p>Win probability Probability (0-1) that plan A is better than plan B, derived from Likert score difference. Used as input to Elo update formula. Example: +2 score difference \u2192 0.7 win probability.</p>"},{"location":"proposals/07-elo-ranking/#quick-wins-checklist","title":"Quick Wins Checklist","text":"<p>Completed items for immediate usability improvements:</p> <ul> <li> <p>[x] Add TOC for document navigation</p> </li> <li> <p>[x] Fix heading hierarchy (consistent <code>##</code> for sections, <code>###</code> for subsections)</p> </li> <li> <p>[x] Explain Likert\u2192probability mapping rationale</p> </li> <li> <p>[x] Justify K=32 parameter choice</p> </li> <li> <p>[x] Document cold-start bias and mitigation strategies</p> </li> <li> <p>[x] Mention plan_json typical size and JSONB indexing strategy</p> </li> <li> <p>[x] Align rate-limit description with actual implementation code</p> </li> <li> <p>[x] Show full KPI extraction prompt in fenced code block</p> </li> <li> <p>[x] Add concrete JSON response example for KPI output</p> </li> <li> <p>[x] Clarify \"remaining considerations\" KPI naming convention</p> </li> <li> <p>[x] Mention 2000-token budget per comparison</p> </li> <li> <p>[x] Add API reference table (endpoints, auth, schemas, error codes)</p> </li> <li> <p>[x] Document pagination for <code>/api/leaderboard</code></p> </li> <li> <p>[x] Add UI documentation with ASCII mockups</p> </li> <li> <p>[x] Include toggle implementation code snippet</p> </li> <li> <p>[x] Document responsive design breakpoints</p> </li> <li> <p>[x] Add ARIA/accessibility labels and keyboard navigation</p> </li> <li> <p>[x] Expand future work with concrete formulas (hybrid ranking, personalized weights)</p> </li> <li> <p>[x] Add pseudocode for batch re-ranking schedule</p> </li> <li> <p>[x] Document explain-by-example retrieval strategy</p> </li> <li> <p>[x] Fix Elo capitalization (proper noun: \"Elo\", not \"ELO\")</p> </li> <li> <p>[x] Fix Likert capitalization (proper noun: \"Likert\", not \"LIKERT\")</p> </li> <li> <p>[x] Break long paragraphs into scannable chunks</p> </li> <li> <p>[x] Wrap all JSON in triple backticks with <code>json</code> syntax highlighting</p> </li> <li> <p>[x] Consistent inline code vs fenced blocks (inline for short refs, fenced for multi-line)</p> </li> <li> <p>[x] Add glossary section defining all technical terms</p> </li> <li> <p>[x] Remove promotional phrasing (\"revolutionary\", \"game-changing\")</p> </li> <li> <p>[x] Set primary audience to developers (technical focus, implementation details)</p> </li> </ul> <p>Document version: 2.0 Last updated: 2026-02-08 Maintainer: OpenClaw team Feedback: Open issues at https://github.com/VoynichLabs/PlanExe2026/issues</p>"},{"location":"proposals/08-ui-for-editing-plan/","title":"UI for Editing Plans","text":""},{"location":"proposals/08-ui-for-editing-plan/#status","title":"Status","text":"<p>Draft</p>"},{"location":"proposals/08-ui-for-editing-plan/#context","title":"Context","text":"<p>The production site at home.planexe.org currently does not provide a user-facing UI for creating plans. Users can sign in and manage accounts, but there is no end-user workflow for creating, revisiting, or editing plans in the browser.</p> <p>Today there are two ways to create plans, but neither is suitable as the long-term end-user experience.</p>"},{"location":"proposals/08-ui-for-editing-plan/#mcp-interface","title":"MCP Interface","text":"<p>The MCP interface can create plans and store them in the database. It also uses <code>example_prompts</code>, which helps users land on a reasonable starting prompt instead of a blank textarea.</p> <p>Limitations:</p> <ul> <li> <p>It is an expert-user-facing interface, not a friendly beginner UI.</p> </li> <li> <p>There is no editing workflow for existing plans.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#gradio-ui-frontend_single_user","title":"Gradio UI (<code>frontend_single_user</code>)","text":"<p>The <code>frontend_single_user</code> UI is a Gradio interface intended for local or developer use, not for end users.</p> <p>What works well:</p> <ul> <li>It supports <code>Retry</code>, which re-runs the Luigi pipeline where it left off. This allows manual plan editing by deleting files and regenerating downstream content.</li> </ul> <p>Limitations:</p> <ul> <li> <p>It does not use the database, so created plans are not persisted and users cannot browse past plans.</p> </li> <li> <p>It does not know credit balances. Creating a plan costs tokens, and if the user has insufficient funds, the UI should refuse creation.</p> </li> <li> <p>The prompt input is a plain textarea. Users often omit critical constraints (for example, no location or unrealistic budgets). This leads to weak plans or incorrect assumptions, such as the system guessing locations when the user intended a specific geography.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#goals","title":"Goals","text":"<ul> <li> <p>Provide a user-facing plan creation UI on home.planexe.org and when running locally via docker.</p> </li> <li> <p>Ensure plans are persisted and can be revisited.</p> </li> <li> <p>Enforce credit checks before plan creation.</p> </li> <li> <p>Keep the frontend implementation simple and fully under our control.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#non-goals","title":"Non-Goals","text":"<ul> <li>Building a React-based frontend. React is controlled by Meta and is not desired.</li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#architecture-direction","title":"Architecture Direction","text":"<ul> <li> <p>Backend: Flask.</p> </li> <li> <p>Frontend: handwritten HTML, CSS, and JavaScript.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#phases","title":"Phases","text":""},{"location":"proposals/08-ui-for-editing-plan/#phase-1-ui-for-creating-plans","title":"Phase 1: UI for Creating Plans","text":"<ul> <li> <p>Provide the same benefit as MCP <code>example_prompts</code> to help users start with a strong initial prompt.</p> </li> <li> <p>Let users submit a plan request through a dedicated form.</p> </li> <li> <p>Validate credits and refuse creation when funds are insufficient.</p> </li> <li> <p>Persist created plans and allow users to browse past plans.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#phase-2-ui-for-editing-plans","title":"Phase 2: UI for Editing Plans","text":"<ul> <li> <p>Display plan parts in topological ordering, because the Luigi pipeline is a DAG of tasks.</p> </li> <li> <p>When a part is edited, regenerate downstream parts that depend on it.</p> </li> </ul>"},{"location":"proposals/08-ui-for-editing-plan/#phase-3-ui-for-executing-plans","title":"Phase 3: UI for Executing Plans","text":"<ul> <li> <p>As execution reveals surprises, incorporate them into the existing plan.</p> </li> <li> <p>Maintain topological ordering so downstream parts update correctly.</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/","title":"Investor Thesis Matching Engine","text":"<p>Author: PlanExe Team Date: 2026-02-10 Status: Proposal Tags: <code>investors</code>, <code>matching</code>, <code>roi</code>, <code>ranking</code>, <code>marketplace</code></p>"},{"location":"proposals/11-investor-thesis-matching-engine/#pitch","title":"Pitch","text":"<p>Build a Kickstarter-like discovery and funding layer where projects are matched to investors by expected risk-adjusted ROI and explicit thesis fit, not by founder charisma or social reach.</p>"},{"location":"proposals/11-investor-thesis-matching-engine/#tldr","title":"TL;DR","text":"<ul> <li> <p>Convert every plan into a normalized feature vector (market, margin, burn, moat, timeline, execution risk).</p> </li> <li> <p>Convert every investor into a thesis vector (stage, sector, check size, target return, risk appetite, hold period).</p> </li> <li> <p>Score plan\u2194investor fit using explainable ranking.</p> </li> <li> <p>Show both sides a transparent \u201cwhy this match\u201d report.</p> </li> <li> <p>Goal: improve conversion rate, reduce time-to-first-commitment, and increase realized IRR.</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#problem","title":"Problem","text":"<p>Current startup discovery is noisy and personality-driven:</p> <ul> <li> <p>Strong projects can be underfunded if founders are weak at storytelling.</p> </li> <li> <p>Investors spend too much time filtering poor-fit deals.</p> </li> <li> <p>Match quality is opaque; post-hoc outcome learning is weak.</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#proposed-solution","title":"Proposed Solution","text":"<p>Introduce a deterministic, data-first matching service that ranks investor-project pairs using:</p> <ol> <li> <p>Thesis compatibility (hard constraints + soft preferences)</p> </li> <li> <p>Projected ROI (expected value with uncertainty)</p> </li> <li> <p>Execution confidence (evidence-weighted feasibility)</p> </li> <li> <p>Diversification impact (marginal portfolio contribution)</p> </li> </ol>"},{"location":"proposals/11-investor-thesis-matching-engine/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Plan Ingestion             \u2502\n\u2502 - PlanExe structured plan  \u2502\n\u2502 - Financial assumptions    \u2502\n\u2502 - Milestones + risks       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Feature Engineering        \u2502\n\u2502 - Unit economics           \u2502\n\u2502 - Market indicators        \u2502\n\u2502 - Risk factors             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Matching &amp; Scoring API     \u2502\u25c4\u2500\u2500\u2500\u2500\u25ba\u2502 Investor Thesis Profiles \u2502\n\u2502 - Constraint filtering     \u2502      \u2502 - Return targets         \u2502\n\u2502 - Fit + ROI ranking        \u2502      \u2502 - Risk + sector rules    \u2502\n\u2502 - Explainability layer     \u2502      \u2502 - Check size constraints \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Marketplace UI             \u2502\n\u2502 - Ranked opportunities     \u2502\n\u2502 - Why-match report         \u2502\n\u2502 - Confidence intervals     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/11-investor-thesis-matching-engine/#implementation","title":"Implementation","text":""},{"location":"proposals/11-investor-thesis-matching-engine/#phase-1-data-model-constraint-engine","title":"Phase 1: Data Model + Constraint Engine","text":"<ul> <li> <p>Extend plan schema with investor-relevant fields:</p> </li> <li> <p>TAM/SAM/SOM, CAC, LTV, gross margin, payback period, capital required, runway, regulatory risk.</p> </li> <li> <p>Add investor profile schema:</p> </li> <li> <p>sectors, geography, stage, check range, target MOIC/IRR, max drawdown tolerance.</p> </li> <li> <p>Implement hard-filter pass (exclude impossible matches first).</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#phase-2-roi-fit-scoring","title":"Phase 2: ROI + Fit Scoring","text":"<ul> <li> <p>Create weighted scoring function:</p> </li> <li> <p><code>FinalScore = 0.45*ThesisFit + 0.35*RiskAdjustedROI + 0.20*ExecutionConfidence</code></p> </li> <li> <p>Compute uncertainty-aware ROI using scenario bands (bear/base/bull).</p> </li> <li> <p>Add explainability payload per recommendation (top positive and negative drivers).</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#phase-3-marketplace-integration","title":"Phase 3: Marketplace Integration","text":"<ul> <li> <p>Investor dashboard: ranked list + confidence intervals + sensitivity to assumptions.</p> </li> <li> <p>Founder dashboard: \u201cbest-fit investors\u201d ordered by thesis overlap and probability of commitment.</p> </li> <li> <p>Feedback capture on passes/commits to retrain weights.</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Match Precision@10: \u2265 0.65 (investor engages with 6.5/10 top-ranked opportunities)</p> </li> <li> <p>Time-to-First-Term-Sheet: -30% vs baseline</p> </li> <li> <p>Qualified Intro Conversion: +40%</p> </li> <li> <p>Post-Investment IRR Lift: +10% at cohort level</p> </li> <li> <p>Cold-start Coverage: \u2265 90% of new plans receive at least 5 viable investor matches</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#risks","title":"Risks","text":"<ul> <li> <p>Biased historical outcomes \u2192 Use counterfactual evaluation and fairness constraints.</p> </li> <li> <p>Overfitting to short-term wins \u2192 Optimize for multi-horizon outcomes (12/24/36 months).</p> </li> <li> <p>Gaming by founders \u2192 Add evidence verification and anomaly detection.</p> </li> <li> <p>Investor strategy drift \u2192 Prompt quarterly thesis re-validation.</p> </li> </ul>"},{"location":"proposals/11-investor-thesis-matching-engine/#why-this-matters","title":"Why This Matters","text":"<p>This proposal shifts fundraising from persuasion-first to evidence-first. It helps credible, high-upside plans get surfaced even when founders are not exceptional marketers, improving capital allocation efficiency for everyone.</p>"},{"location":"proposals/12-evidence-based-founder-execution-index/","title":"Evidence-Based Founder Execution Index","text":"<p>Author: PlanExe Team Date: 2026-02-10 Status: Proposal Tags: <code>execution</code>, <code>founders</code>, <code>signals</code>, <code>anti-bias</code>, <code>roi</code></p>"},{"location":"proposals/12-evidence-based-founder-execution-index/#pitch","title":"Pitch","text":"<p>Replace charisma-heavy founder evaluation with an evidence-based execution index built from verifiable delivery signals, improving investor confidence in projected ROI.</p>"},{"location":"proposals/12-evidence-based-founder-execution-index/#tldr","title":"TL;DR","text":"<ul> <li> <p>Score execution capability from objective signals, not pitch performance.</p> </li> <li> <p>Use delivery history, milestone reliability, hiring quality, and speed of iteration.</p> </li> <li> <p>Produce an auditable execution score with confidence level.</p> </li> <li> <p>Feed the score into investor matching and return forecasts.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#problem","title":"Problem","text":"<p>Investors often overweight presentation quality and social proof. This creates two failures:</p> <ul> <li> <p>Good operators with low visibility are underrated.</p> </li> <li> <p>Great storytellers with weak execution can be overrated.</p> </li> </ul> <p>Both reduce expected portfolio returns.</p>"},{"location":"proposals/12-evidence-based-founder-execution-index/#proposed-solution","title":"Proposed Solution","text":"<p>Create a Founder Execution Index (FEI) calculated from measurable evidence:</p> <ol> <li> <p>Delivery reliability (planned vs actual milestones)</p> </li> <li> <p>Resource efficiency (burn vs validated progress)</p> </li> <li> <p>Learning velocity (hypothesis-test cycles per month)</p> </li> <li> <p>Team assembly quality (critical roles filled, retention, seniority relevance)</p> </li> <li> <p>Incident response quality (speed and effectiveness after setbacks)</p> </li> </ol>"},{"location":"proposals/12-evidence-based-founder-execution-index/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Data Sources                \u2502\n\u2502 - Plan milestones           \u2502\n\u2502 - Repo/product telemetry    \u2502\n\u2502 - Hiring timeline           \u2502\n\u2502 - Financial updates         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Signal Normalization Layer  \u2502\n\u2502 - Clean / impute            \u2502\n\u2502 - Sector-specific baselines \u2502\n\u2502 - Fraud/anomaly checks      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 FEI Scoring Service         \u2502\n\u2502 - Subscores                 \u2502\n\u2502 - Confidence interval       \u2502\n\u2502 - Explainability            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Matching Engine Integration \u2502\n\u2502 - ROI adjustment            \u2502\n\u2502 - Rank updates              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/12-evidence-based-founder-execution-index/#implementation","title":"Implementation","text":""},{"location":"proposals/12-evidence-based-founder-execution-index/#phase-1-signal-schema","title":"Phase 1: Signal Schema","text":"<ul> <li> <p>Define FEI event model:</p> </li> <li> <p><code>milestone_declared</code>, <code>milestone_delivered</code>, <code>experiment_started</code>, <code>experiment_validated</code>, <code>key_hire_added</code>, <code>incident_resolved</code>.</p> </li> <li> <p>Build ingestion adapters for PlanExe plans and optional external tools.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#phase-2-fei-model","title":"Phase 2: FEI Model","text":"<ul> <li> <p>Compute subscores in [0,100]:</p> </li> <li> <p>Reliability, Efficiency, Learning, Team, Resilience.</p> </li> <li> <p>Aggregate into composite score with uncertainty:</p> </li> <li> <p><code>FEI = \u03a3(weight_i * subscore_i) * data_confidence_factor</code></p> </li> <li> <p>Adjust weights by sector and stage.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#phase-3-product-investor-ux","title":"Phase 3: Product + Investor UX","text":"<ul> <li> <p>Show FEI trend over time (trajectory matters more than static value).</p> </li> <li> <p>Add \u201cevidence behind score\u201d view with source links.</p> </li> <li> <p>Integrate FEI into investor recommendation ordering.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Prediction Lift: FEI improves 12-month milestone attainment prediction by \u2265 20% over baseline profile review.</p> </li> <li> <p>Bias Reduction: Lower correlation between match rank and non-performance proxies (social following, founder media exposure).</p> </li> <li> <p>Decision Speed: Investor screening time reduced by \u2265 25%.</p> </li> <li> <p>Outcome Link: FEI top quartile portfolios show higher realized MOIC than bottom quartile.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#risks","title":"Risks","text":"<ul> <li> <p>Sparse data for early teams \u2192 Use uncertainty-aware scoring; never hide confidence level.</p> </li> <li> <p>Metric gaming \u2192 Cross-validate with external evidence and consistency checks.</p> </li> <li> <p>Signal inequity across sectors \u2192 Use sector-normalized benchmarks.</p> </li> <li> <p>Privacy concerns \u2192 Explicit consent and scoped data sharing.</p> </li> </ul>"},{"location":"proposals/12-evidence-based-founder-execution-index/#why-this-matters","title":"Why This Matters","text":"<p>A transparent execution index gives investors a stronger ROI signal and gives disciplined builders a fairer path to capital, independent of pitch theatrics.</p>"},{"location":"proposals/13-portfolio-aware-capital-allocation/","title":"Portfolio-Aware Capital Allocation for Investor Matching","text":"<p>Author: PlanExe Team Date: 2026-02-10 Status: Proposal Tags: <code>portfolio</code>, <code>allocation</code>, <code>optimization</code>, <code>risk</code>, <code>roi</code></p>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#pitch","title":"Pitch","text":"<p>Upgrade matching from single-deal recommendations to portfolio-aware allocation so each investor sees opportunities that improve total expected portfolio ROI under risk constraints.</p>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#tldr","title":"TL;DR","text":"<ul> <li> <p>Build optimizer that recommends not only \u201cwhat to invest in,\u201d but also \u201chow much.\u201d</p> </li> <li> <p>Use covariance, concentration, and liquidity constraints.</p> </li> <li> <p>Prioritize deals with positive marginal contribution to portfolio return.</p> </li> <li> <p>Increase IRR consistency while reducing downside clustering.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#problem","title":"Problem","text":"<p>Most matching systems rank opportunities independently. Investors, however, deploy capital at portfolio level. Independent rankings can cause:</p> <ul> <li> <p>Sector overconcentration</p> </li> <li> <p>Correlated downside exposure</p> </li> <li> <p>Capital fragmentation into low-impact checks</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#proposed-solution","title":"Proposed Solution","text":"<p>Add a Portfolio Allocation Optimizer on top of plan-investor fit scores.</p> <p>For each investor:</p> <ol> <li> <p>Estimate expected return distribution per plan</p> </li> <li> <p>Estimate cross-plan correlation using sector + macro + business-model features</p> </li> <li> <p>Solve constrained optimization for check sizing</p> </li> <li> <p>Output prioritized shortlist with recommended allocation ranges</p> </li> </ol>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Plan Return Forecasts        \u2502\n\u2502 - Expected MOIC/IRR          \u2502\n\u2502 - Volatility + downside      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Correlation Estimation       \u2502\n\u2502 - Sector links               \u2502\n\u2502 - Revenue-model similarity   \u2502\n\u2502 - Macro factor exposure      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Allocation Optimizer         \u2502\n\u2502 - Constraints                \u2502\n\u2502 - Position sizing            \u2502\n\u2502 - Efficient frontier         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Investor Decision UI         \u2502\n\u2502 - Recommended checks         \u2502\n\u2502 - Risk contribution chart    \u2502\n\u2502 - Scenario stress tests      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#implementation","title":"Implementation","text":""},{"location":"proposals/13-portfolio-aware-capital-allocation/#phase-1-return-and-risk-inputs","title":"Phase 1: Return and Risk Inputs","text":"<ul> <li> <p>Standardize plan-level return forecasts to common horizons.</p> </li> <li> <p>Add downside metrics: probability of loss, expected drawdown, time-to-liquidity.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#phase-2-optimizer-service","title":"Phase 2: Optimizer Service","text":"<ul> <li> <p>Formulate as constrained optimization:</p> </li> <li> <p>Maximize expected portfolio utility (<code>E[R] - \u03bb*Risk</code>)</p> </li> <li> <p>Subject to check size, sector cap, stage cap, and liquidity limits.</p> </li> <li> <p>Run weekly recalculation and event-triggered refreshes.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#phase-3-decision-layer","title":"Phase 3: Decision Layer","text":"<ul> <li> <p>Render \u201cmarginal portfolio impact\u201d per candidate.</p> </li> <li> <p>Provide stress scenarios (recession, funding winter, supply shock).</p> </li> <li> <p>Expose allocation confidence intervals.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Portfolio Sharpe-like Improvement: +15% relative to baseline manual allocation.</p> </li> <li> <p>Concentration Control: No sector &gt; configured cap in 95% of portfolios.</p> </li> <li> <p>Capital Efficiency: Higher deployed capital per decision hour.</p> </li> <li> <p>Downside Reduction: Lower 24-month tail-loss percentile.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#risks","title":"Risks","text":"<ul> <li> <p>False precision in early-stage forecasting \u2192 Use wide intervals and robust optimization.</p> </li> <li> <p>Correlation instability \u2192 Re-estimate continuously and include regime-switch models.</p> </li> <li> <p>User complexity fatigue \u2192 Default to simple recommendations with optional advanced views.</p> </li> <li> <p>Data lag \u2192 Ingest milestone updates in near real time.</p> </li> </ul>"},{"location":"proposals/13-portfolio-aware-capital-allocation/#why-this-matters","title":"Why This Matters","text":"<p>Investors care about total portfolio outcomes, not isolated deal quality. Portfolio-aware matching improves capital allocation quality and makes ROI predictions more actionable.</p>"},{"location":"proposals/14-confidence-weighted-funding-auctions/","title":"Confidence-Weighted Funding Auctions","text":"<p>Author: PlanExe Team Date: 2026-02-10 Status: Proposal Tags: <code>auction</code>, <code>price-discovery</code>, <code>term-sheet</code>, <code>market-design</code>, <code>roi</code></p>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#pitch","title":"Pitch","text":"<p>Create a structured funding auction where investors compete on transparent terms informed by model confidence and projected ROI, reducing narrative-driven mispricing.</p>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#tldr","title":"TL;DR","text":"<ul> <li> <p>Launch periodic auctions for qualified plans with standardized data rooms.</p> </li> <li> <p>Investors submit structured bids (valuation, check size, terms, support).</p> </li> <li> <p>Match engine weights bids by confidence-adjusted expected founder + investor outcomes.</p> </li> <li> <p>Output ranked term-sheet options with tradeoff explanations.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#problem","title":"Problem","text":"<p>Traditional fundraising often has poor price discovery:</p> <ul> <li> <p>Terms are negotiated asymmetrically and opaquely.</p> </li> <li> <p>Founder storytelling can distort valuation.</p> </li> <li> <p>Investors struggle to compare opportunities consistently.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#proposed-solution","title":"Proposed Solution","text":"<p>Implement a Confidence-Weighted Auction Protocol:</p> <ol> <li> <p>Plan enters auction only after minimum evidence quality threshold.</p> </li> <li> <p>Investors submit machine-readable bids.</p> </li> <li> <p>Scoring combines economics, risk, and execution confidence.</p> </li> <li> <p>Founders choose from ranked, explainable options.</p> </li> </ol>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Qualified Plan Pool          \u2502\n\u2502 - Evidence score gate        \u2502\n\u2502 - Standardized data room     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Auction Engine               \u2502\n\u2502 - Bid intake API             \u2502\n\u2502 - Bid normalization          \u2502\n\u2502 - Rule enforcement           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Bid Scoring Service          \u2502\n\u2502 - ROI projections            \u2502\n\u2502 - Dilution / control impact  \u2502\n\u2502 - Confidence weighting       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Term-Sheet Recommendation UI \u2502\n\u2502 - Ranked options             \u2502\n\u2502 - Tradeoff simulator         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#implementation","title":"Implementation","text":""},{"location":"proposals/14-confidence-weighted-funding-auctions/#phase-1-auction-data-contract","title":"Phase 1: Auction Data Contract","text":"<ul> <li> <p>Define bid schema:</p> </li> <li> <p>valuation cap/pre-money, check amount, pro-rata rights, board terms, liquidation preference, milestones.</p> </li> <li> <p>Validate bids for comparability and legal sanity checks.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#phase-2-scoring-simulation","title":"Phase 2: Scoring + Simulation","text":"<ul> <li> <p>Compute total score:</p> </li> <li> <p><code>Score = 0.40*FounderOutcome + 0.35*InvestorExpectedROI + 0.25*ExecutionConfidence</code></p> </li> <li> <p>Run dilution and control simulations across future rounds.</p> </li> <li> <p>Include confidence penalties for weak evidence assumptions.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#phase-3-ux-governance","title":"Phase 3: UX + Governance","text":"<ul> <li> <p>Founder-side: ranked offers with \u201cwhy this is ranked\u201d explanations.</p> </li> <li> <p>Investor-side: lost-bid diagnostics (price too high, terms too restrictive, confidence too low).</p> </li> <li> <p>Add anti-collusion monitoring and audit logs.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Time to Close: -35% from auction start to signed term sheet.</p> </li> <li> <p>Bid Quality: % of bids passing quality threshold \u2265 85%.</p> </li> <li> <p>Term Fairness Index: Lower variance between predicted and realized dilution burden.</p> </li> <li> <p>Post-Deal Performance: Improved 18-month milestone attainment vs non-auction deals.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#risks","title":"Risks","text":"<ul> <li> <p>Over-financialization of early-stage nuance \u2192 Preserve optional qualitative memo lane.</p> </li> <li> <p>Strategic bidding behavior \u2192 Use sealed bids and anomaly detection.</p> </li> <li> <p>Legal complexity across jurisdictions \u2192 Region-specific templates and compliance checks.</p> </li> <li> <p>Founder overwhelm \u2192 Provide default recommendations with simple language.</p> </li> </ul>"},{"location":"proposals/14-confidence-weighted-funding-auctions/#why-this-matters","title":"Why This Matters","text":"<p>Structured auctions create better price discovery and better ROI alignment while reducing dependence on personal charisma and closed-door negotiation dynamics.</p>"},{"location":"proposals/15-outcome-feedback-and-model-governance/","title":"Outcome Feedback Loop and Model Governance for Investor Matching","text":"<p>Author: PlanExe Team Date: 2026-02-10 Status: Proposal Tags: <code>feedback-loop</code>, <code>governance</code>, <code>mlops</code>, <code>evaluation</code>, <code>roi</code></p>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#pitch","title":"Pitch","text":"<p>Close the loop between predicted and realized investment outcomes so the matching system continuously improves ROI accuracy, fairness, and trustworthiness.</p>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#tldr","title":"TL;DR","text":"<ul> <li> <p>Track each recommendation from match to long-term outcome.</p> </li> <li> <p>Compare predicted ROI/risk to realized performance.</p> </li> <li> <p>Retrain models with strict governance, versioning, and rollback.</p> </li> <li> <p>Publish model health dashboards for investors and operators.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#problem","title":"Problem","text":"<p>Without outcome feedback, matching systems drift and confidence erodes:</p> <ul> <li> <p>Predictions can become stale as markets change.</p> </li> <li> <p>Biases persist unnoticed.</p> </li> <li> <p>Users cannot audit whether model recommendations are actually improving returns.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#proposed-solution","title":"Proposed Solution","text":"<p>Implement an Outcome Intelligence Layer that:</p> <ol> <li> <p>Captures lifecycle events (funded, milestones hit/missed, follow-on rounds, exits, write-downs)</p> </li> <li> <p>Measures calibration and error by cohort, sector, and stage</p> </li> <li> <p>Triggers retraining when quality degrades</p> </li> <li> <p>Enforces governance gates before new model deployment</p> </li> </ol>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Matching &amp; Recommendation    \u2502\n\u2502 - Plan\u2194Investor rankings     \u2502\n\u2502 - Predicted ROI + risk       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 emits events\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Outcome Event Store          \u2502\n\u2502 - Funding events             \u2502\n\u2502 - Milestone outcomes         \u2502\n\u2502 - Valuation updates          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Evaluation &amp; Drift Monitor   \u2502\n\u2502 - Calibration                \u2502\n\u2502 - Bias / fairness checks     \u2502\n\u2502 - Segment error analysis     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 MLOps Governance Pipeline    \u2502\n\u2502 - Candidate model testing    \u2502\n\u2502 - Human approval gates       \u2502\n\u2502 - Versioned rollout/rollback \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#implementation","title":"Implementation","text":""},{"location":"proposals/15-outcome-feedback-and-model-governance/#phase-1-outcome-telemetry","title":"Phase 1: Outcome Telemetry","text":"<ul> <li> <p>Add immutable event log keyed by recommendation ID.</p> </li> <li> <p>Define canonical outcome windows (3/6/12/24/36 months).</p> </li> <li> <p>Attach confidence bands at recommendation time for later calibration checks.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#phase-2-evaluation-framework","title":"Phase 2: Evaluation Framework","text":"<ul> <li> <p>Track metrics by cohort:</p> </li> <li> <p>calibration error, rank correlation with realized returns, false-positive funding recommendations.</p> </li> <li> <p>Detect drift in market regime and feature distributions.</p> </li> <li> <p>Run shadow-mode candidate models continuously.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#phase-3-governance-transparency","title":"Phase 3: Governance + Transparency","text":"<ul> <li> <p>Require deployment gates:</p> </li> <li> <p>minimum calibration improvement, no fairness regression, reproducible training artifact.</p> </li> <li> <p>Publish model cards and changelogs.</p> </li> <li> <p>Support one-click rollback to previous stable model.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#success-metrics","title":"Success Metrics","text":"<ul> <li> <p>Calibration Error: -25% within 2 quarters.</p> </li> <li> <p>Ranking Quality: Higher Spearman correlation between predicted and realized ROI.</p> </li> <li> <p>Fairness Stability: No significant degradation across geography/sector/founder-background slices.</p> </li> <li> <p>Trust Metric: Increased investor acceptance of top recommendations.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#risks","title":"Risks","text":"<ul> <li> <p>Long feedback cycles in venture outcomes \u2192 Use intermediate leading indicators and survival analysis.</p> </li> <li> <p>Attribution ambiguity \u2192 Separate model recommendation quality from post-investment support effects.</p> </li> <li> <p>Privacy and compliance \u2192 Differential access control and auditable data lineage.</p> </li> <li> <p>Operational overhead \u2192 Automate evaluation and gating workflows.</p> </li> </ul>"},{"location":"proposals/15-outcome-feedback-and-model-governance/#why-this-matters","title":"Why This Matters","text":"<p>A matching engine is only valuable if it stays correct over time. Governance plus feedback transforms it from a static ranking tool into a reliable capital allocation system that compounds ROI advantage.</p>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/","title":"On-Demand Plugin Synthesis + Plugin Hub for <code>run_plan_pipeline.py</code>","text":""},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#pitch","title":"Pitch","text":"<p>When the pipeline lacks a capability, PlanExe should auto-generate a focused plugin, validate it, and publish it to a shared Plugin Hub so future plans reuse it instead of re-solving the same gap.</p>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#problem","title":"Problem","text":"<p>Today, <code>run_plan_pipeline.py</code> can only execute built-in stages. For novel domains, the plan quality drops if a required transformation or validator does not exist.</p>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#proposal","title":"Proposal","text":"<p>Add a missing-capability loop:</p> <ol> <li> <p>Detect unresolved task capability in pipeline stage execution.</p> </li> <li> <p>Generate plugin spec (inputs/outputs/contracts).</p> </li> <li> <p>Code plugin on demand in a sandbox.</p> </li> <li> <p>Run tests + security checks.</p> </li> <li> <p>If passed, register plugin in Plugin Hub.</p> </li> <li> <p>Retry pipeline stage with new plugin.</p> </li> </ol>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#where-it-plugs-in","title":"Where it plugs in","text":"<ul> <li> <p>Add <code>PluginResolver</code> before stage execution in <code>run_plan_pipeline.py</code>.</p> </li> <li> <p>Add <code>CapabilityNotFoundError</code> handling path that calls <code>PluginSynthesizer</code>.</p> </li> <li> <p>Add <code>PluginHubClient</code> for publish/fetch.</p> </li> </ul>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#minimal-interfaces","title":"Minimal interfaces","text":"<pre><code>class PluginSpec(BaseModel):\n    capability: str\n    stage_name: str\n    input_schema: dict\n    output_schema: dict\n    constraints: list[str]\n\nclass PluginRecord(BaseModel):\n    plugin_id: str\n    capability: str\n    version: str\n    checksum: str\n    trust_tier: str\n</code></pre>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#data-model-additions","title":"Data model additions","text":"<ul> <li> <p><code>plugin_hub_plugins</code> (id, capability, version, checksum, owner, trust_tier, created_at)</p> </li> <li> <p><code>plugin_hub_usage</code> (plugin_id, run_id, success, latency_ms, error_type)</p> </li> <li> <p><code>plugin_hub_test_reports</code> (plugin_id, report_json, pass_rate)</p> </li> </ul>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#rollout","title":"Rollout","text":"<ul> <li> <p>Phase 1: detect + suggest plugin (no auto-code)</p> </li> <li> <p>Phase 2: auto-code in dry-run mode (no publish)</p> </li> <li> <p>Phase 3: auto-code + gated publish + reuse</p> </li> </ul>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#risks-controls","title":"Risks &amp; controls","text":"<ul> <li> <p>Risk: low-quality generated plugins</p> </li> <li> <p>Control: required contract tests + static analysis + sandbox execution</p> </li> <li> <p>Risk: supply-chain poisoning</p> </li> <li> <p>Control: signed plugins + checksums + trust tiers + allowlist policy</p> </li> </ul>"},{"location":"proposals/16-on-demand-plugin-synthesis-hub/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>% of failed stages recovered via plugin synthesis</p> </li> <li> <p>Plugin reuse rate across runs</p> </li> <li> <p>Median time-to-capability (first missing capability to successful rerun)</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/","title":"Near-Match Plugin Adaptation Lifecycle","text":""},{"location":"proposals/17-plugin-adaptation-lifecycle/#pitch","title":"Pitch","text":"<p>When an existing plugin is close but not exact, PlanExe should branch, adapt, validate, and optionally merge the improvement\u2014treating plugins like living assets with versioned lifecycle.</p>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#problem","title":"Problem","text":"<p>A strict exact-match lookup causes unnecessary regeneration. Most missing capabilities are near neighbors of existing plugins.</p>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#proposal","title":"Proposal","text":"<p>Implement a lifecycle for adapt instead of rebuild:</p> <ol> <li> <p>Retrieve top-N similar plugins by capability embedding.</p> </li> <li> <p>Compute fit score against requested contract.</p> </li> <li> <p>If fit &gt;= threshold, create adaptation branch (<code>plugin@vX-adapt-runY</code>).</p> </li> <li> <p>Apply targeted code edits.</p> </li> <li> <p>Validate against old + new test suites.</p> </li> <li> <p>Promote to new semantic version if quality holds.</p> </li> </ol>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#lifecycle-states","title":"Lifecycle states","text":"<p><code>candidate -&gt; adapted -&gt; validated -&gt; canary -&gt; stable -&gt; deprecated</code></p>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#versioning-rules","title":"Versioning rules","text":"<ul> <li> <p>Patch: non-breaking bug/compat fix</p> </li> <li> <p>Minor: backward-compatible capability expansion</p> </li> <li> <p>Major: contract-breaking changes</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#review-policy","title":"Review policy","text":"<ul> <li> <p>Auto-promote only for patch/minor with full test pass and no security regressions.</p> </li> <li> <p>Require human review for major upgrades or trust-tier elevation.</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#run_plan_pipelinepy-integration","title":"<code>run_plan_pipeline.py</code> integration","text":"<ul> <li> <p>Add <code>PluginAdaptationPlanner</code> before synthesis path.</p> </li> <li> <p>Preference order: exact match -&gt; near-match adapt -&gt; on-demand synthesis.</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#data-model-additions","title":"Data model additions","text":"<ul> <li> <p><code>plugin_lineage</code> (parent_plugin_id, child_plugin_id, reason)</p> </li> <li> <p><code>plugin_versions</code> (plugin_id, semver, state, changelog)</p> </li> <li> <p><code>plugin_canary_stats</code> (version_id, success_rate, rollback_count)</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#rollback-strategy","title":"Rollback strategy","text":"<ul> <li> <p>Canary cohort first (e.g., 5% runs)</p> </li> <li> <p>Auto-rollback if failure rate delta exceeds threshold</p> </li> </ul>"},{"location":"proposals/17-plugin-adaptation-lifecycle/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>% missing capabilities solved via adaptation vs full synthesis</p> </li> <li> <p>Adaptation lead time</p> </li> <li> <p>Regression rate after adaptation</p> </li> </ul>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/","title":"Plugin Benchmarking Harness Across Diverse Plan Types","text":""},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#pitch","title":"Pitch","text":"<p>Create a benchmark harness that continuously measures plugin quality across a broad matrix of plan domains, complexity levels, and risk profiles\u2014so plugin quality is evidence-based, not anecdotal.</p>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#problem","title":"Problem","text":"<p>Without systematic benchmarking, Plugin Hub quality will drift and overfit to popular plan categories.</p>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#proposal","title":"Proposal","text":"<p>Build a benchmark framework with:</p> <ul> <li> <p>Dataset matrix: business, software, nonprofit, policy, industrial, scientific</p> </li> <li> <p>Scenario tiers: simple, medium, complex, adversarial</p> </li> <li> <p>Golden outputs and contract checks per scenario</p> </li> <li> <p>Regression suite run on each plugin publish</p> </li> </ul>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#core-scoring-dimensions","title":"Core scoring dimensions","text":"<ol> <li> <p>Contract adherence (schema + invariants)</p> </li> <li> <p>Correctness against golden cases</p> </li> <li> <p>Robustness under noisy inputs</p> </li> <li> <p>Latency and cost</p> </li> <li> <p>Generalization across domains</p> </li> </ol>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#execution-design","title":"Execution design","text":"<ul> <li> <p><code>benchmark_runner.py</code> executes plugin against scenario bundle</p> </li> <li> <p>Stores metrics in <code>plugin_benchmarks</code></p> </li> <li> <p>Produces leaderboard and confidence bands</p> </li> </ul>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#suggested-score-formula","title":"Suggested score formula","text":"<p><code>overall = 0.35*correctness + 0.20*robustness + 0.20*contract + 0.15*generalization + 0.10*latency_cost</code></p>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#run_plan_pipelinepy-policy-hook","title":"<code>run_plan_pipeline.py</code> policy hook","text":"<ul> <li> <p>Production selection should prefer plugins above minimum benchmark grade (e.g., B+)</p> </li> <li> <p>If all candidates fail threshold, fall back to synthesis + quarantine mode</p> </li> </ul>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#data-model-additions","title":"Data model additions","text":"<ul> <li> <p><code>plugin_benchmark_runs</code> (plugin_id, suite_id, score_json, created_at)</p> </li> <li> <p><code>benchmark_scenarios</code> (suite_id, domain, difficulty, expected_contract)</p> </li> <li> <p><code>plugin_quality_grade</code> (plugin_id, grade, confidence, last_eval_at)</p> </li> </ul>"},{"location":"proposals/18-plugin-benchmarking-coverage-harness/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Benchmark coverage % across supported plan categories</p> </li> <li> <p>Failed-in-prod rate vs benchmark grade</p> </li> <li> <p>Time to detect regressions after plugin updates</p> </li> </ul>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/","title":"Safety + Governance for Runtime Plugin Loading","text":""},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#pitch","title":"Pitch","text":"<p>If PlanExe can code and load plugins dynamically, governance must be first-class: trust tiers, policy gates, signed artifacts, and audit trails before execution in <code>run_plan_pipeline.py</code>.</p>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#problem","title":"Problem","text":"<p>Dynamic plugin ecosystems introduce security, compliance, and reliability risks unless runtime loading is policy-controlled.</p>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#proposal","title":"Proposal","text":"<p>Introduce policy-governed runtime loading:</p> <ul> <li> <p>Every plugin has trust tier (<code>experimental</code>, <code>verified</code>, <code>trusted</code>)</p> </li> <li> <p>Stage-level policy defines allowed tiers</p> </li> <li> <p>Load only signed plugins with valid checksum and provenance</p> </li> <li> <p>Log full execution trace for every plugin invocation</p> </li> </ul>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#runtime-checks-in-run_plan_pipelinepy","title":"Runtime checks in <code>run_plan_pipeline.py</code>","text":"<p>Before plugin execution:</p> <ol> <li> <p>Verify signature + checksum</p> </li> <li> <p>Validate declared capabilities vs requested stage contract</p> </li> <li> <p>Enforce policy (tier + owner + allowlist)</p> </li> <li> <p>Enforce resource limits (CPU/memory/timeouts)</p> </li> </ol>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#governance-controls","title":"Governance controls","text":"<ul> <li> <p>Quarantine mode for new plugins</p> </li> <li> <p>Kill switch per plugin/version</p> </li> <li> <p>Mandatory re-validation on dependency updates</p> </li> <li> <p>Security scan (SAST + dependency CVE check)</p> </li> </ul>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#data-model-additions","title":"Data model additions","text":"<ul> <li> <p><code>plugin_policy</code> (stage_name, allowed_tiers, owners, max_runtime_ms)</p> </li> <li> <p><code>plugin_audit_log</code> (run_id, plugin_id, version, decision, reason)</p> </li> <li> <p><code>plugin_security_reports</code> (plugin_id, scan_result, vuln_count)</p> </li> </ul>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#incident-response","title":"Incident response","text":"<ul> <li> <p>One-click disable plugin/version</p> </li> <li> <p>Backfill affected runs via audit lookup</p> </li> <li> <p>Auto-notify maintainers on abnormal failure spikes</p> </li> </ul>"},{"location":"proposals/19-plugin-safety-governance-for-runtime-loading/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Blocked unsafe plugin load attempts</p> </li> <li> <p>Mean time to contain plugin incident</p> </li> <li> <p>% plugin executions with complete provenance</p> </li> </ul>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/","title":"Plugin Hub Discovery, Ranking, and Reuse Economy","text":""},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#pitch","title":"Pitch","text":"<p>A plugin hub only creates compounding value if the best plugin is discoverable quickly. Add semantic search, performance ranking, and reuse incentives so future plan creations get stronger over time.</p>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#problem","title":"Problem","text":"<p>As plugin count grows, retrieval quality becomes the bottleneck. Poor discovery leads to duplicate plugins and lower plan quality.</p>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#proposal","title":"Proposal","text":"<p>Build a hub retrieval layer with:</p> <ul> <li> <p>Semantic capability search (embedding-based)</p> </li> <li> <p>Multi-factor ranking (fit, benchmark grade, reliability, recency)</p> </li> <li> <p>Reuse feedback loop (successful reuse boosts rank)</p> </li> <li> <p>Duplicate detection + merge suggestions</p> </li> </ul>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#retrieval-ranking-formula-example","title":"Retrieval ranking formula (example)","text":"<p><code>rank = 0.40*capability_fit + 0.25*benchmark_grade + 0.20*reliability + 0.10*recency + 0.05*reuse_trust</code></p>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#run_plan_pipelinepy-integration","title":"<code>run_plan_pipeline.py</code> integration","text":"<ul> <li> <p>Replace first-hit plugin selection with top-k ranked retrieval</p> </li> <li> <p>Execute top candidate; fallback to runner-up on hard failure</p> </li> <li> <p>Emit post-run feedback to hub ranking system</p> </li> </ul>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#data-model-additions","title":"Data model additions","text":"<ul> <li> <p><code>plugin_embeddings</code> (plugin_id, capability_vector)</p> </li> <li> <p><code>plugin_rank_features</code> (plugin_id, fit_score, reliability, recency, reuse_count)</p> </li> <li> <p><code>plugin_feedback</code> (plugin_id, run_id, outcome, quality_label)</p> </li> </ul>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#reuse-incentives","title":"Reuse incentives","text":"<ul> <li> <p>Reward high-performing plugins with higher trust and visibility</p> </li> <li> <p>Penalize frequent regressions via automatic rank decay</p> </li> <li> <p>Flag stale plugins for retraining or deprecation</p> </li> </ul>"},{"location":"proposals/20-plugin-hub-discovery-ranking-and-reuse/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Plugin reuse rate over time</p> </li> <li> <p>Duplicate plugin creation rate</p> </li> <li> <p>Median retrieval-to-success latency</p> </li> <li> <p>Top-1 retrieval success rate</p> </li> </ul>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/","title":"Expert Discovery + Fit Scoring for Plan Verification","text":""},{"location":"proposals/21-expert-discovery-and-fit-scoring/#pitch","title":"Pitch","text":"<p>Add an Expert Discovery layer that finds and ranks domain experts for a given plan (e.g., cross-border bridge projects), so users can move from draft plans to verified, execution-ready plans.</p>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#problem","title":"Problem","text":"<p>Users can generate high-quality plans, but often cannot identify the right experts to validate assumptions, engineering safety, legal constraints, and execution feasibility.</p>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#proposal","title":"Proposal","text":"<p>Build a pipeline that:</p> <ol> <li> <p>Extracts verification domains from plan artifacts.</p> </li> <li> <p>Searches expert sources (licenses, publications, associations, project portfolios).</p> </li> <li> <p>Computes a fit score per expert.</p> </li> <li> <p>Produces a shortlist with reasoning and risk flags.</p> </li> </ol>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#core-fit-model","title":"Core fit model","text":"<p><code>fit_score = 0.35*domain_match + 0.20*project_similarity + 0.15*regional_relevance + 0.15*credential_strength + 0.15*availability_signal</code></p>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#data-sources-initial","title":"Data sources (initial)","text":"<ul> <li> <p>Professional associations and certification registries</p> </li> <li> <p>Public procurement/project databases</p> </li> <li> <p>Publications/patents and conference profiles</p> </li> <li> <p>Verified platform profiles (LinkedIn-like signals as optional)</p> </li> </ul>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#output-for-user","title":"Output for user","text":"<ul> <li> <p>Top 5\u201320 experts by category (engineering, legal, environmental, finance)</p> </li> <li> <p>Why each expert is relevant</p> </li> <li> <p>Gaps in verification coverage</p> </li> </ul>"},{"location":"proposals/21-expert-discovery-and-fit-scoring/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Time from plan creation to first expert outreach</p> </li> <li> <p>Expert acceptance rate</p> </li> <li> <p>% plans with complete verification coverage</p> </li> </ul>"},{"location":"proposals/22-multi-stage-verification-workflow/","title":"Multi-Stage Expert Verification Workflow","text":""},{"location":"proposals/22-multi-stage-verification-workflow/#pitch","title":"Pitch","text":"<p>Introduce a structured verification workflow that separates quick plausibility checks from deep technical/legal reviews, making expert collaboration predictable and scalable.</p>"},{"location":"proposals/22-multi-stage-verification-workflow/#problem","title":"Problem","text":"<p>Verification is currently ad hoc. Users need a clear path from \"draft plan\" to \"verified plan\" with stage gates.</p>"},{"location":"proposals/22-multi-stage-verification-workflow/#proposal","title":"Proposal","text":"<p>Define verification stages:</p> <ol> <li> <p>Stage A: Triage Review (fast) \u2014 identify critical flaws and missing evidence.</p> </li> <li> <p>Stage B: Domain Review (deep) \u2014 engineering/legal/environmental/financial domain checks.</p> </li> <li> <p>Stage C: Integration Review \u2014 reconcile cross-domain conflicts.</p> </li> <li> <p>Stage D: Final Verification Report \u2014 signed conclusions + conditions.</p> </li> </ol>"},{"location":"proposals/22-multi-stage-verification-workflow/#workflow-artifacts","title":"Workflow artifacts","text":"<ul> <li> <p>Verification checklist per domain</p> </li> <li> <p>Issue register with severity and owners</p> </li> <li> <p>Evidence pack (documents, assumptions, calculations)</p> </li> <li> <p>Final signed verification summary</p> </li> </ul>"},{"location":"proposals/22-multi-stage-verification-workflow/#run_plan_pipelinepy-integration","title":"<code>run_plan_pipeline.py</code> integration","text":"<ul> <li> <p>Add optional <code>verification_mode=true</code></p> </li> <li> <p>Pipeline emits domain-specific review packets</p> </li> <li> <p>Plan status transitions: <code>draft -&gt; in_review -&gt; conditionally_verified -&gt; verified</code></p> </li> </ul>"},{"location":"proposals/22-multi-stage-verification-workflow/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Verification cycle time</p> </li> <li> <p>Critical issue catch rate before implementation</p> </li> <li> <p>Rework reduction after verification</p> </li> </ul>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/","title":"Expert Collaboration Marketplace + Reputation Graph","text":""},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#pitch","title":"Pitch","text":"<p>Create a trusted expert collaboration layer where plans can be reviewed by verified specialists, with transparent reputation and outcome tracking.</p>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#problem","title":"Problem","text":"<p>Finding experts is not enough; users need confidence in reviewer quality, reliability, and relevance.</p>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#proposal","title":"Proposal","text":"<p>Add a marketplace-like module with:</p> <ul> <li> <p>Verified expert profiles (credentials + domain tags)</p> </li> <li> <p>Reputation graph (quality of past reviews, response time, conflicts)</p> </li> <li> <p>Matching requests with SLA options</p> </li> <li> <p>Structured collaboration threads tied to plan sections</p> </li> </ul>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#reputation-dimensions","title":"Reputation dimensions","text":"<ol> <li> <p>Technical accuracy of reviews</p> </li> <li> <p>Resolution quality (did recommendations help?)</p> </li> <li> <p>Timeliness and reliability</p> </li> <li> <p>Conflict-of-interest transparency</p> </li> </ol>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#anti-gaming-controls","title":"Anti-gaming controls","text":"<ul> <li> <p>Blind secondary reviews for high-stakes plans</p> </li> <li> <p>Anomaly detection for reciprocal rating abuse</p> </li> <li> <p>Weighted trust from verified outcomes (not just stars)</p> </li> </ul>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#user-facing-value","title":"User-facing value","text":"<ul> <li> <p>Better expert selection confidence</p> </li> <li> <p>Faster collaboration starts</p> </li> <li> <p>Persistent expert network for future plans</p> </li> </ul>"},{"location":"proposals/23-expert-collaboration-marketplace-and-reputation/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Median time-to-match expert</p> </li> <li> <p>Review completion rate</p> </li> <li> <p>User satisfaction with verification outcomes</p> </li> </ul>"},{"location":"proposals/24-cross-border-project-verification-framework/","title":"Cross-Border Project Verification Framework (Bridge Example)","text":""},{"location":"proposals/24-cross-border-project-verification-framework/#pitch","title":"Pitch","text":"<p>For cross-border infrastructure plans (e.g., bridge between Country A and B), provide a dedicated verification framework that coordinates jurisdictional, engineering, and geopolitical expert review.</p>"},{"location":"proposals/24-cross-border-project-verification-framework/#problem","title":"Problem","text":"<p>Cross-border projects have failure modes beyond engineering: bilateral regulation, environmental treaties, customs/logistics impacts, and political risk.</p>"},{"location":"proposals/24-cross-border-project-verification-framework/#proposal","title":"Proposal","text":"<p>Create a cross-border verification pack with required domains:</p> <ul> <li> <p>Structural/civil engineering</p> </li> <li> <p>Geotechnical and hydrology</p> </li> <li> <p>Environmental and permitting law (both countries)</p> </li> <li> <p>Public finance and procurement</p> </li> <li> <p>Geopolitical risk and bilateral governance</p> </li> </ul>"},{"location":"proposals/24-cross-border-project-verification-framework/#required-outputs","title":"Required outputs","text":"<ol> <li> <p>Jurisdiction matrix (A vs B requirements)</p> </li> <li> <p>Conflict map (where standards diverge)</p> </li> <li> <p>Harmonization plan (how to resolve conflicts)</p> </li> <li> <p>Independent risk verdict (go/no-go conditions)</p> </li> </ol>"},{"location":"proposals/24-cross-border-project-verification-framework/#expert-orchestration-model","title":"Expert orchestration model","text":"<ul> <li> <p>Two domestic expert leads (one per country)</p> </li> <li> <p>One independent neutral chair</p> </li> <li> <p>Domain specialists mapped to unresolved risks</p> </li> </ul>"},{"location":"proposals/24-cross-border-project-verification-framework/#run_plan_pipelinepy-support","title":"<code>run_plan_pipeline.py</code> support","text":"<ul> <li> <p>Detect cross-border project signatures</p> </li> <li> <p>Auto-generate jurisdiction-specific review packets</p> </li> <li> <p>Require dual-signoff before \"verified\" state</p> </li> </ul>"},{"location":"proposals/24-cross-border-project-verification-framework/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Number of jurisdiction conflicts resolved pre-procurement</p> </li> <li> <p>Reduction in late-stage legal blockers</p> </li> <li> <p>Stakeholder confidence score in cross-border readiness</p> </li> </ul>"},{"location":"proposals/25-verification-incentives-governance-and-liability/","title":"Verification Incentives, Governance, and Liability Model","text":""},{"location":"proposals/25-verification-incentives-governance-and-liability/#pitch","title":"Pitch","text":"<p>Expert verification only scales if incentives and liability are clear. Add governance, compensation, and legal responsibility templates for trusted collaboration.</p>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#problem","title":"Problem","text":"<p>Experts may avoid participation without clear compensation, scope boundaries, and liability protections.</p>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#proposal","title":"Proposal","text":"<p>Define a governance model with:</p> <ul> <li> <p>Verification scopes (advisory vs certifying opinions)</p> </li> <li> <p>Compensation models (flat fee, milestone, retainer)</p> </li> <li> <p>Liability boundaries and insurance expectations</p> </li> <li> <p>Conflict-of-interest disclosures and recusal policy</p> </li> </ul>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#verification-contract-templates","title":"Verification contract templates","text":"<ol> <li> <p>Expert engagement agreement</p> </li> <li> <p>Review scope and deliverable definition</p> </li> <li> <p>Evidence standard and acceptance criteria</p> </li> <li> <p>Escalation + dispute resolution process</p> </li> </ol>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#risk-controls","title":"Risk controls","text":"<ul> <li> <p>Mandatory disclosures for financial/political conflicts</p> </li> <li> <p>Multi-expert quorum for critical safety decisions</p> </li> <li> <p>Immutable audit trail of comments and signed decisions</p> </li> </ul>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#product-integration","title":"Product integration","text":"<ul> <li> <p>\"Verified by\" badges with confidence and scope labels</p> </li> <li> <p>Verification status visible to investors/stakeholders</p> </li> <li> <p>Exportable compliance bundle for due diligence</p> </li> </ul>"},{"location":"proposals/25-verification-incentives-governance-and-liability/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Expert participation and retention</p> </li> <li> <p>Legal disputes per 100 verified plans</p> </li> <li> <p>Investor trust uplift for verified plans</p> </li> </ul>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/","title":"News Intake + Opportunity Sensing Grid for Autonomous Bidding","text":""},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#pitch","title":"Pitch","text":"<p>Build a continuous news-intake grid that detects project opportunities (bridge, IT infrastructure, utilities, public procurement) and turns them into structured planning prompts at scale.</p>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#why","title":"Why","text":"<p>If an autonomous AI organization generates ~1000 plans/day, the bottleneck is not planning\u2014it is finding high-value opportunities early and classifying them correctly.</p>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#proposal","title":"Proposal","text":"<p>Implement a multi-source intake pipeline:</p> <ol> <li> <p>Ingest signals from procurement feeds, industry media, government notices, and infrastructure newsletters.</p> </li> <li> <p>Normalize each item to an <code>opportunity_event</code> schema.</p> </li> <li> <p>Score urgency + bidability + strategic fit.</p> </li> <li> <p>Auto-generate candidate prompts for plan creation.</p> </li> </ol>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#source-categories-to-monitor","title":"Source categories to monitor","text":"<ul> <li> <p>Public procurement portals (national + regional)</p> </li> <li> <p>Government transport/infrastructure bulletins</p> </li> <li> <p>Utility/telecom modernization notices</p> </li> <li> <p>Construction/engineering trade publications</p> </li> <li> <p>Press wires (major project announcements)</p> </li> <li> <p>Local/regional news for early non-centralized opportunities</p> </li> </ul>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#core-schema","title":"Core schema","text":"<pre><code>{\n  \"event_id\": \"...\",\n  \"source\": \"...\",\n  \"domain\": \"bridge|it_infra|energy|...\",\n  \"region\": \"...\",\n  \"estimated_budget\": \"...\",\n  \"deadline_hint\": \"...\",\n  \"confidence\": 0.0,\n  \"raw_text\": \"...\"\n}\n</code></pre>"},{"location":"proposals/26-news-intake-and-opportunity-sensing-grid/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Opportunity recall vs known project announcements</p> </li> <li> <p>Time-to-detection after first public signal</p> </li> <li> <p>% opportunities converted to high-quality planning prompts</p> </li> </ul>"},{"location":"proposals/27-multi-angle-topic-verification-engine/","title":"Multi-Angle Topic Verification Engine Before Bidding","text":""},{"location":"proposals/27-multi-angle-topic-verification-engine/#pitch","title":"Pitch","text":"<p>Before the organization allocates resources to bids, verify each topic from multiple angles (technical, legal, financial, geopolitical, reputational) to reduce false positives and costly misfires.</p>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#why","title":"Why","text":"<p>Single-source news is noisy and often incomplete. Bidding on weak or misclassified opportunities burns compute, expert capacity, and reputation.</p>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#proposal","title":"Proposal","text":"<p>For each detected opportunity, run a verification bundle:</p> <ol> <li> <p>Source triangulation (minimum 3 independent sources)</p> </li> <li> <p>Contradiction detection across sources</p> </li> <li> <p>Domain-specific plausibility checks</p> </li> <li> <p>Regulatory feasibility checks</p> </li> <li> <p>Counterparty legitimacy signals</p> </li> </ol>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#verification-dimensions","title":"Verification dimensions","text":"<ul> <li> <p>Technical: Is scope physically/logically feasible?</p> </li> <li> <p>Legal/regulatory: Is tender structure valid and compliant?</p> </li> <li> <p>Financial: Is budget/deadline credible?</p> </li> <li> <p>Political/geopolitical: Is project likely to be paused/blocked?</p> </li> <li> <p>Reputation: Is this a scam/PR trap/high controversy event?</p> </li> </ul>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#decision-classes","title":"Decision classes","text":"<ul> <li> <p><code>verified_strong</code></p> </li> <li> <p><code>verified_with_risks</code></p> </li> <li> <p><code>insufficient_evidence</code></p> </li> <li> <p><code>do_not_pursue</code></p> </li> </ul>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#integration-point","title":"Integration point","text":"<ul> <li>Add a pre-plan gate: opportunities must pass verification threshold before entering high-volume planning queue.</li> </ul>"},{"location":"proposals/27-multi-angle-topic-verification-engine/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>False-positive reduction rate</p> </li> <li> <p>% bid opportunities that pass post-hoc reality checks</p> </li> <li> <p>Reduction in wasted plan generations on invalid topics</p> </li> </ul>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/","title":"Autonomous Bid Factory Orchestration (1000 Plans/Day)","text":""},{"location":"proposals/28-autonomous-bid-factory-orchestration/#pitch","title":"Pitch","text":"<p>Create a queue-based orchestration system that can generate, refine, and package up to 1000 plans/day while enforcing budget, quality, and domain-priority constraints.</p>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#why","title":"Why","text":"<p>At this throughput, naive generation creates noise. The system needs disciplined orchestration, quotas, and escalation paths.</p>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#proposal","title":"Proposal","text":"<p>Build a 4-stage bid factory:</p> <ol> <li> <p>Intake: validated opportunities from news pipeline</p> </li> <li> <p>Generation: multi-domain plan drafting</p> </li> <li> <p>Selection: quality ranking + risk filtering</p> </li> <li> <p>Packaging: bid artifacts and submission-ready bundles</p> </li> </ol>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#throughput-controls","title":"Throughput controls","text":"<ul> <li> <p>Domain quotas (avoid one domain monopolizing capacity)</p> </li> <li> <p>Region quotas</p> </li> <li> <p>Risk-adjusted compute budgets</p> </li> <li> <p>SLA tiers (urgent tenders vs long-lead projects)</p> </li> </ul>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#required-outputs-per-candidate","title":"Required outputs per candidate","text":"<ul> <li> <p>Executive summary</p> </li> <li> <p>Technical approach</p> </li> <li> <p>Cost/schedule assumptions</p> </li> <li> <p>Risk register</p> </li> <li> <p>Compliance checklist</p> </li> <li> <p>Bid/no-bid recommendation</p> </li> </ul>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#operational-safeguards","title":"Operational safeguards","text":"<ul> <li> <p>Backpressure when queue exceeds threshold</p> </li> <li> <p>Automatic downgrade to sketch-level plans under load</p> </li> <li> <p>Escalation to deep reasoning only for shortlisted opportunities</p> </li> </ul>"},{"location":"proposals/28-autonomous-bid-factory-orchestration/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Plans/day at target quality threshold</p> </li> <li> <p>Cost per usable bid package</p> </li> <li> <p>Bid conversion readiness rate</p> </li> </ul>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/","title":"ELO-Ranked Bid Selection + Escalation Pipeline","text":""},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#pitch","title":"Pitch","text":"<p>Use <code>07-elo-ranking.md</code> as the core selection engine: rank thousands of generated plans, shortlist the strongest, and escalate only top candidates into expensive expert-grade refinement.</p>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#why","title":"Why","text":"<p>If 1000 plans are generated daily, selection quality is the economic center. ELO ranking provides a robust comparative filter before deeper spend.</p>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#proposal","title":"Proposal","text":"<p>Implement a 3-tier funnel:</p> <ol> <li> <p>Tier 1: bulk generation + lightweight scoring</p> </li> <li> <p>Tier 2: ELO pairwise ranking and percentile assignment</p> </li> <li> <p>Tier 3: top percentile receives expert verification + final bid packaging</p> </li> </ol>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#suggested-cutoffs","title":"Suggested cutoffs","text":"<ul> <li> <p>Keep top 20% after first pass</p> </li> <li> <p>Keep top 5% after ELO cross-domain ranking</p> </li> <li> <p>Send top 1% to full bid escalation</p> </li> </ul>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#selection-features","title":"Selection features","text":"<ul> <li> <p>ELO score + confidence band</p> </li> <li> <p>Domain fitness to opportunity</p> </li> <li> <p>Verification status from Proposal 27</p> </li> <li> <p>Resource feasibility for timely delivery</p> </li> </ul>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#feedback-loop","title":"Feedback loop","text":"<ul> <li> <p>Track actual outcomes (win/loss, shortlist/not shortlisted)</p> </li> <li> <p>Feed outcomes back into ranking calibration</p> </li> <li> <p>Penalize plans that score high but underperform in real bids</p> </li> </ul>"},{"location":"proposals/29-elo-ranked-bid-selection-and-escalation/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Precision of top-ranked plans</p> </li> <li> <p>Win-rate lift from ELO-based shortlisting</p> </li> <li> <p>Cost savings from reduced deep-review volume</p> </li> </ul>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/","title":"Governance, Risk, and Ethics for Autonomous Bidding Organizations","text":""},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#pitch","title":"Pitch","text":"<p>Define governance boundaries for an AI-run bidding organization so autonomous planning remains auditable, lawful, and aligned with stakeholder trust.</p>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#why","title":"Why","text":"<p>High-volume autonomous bidding can create legal, ethical, and market-manipulation risks without policy guardrails.</p>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#proposal","title":"Proposal","text":"<p>Establish a governance framework with:</p> <ol> <li> <p>Bid/no-bid policy rules</p> </li> <li> <p>Human oversight thresholds for high-impact projects</p> </li> <li> <p>Audit logs for all generated recommendations</p> </li> <li> <p>Conflict-of-interest and collusion safeguards</p> </li> <li> <p>Jurisdiction-aware compliance controls</p> </li> </ol>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#policy-examples","title":"Policy examples","text":"<ul> <li> <p>Mandatory human signoff for projects above budget threshold</p> </li> <li> <p>No autonomous bids in restricted/sanctioned jurisdictions</p> </li> <li> <p>Mandatory disclosure bundle for generated assumptions</p> </li> <li> <p>Automatic refusal when source verification is weak</p> </li> </ul>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#auditability-requirements","title":"Auditability requirements","text":"<ul> <li> <p>Immutable run record from intake -&gt; ranking -&gt; decision</p> </li> <li> <p>Explainability for why a bid was prioritized or rejected</p> </li> <li> <p>Reproducible artifact exports for regulators/partners</p> </li> </ul>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#ethics-controls","title":"Ethics controls","text":"<ul> <li> <p>Prevent exploitative targeting in fragile regions</p> </li> <li> <p>Avoid misinformation amplification from unverified news</p> </li> <li> <p>Include public-interest risk flags for critical infrastructure</p> </li> </ul>"},{"location":"proposals/30-autonomous-bid-governance-risk-and-ethics/#success-metrics","title":"Success metrics","text":"<ul> <li> <p>Compliance incident rate</p> </li> <li> <p>Audit completeness score</p> </li> <li> <p>Stakeholder trust and acceptance of AI-generated bid decisions</p> </li> </ul>"},{"location":"proposals/AGENTS/","title":"Proposals Authoring Guide","text":"<p>This folder contains product and research proposals that render under <code>/proposals/</code> on docs. The best proposals in this folder share a few consistent traits: they are precise, actionable, and anchored in PlanExe\u2019s existing pipeline.</p> <p>Below is the distilled guidance based on the current proposals in this folder.</p>"},{"location":"proposals/AGENTS/#what-makes-a-proposal-good-observed-patterns","title":"What Makes a Proposal Good (Observed Patterns)","text":"<ul> <li>Clear pitch + why now: A short, specific pitch followed by a concrete \u201cwhy\u201d (the bottleneck, failure mode, or opportunity).</li> <li>Concrete artifacts: The best proposals list tangible outputs (schemas, APIs, workflow artifacts, rank formulas, decision classes).</li> <li>Integration points: They explain where the change fits (e.g., <code>run_plan_pipeline.py</code>, routing config, queue, admin UI, MCP).</li> <li>Phased implementation: They sequence the work in small, verifiable phases.</li> <li>Measurable success: They define metrics with directionality or target ranges.</li> <li>Risks with mitigations: They name real failure modes and how to reduce them.</li> <li>Examples or diagrams: When relevant, they include a snippet, architecture diagram, or formula.</li> </ul>"},{"location":"proposals/AGENTS/#naming-and-title","title":"Naming and Title","text":"<ul> <li>Filename: keep the numeric prefix for ordering, e.g. <code>27-multi-angle-topic-verification-engine.md</code>.</li> <li>Title: do not include the number in the H1.</li> <li>Good: <code># Multi-Angle Topic Verification Engine Before Bidding</code></li> <li>Avoid: <code># 27) Multi-Angle Topic Verification Engine Before Bidding</code></li> </ul>"},{"location":"proposals/AGENTS/#metadata-block-required","title":"Metadata Block (Required)","text":"<p>Place directly under the H1. Example:</p> <pre><code>**Author:** PlanExe Team  \n**Date:** 2026-02-10  \n**Status:** Proposal  \n**Tags:** `investors`, `matching`, `roi`, `ranking`, `marketplace`\n</code></pre> <p>Notes: - Use backticks for each tag so MkDocs renders them cleanly. - Keep tags short and searchable.</p>"},{"location":"proposals/AGENTS/#front-matter-required","title":"Front Matter (Required)","text":"<p>All proposals must include YAML front matter (<code>---</code> blocks with <code>title</code>, <code>date</code>, <code>status</code>, <code>author</code>). Keep it consistent: - The front matter <code>title</code> must match the H1 (no numeric prefix). - Don\u2019t rely on the filename for display titles. - Quote <code>title</code> values that contain <code>:</code> to keep YAML valid.</p>"},{"location":"proposals/AGENTS/#required-sections","title":"Required Sections","text":"<p>Every proposal should include at least: - Pitch: one short paragraph stating the idea. - Problem: why this matters now. - Proposal / Solution: what we intend to build. - Success metrics: how we will measure outcomes. - Risks: key risks and mitigations.</p> <p>Optional but recommended: - Architecture or Workflow - Phases or Implementation - Data model / API / formula when relevant - Integration (where it plugs into current PlanExe systems)</p>"},{"location":"proposals/AGENTS/#markdown-formatting-rules-mkdocs-material","title":"Markdown Formatting Rules (MkDocs Material)","text":"<p>MkDocs is strict about lists. To avoid lists rendering as a single paragraph: - Always add a blank line before numbered or bulleted lists. - Keep list items on their own lines.</p> <p>Correct:</p> <pre><code>## Proposal\nDefine verification stages:\n\n1. **Stage A: Triage Review (fast)** \u2014 identify critical flaws and missing evidence.\n2. **Stage B: Domain Review (deep)** \u2014 engineering/legal/environmental/financial domain checks.\n3. **Stage C: Integration Review** \u2014 reconcile cross-domain conflicts.\n4. **Stage D: Final Verification Report** \u2014 signed conclusions + conditions.\n</code></pre> <p>Avoid:</p> <pre><code>## Proposal\nDefine verification stages:\n1. **Stage A: Triage Review (fast)** \u2014 identify critical flaws and missing evidence.\n</code></pre>"},{"location":"proposals/AGENTS/#suggested-template","title":"Suggested Template","text":"<pre><code># Title (no number)\n\n**Author:** PlanExe Team  \n**Date:** YYYY-MM-DD  \n**Status:** Proposal  \n**Tags:** `tag1`, `tag2`, `tag3`\n\n---\n\n## Pitch\nOne paragraph.\n\n## Problem\nWhy this matters.\n\n## Proposal\nWhat we plan to build.\n\n## Implementation (optional)\nPhases or architecture.\n\n## Integration (optional)\nWhere it plugs into PlanExe.\n\n## Success Metrics\n- Metric 1\n- Metric 2\n\n## Risks\n- Risk 1\n- Risk 2\n</code></pre>"}]}